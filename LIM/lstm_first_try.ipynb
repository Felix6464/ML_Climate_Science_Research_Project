{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import utils as ut\n",
    "import LIM_class\n",
    "\n",
    "plt.style.use(\"../plotting.mplstyle\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1000, 192, 288)\n",
      "Data : <xarray.DataArray 'ts' (time: 1000, lat: 192, lon: 288)>\n",
      "[55296000 values with dtype=float32]\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 -90.0 -89.06 -88.12 -87.17 ... 87.17 88.12 89.06 90.0\n",
      "  * lon      (lon) float64 -180.0 -178.8 -177.5 -176.2 ... 176.2 177.5 178.8\n",
      "  * time     (time) object 0001-01-15 12:00:00 ... 0084-04-15 00:00:00\n",
      "Attributes: (12/19)\n",
      "    cell_measures:  area: areacella\n",
      "    cell_methods:   area: time: mean\n",
      "    comment:        Surface temperature (skin for open ocean)\n",
      "    description:    Surface temperature (skin for open ocean)\n",
      "    frequency:      mon\n",
      "    id:             ts\n",
      "    ...             ...\n",
      "    time_label:     time-mean\n",
      "    time_title:     Temporal mean\n",
      "    title:          Surface Temperature\n",
      "    type:           real\n",
      "    units:          K\n",
      "    variable_id:    ts\n",
      "x_proc : <xarray.DataArray 'ts' (time: 1000, z: 55296)>\n",
      "array([[243.5303 , 243.53464, 243.53464, ..., 239.60129, 239.60106,\n",
      "        239.60085],\n",
      "       [232.6508 , 232.6557 , 232.6557 , ..., 244.87605, 244.8742 ,\n",
      "        244.87254],\n",
      "       [220.1912 , 220.1961 , 220.1961 , ..., 236.15085, 236.1529 ,\n",
      "        236.15474],\n",
      "       ...,\n",
      "       [233.94894, 233.95393, 233.95355, ..., 240.75266, 240.75166,\n",
      "        240.75078],\n",
      "       [220.58868, 220.5936 , 220.59358, ..., 237.62427, 237.6226 ,\n",
      "        237.62111],\n",
      "       [213.01923, 213.02428, 213.02428, ..., 252.93338, 252.93478,\n",
      "        252.93605]], dtype=float32)\n",
      "Coordinates:\n",
      "  * time     (time) object 0001-01-15 12:00:00 ... 0084-04-15 00:00:00\n",
      "  * z        (z) object MultiIndex\n",
      "  * lat      (z) float64 -90.0 -90.0 -90.0 -90.0 -90.0 ... 90.0 90.0 90.0 90.0\n",
      "  * lon      (z) float64 -180.0 -178.8 -177.5 -176.2 ... 175.0 176.2 177.5 178.8\n",
      "Attributes: (12/19)\n",
      "    cell_measures:  area: areacella\n",
      "    cell_methods:   area: time: mean\n",
      "    comment:        Surface temperature (skin for open ocean)\n",
      "    description:    Surface temperature (skin for open ocean)\n",
      "    frequency:      mon\n",
      "    id:             ts\n",
      "    ...             ...\n",
      "    time_label:     time-mean\n",
      "    time_title:     Temporal mean\n",
      "    title:          Surface Temperature\n",
      "    type:           real\n",
      "    units:          K\n",
      "    variable_id:    ts\n"
     ]
    }
   ],
   "source": [
    "data = xr.open_dataset(\"./data/ts_Amon_CESM2_piControl_r1i1p1f1.nc\")[\"ts\"]\n",
    "#data = xr.open_dataset(\"./data/zos_Amon_CESM2_piControl_r1i1p1f1.nc\")[\"zos\"]\n",
    "#data = xr.open_dataset(\"./data/ssta_1950_2021.nc\")[\"ssta\"]\n",
    "\n",
    "data = data[:1000, :, :]\n",
    "print(\"Data shape: {}\".format(data.shape))\n",
    "print(\"Data : {}\".format(data))\n",
    "\n",
    "\n",
    "# Use Principal Component Analysis to reduce the dimensionality of the data\n",
    "pca_10 = ut.SpatioTemporalPCA(data, n_components=20)\n",
    "eof_10 = pca_10.eofs()\n",
    "pc_10 = pca_10.principal_components()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius norm: 3.6242599487304688\n",
      "WARNING: Eigenvalues greater than 1 detected.\n",
      "WARNING: Covariance matrix has negative values!\n",
      "Data train : [[-12131.013  -12508.043  -13168.99   ... -16400.004  -16707.822\n",
      "  -16536.889 ]\n",
      " [-19987.258  -19433.979  -18764.057  ... -19822.018  -19794.018\n",
      "  -19904.496 ]\n",
      " [-11377.144  -11319.862  -11080.628  ... -11533.66   -11334.262\n",
      "  -11113.6875]\n",
      " [ -5021.9053  -4621.798   -5088.3076 ...  -5040.953   -5072.3975\n",
      "   -5043.0195]\n",
      " [ -4725.376   -4819.449   -4731.8687 ...  -4853.042   -4698.279\n",
      "   -4659.7495]] + shape: (20, 800) + type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Create training and test data\n",
    "data = pca_10.principal_components()\n",
    "index_train = int(0.8 * len(data[\"time\"]))\n",
    "data_train = data[:, :index_train]\n",
    "data_test = data[:, index_train:]\n",
    "\n",
    "# Creating an example LIM object\n",
    "tau = 1\n",
    "model = LIM_class.LIM(tau)\n",
    "model.fit(data_train.data)\n",
    "print(\"Data train : {} + shape: {} + type: {}\".format(data_train.data[:5], data_train.data.shape, type(data_train.data)))\n",
    "\n",
    "eigenvalues, _, _ = ut.matrix_decomposition(model.green_function)\n",
    "t_decay = [abs(-(1/np.log(eigenvalue.real))) for eigenvalue in eigenvalues]\n",
    "\n",
    "#print(\"Eigenvalues : min {} + max {}\".format(min(eigenvalues), max(eigenvalues)))\n",
    "#print(\"T-decay : min {} + max {}\".format(min(t_decay), max(t_decay)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_delta: 0.3568441801550697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\PycharmProjects\\ML_Climate_Science_Research_Project\\LIM\\LIM_class.py:239: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "  random_part = np.array(np.random.multivariate_normal([0 for n in range(num_comp)], self.noise_covariance))\n"
     ]
    }
   ],
   "source": [
    "# Simulate stochastic differential equation\n",
    "x_input = data_train.isel(time=0)\n",
    "times = x_input['time']\n",
    "x = x_input.data\n",
    "\n",
    "lim_integration, times_ = model.noise_integration(x, timesteps=499, seed=10, num_comp=len(pc_10))\n",
    "\n",
    "lim_integration = lim_integration.T\n",
    "pc_10 = np.array(pc_10)\n",
    "#print(\"LIMMM : {} + next {} + next {}\".format(lim_integration[0][:5], lim_integration[0][50], lim_integration[0][-3:]))\n",
    "#print(\"PCCC : {} + next {} + next {}\".format(pc_10[0][:5], pc_10[0][50], pc_10[0][-3:] ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data : [-12131.013  -12508.043  -13168.99   -14437.364  -15599.752  -16477.545\n",
      " -16701.354  -16649.082  -16055.64   -14854.778  -13607.776  -12565.216\n",
      " -12101.724  -12477.988  -13462.829  -14834.246  -15734.503  -16414.752\n",
      " -16618.287  -16677.6    -16066.892  -14915.94   -13507.861  -12458.042\n",
      " -12194.773  -12769.49   -13404.357  -14670.397  -15743.118  -16466.229\n",
      " -16688.477  -16513.58   -16084.348  -14809.902  -13479.406  -12629.81\n",
      " -12002.597  -12563.653  -13342.342  -14694.043  -15742.9375 -16445.773\n",
      " -16814.963  -16686.688  -15925.827  -14922.785  -13649.222  -12464.634\n",
      " -12323.74   -12632.03   -13460.442  -14702.308  -15735.301  -16414.889\n",
      " -16687.78   -16681.625  -16067.676  -15000.707  -13625.415  -12637.331\n",
      " -12281.062  -12681.287  -13458.915  -14381.694  -16004.489  -16499.076\n",
      " -16751.95   -16557.303  -16101.676  -14928.337  -13703.922  -12622.627\n",
      " -12447.257  -12631.951  -13505.499  -14578.897  -15712.284  -16506.275\n",
      " -16648.412  -16566.594  -16013.218  -14923.271  -13578.385  -12474.216\n",
      " -12204.042  -12491.129  -13423.945  -14511.831  -15834.489  -16467.914\n",
      " -16709.51   -16656.053  -16134.712  -15015.044  -13636.792  -12568.294\n",
      " -12327.5    -12547.721  -13371.512  -14494.874  -15853.176  -16531.465\n",
      " -16692.85   -16592.863  -16079.656  -15009.52   -13548.957  -12438.156\n",
      " -12101.402  -12483.044  -13392.462  -14502.1045 -15783.033  -16367.013\n",
      " -16665.23   -16553.398  -15936.823  -14941.934  -13743.702  -12533.077\n",
      " -12012.605  -12444.413  -13256.413  -14477.407  -15593.853  -16419.404\n",
      " -16739.348  -16688.988  -16048.355  -14948.671  -13778.488  -12742.718\n",
      " -11935.071  -12404.393  -13420.611  -14566.751  -15710.596  -16511.936\n",
      " -16721.701  -16704.846  -16136.24   -14925.742  -13652.709  -12572.749\n",
      " -12310.172  -12670.817  -13398.645  -14576.965  -15842.967  -16497.799\n",
      " -16768.318  -16655.062  -16029.107  -14911.727  -13511.443  -12571.153\n",
      " -12311.106  -12539.492  -13416.914  -14558.619  -15628.919  -16433.766\n",
      " -16819.885  -16601.51   -16021.884  -14925.503  -13470.549  -12635.797\n",
      " -12074.978  -12403.895  -13386.86   -14612.446  -15926.955  -16499.475\n",
      " -16724.139  -16692.875  -16084.187  -14912.247  -13651.281  -12464.862\n",
      " -12125.571  -12341.558  -13358.188  -14623.996  -15654.242  -16338.474\n",
      " -16766.646  -16658.037  -16004.055  -14884.745  -13619.291  -12431.9795\n",
      " -12076.165  -12504.063  -13199.256  -14443.474  -15764.624  -16379.082\n",
      " -16729.18   -16596.941  -15971.374  -14863.1045 -13525.408  -12793.535\n",
      " -12101.316  -12419.99   -13373.843  -14365.062  -15682.935  -16336.893\n",
      " -16717.271  -16599.428  -16105.94   -14859.061  -13684.917  -12535.608\n",
      " -12136.087  -12573.692  -13362.584  -14589.824  -15733.867  -16449.816\n",
      " -16777.441  -16637.2    -16050.45   -14979.074  -13555.871  -12538.321\n",
      " -12302.232  -12530.157  -13590.673  -14548.59   -15766.47   -16450.467\n",
      " -16614.947  -16621.256  -15967.497  -15003.42   -13631.775  -12523.768\n",
      " -11998.883  -12582.643  -13148.15   -14537.787  -15788.288  -16439.094\n",
      " -16675.81   -16547.365  -16058.306  -14839.763  -13563.775  -12554.762\n",
      " -12180.865  -12428.204  -13222.958  -14466.807  -15699.713  -16423.049\n",
      " -16616.498  -16463.82   -16102.947  -14845.289  -13508.148  -12414.673\n",
      " -12119.887  -12707.396  -13485.186  -14500.959  -15717.894  -16491.768\n",
      " -16593.824  -16547.139  -16122.368  -14969.138  -13552.2295 -12686.149\n",
      " -12349.305  -12632.576  -13389.534  -14555.422  -15759.271  -16420.787\n",
      " -16564.186  -16566.686  -16171.126  -14922.89   -13643.666  -12598.654\n",
      " -12243.646  -12721.233  -13374.986  -14427.023  -15607.96   -16567.287\n",
      " -16727.668  -16629.809  -16146.461  -14893.641  -13542.206  -12448.065\n",
      " -12234.091  -12563.48   -13297.799  -14446.233  -15708.863  -16485.275\n",
      " -16674.26   -16676.973  -16034.561  -14863.5    -13487.874  -12612.245\n",
      " -12109.202  -12482.238  -13310.417  -14622.69   -15860.829  -16426.05\n",
      " -16688.953  -16697.002  -16046.437  -15096.733  -13490.665  -12459.644\n",
      " -12065.737  -12584.473  -13439.664  -14474.729  -15637.839  -16254.894\n",
      " -16696.523  -16641.432  -15991.615  -15044.585  -13655.647  -12518.465\n",
      " -12070.185  -12497.188  -13471.292  -14675.215  -15625.625  -16474.71\n",
      " -16727.5    -16536.36   -16030.174  -14889.1    -13418.902  -12506.141\n",
      " -12116.198  -12368.715  -13314.114  -14486.03   -15623.076  -16409.97\n",
      " -16669.934  -16638.582  -16039.288  -14854.918  -13473.036  -12562.149\n",
      " -12091.438  -12449.584  -13254.822  -14554.214  -15767.204  -16451.58\n",
      " -16620.361  -16545.697  -16051.275  -14934.976  -13703.399  -12576.989\n",
      " -12140.22   -12542.69   -13435.409  -14576.503  -15719.637  -16455.395\n",
      " -16723.97   -16570.768  -16140.395  -14890.029  -13748.501  -12661.331\n",
      " -12212.578  -12548.483  -13385.962  -14415.407  -15610.704  -16482.057\n",
      " -16749.809  -16566.668  -16118.112  -14932.195  -13733.684  -12633.961\n",
      " -12107.089  -12486.945  -13313.491  -14500.107  -15797.944  -16567.473\n",
      " -16821.816  -16653.033  -16204.552  -15035.764  -13592.095  -12604.826\n",
      " -12209.954  -12668.409  -13451.428  -14510.871  -15845.892  -16487.795\n",
      " -16688.     -16511.594  -16103.101  -14835.928  -13530.017  -12651.91\n",
      " -12118.671  -12392.492  -13485.445  -14347.282  -15620.993  -16452.217\n",
      " -16557.023  -16482.633  -15994.379  -14883.155  -13504.216  -12508.665\n",
      " -12000.8545 -12442.07   -13451.811  -14514.266  -15719.147  -16371.735\n",
      " -16766.467  -16679.021  -16102.012  -14942.823  -13543.441  -12473.948\n",
      " -12106.448  -12419.569  -13109.436  -14556.425  -15579.015  -16422.094\n",
      " -16628.77   -16603.912  -16007.915  -14880.701  -13401.33   -12423.14\n",
      " -12005.81   -12562.9375 -13204.964  -14427.181  -15711.293  -16387.424\n",
      " -16637.668  -16684.777  -16156.389  -14931.384  -13292.1045 -12199.249\n",
      " -12131.789  -12624.85   -13421.397  -14524.426  -15725.464  -16418.637\n",
      " -16668.633  -16642.473  -16178.103  -14866.866  -13468.47   -12666.519\n",
      " -12076.597  -12562.295  -13267.764  -14447.051  -15680.801  -16514.559\n",
      " -16731.125  -16533.912  -15991.211  -14947.818  -13558.922  -12511.563\n",
      " -12388.667  -12651.128  -13354.528  -14635.162  -15709.752  -16392.764\n",
      " -16611.75   -16632.588  -15961.098  -14851.151  -13583.985  -12508.57\n",
      " -12035.23   -12447.388  -13196.447  -14544.784  -15727.983  -16581.234\n",
      " -16740.027  -16662.48   -15973.462  -14846.1045 -13504.303  -12447.105\n",
      " -12274.269  -12555.094  -13399.298  -14559.802  -15753.061  -16384.588\n",
      " -16730.459  -16641.658  -16024.681  -14791.486  -13448.684  -12370.698\n",
      " -11932.931  -12370.234  -13251.451  -14374.823  -15737.092  -16358.728\n",
      " -16676.43   -16424.889  -15979.514  -14789.248  -13447.245  -12587.208\n",
      " -12084.091  -12677.502  -13406.36   -14489.582  -15669.564  -16467.951\n",
      " -16661.062  -16513.305  -15953.064  -14762.769  -13509.8125 -12440.202\n",
      " -12240.953  -12556.591  -13385.301  -14438.988  -15750.098  -16422.871\n",
      " -16744.635  -16606.928  -15990.713  -14847.803  -13522.56   -12462.951\n",
      " -12017.968  -12395.069  -13323.351  -14579.87   -15675.238  -16379.035\n",
      " -16771.855  -16594.592  -16138.755  -14800.035  -13392.159  -12469.029\n",
      " -12059.255  -12512.593  -13344.494  -14514.386  -15716.327  -16505.38\n",
      " -16752.842  -16616.316  -16026.549  -14848.913  -13454.897  -12340.74\n",
      " -11995.257  -12362.701  -13305.801  -14393.305  -15682.972  -16507.602\n",
      " -16805.307  -16613.385  -16084.479  -14868.978  -13506.474  -12520.389\n",
      " -12308.404  -12549.42   -13577.945  -14697.105  -15797.168  -16444.004\n",
      " -16708.25   -16548.088  -15965.452  -14775.662  -13630.111  -12370.249\n",
      " -12014.962  -12439.681  -13329.774  -14260.061  -15667.361  -16396.45\n",
      " -16583.785  -16691.33   -16048.074  -14892.338  -13446.447  -12418.474\n",
      " -12257.415  -12445.885  -13241.143  -14614.937  -15613.79   -16414.188\n",
      " -16679.027  -16625.031  -16058.164  -14941.514  -13649.172  -12466.509\n",
      " -12180.138  -12758.821  -13443.998  -14654.391  -15731.399  -16404.258\n",
      " -16624.898  -16665.533  -16061.464  -14915.092  -13482.942  -12481.63\n",
      " -12229.446  -12459.797  -13292.804  -14555.3955 -15668.602  -16465.95\n",
      " -16772.758  -16506.1    -16023.86   -14812.75   -13555.022  -12448.779\n",
      " -12458.2295 -12384.824  -13451.42   -14378.575  -15738.135  -16427.426\n",
      " -16733.402  -16565.72   -16034.547  -14832.799  -13304.26   -12401.279\n",
      " -12103.098  -12370.406  -13177.596  -14362.668  -15660.575  -16469.305\n",
      " -16678.287  -16576.914  -16054.253  -14755.343  -13481.851  -12298.153\n",
      " -12023.355  -12557.7705 -13218.398  -14360.947  -15620.8955 -16391.559\n",
      " -16689.148  -16655.922  -15982.599  -14881.268  -13360.858  -12506.771\n",
      " -11780.224  -12311.118  -13195.363  -14397.568  -15639.597  -16291.112\n",
      " -16666.557  -16558.576  -16101.289  -14980.629  -13416.953  -12543.194\n",
      " -12196.795  -12462.022  -13389.879  -14396.787  -15712.8545 -16485.229\n",
      " -16766.684  -16638.195  -16064.84   -14840.432  -13473.308  -12380.063\n",
      " -12116.401  -12406.221  -13129.752  -14517.847  -15762.15   -16448.908\n",
      " -16663.736  -16556.756  -16043.934  -14856.073  -13347.953  -12357.455\n",
      " -12163.777  -12555.517  -13280.256  -14473.226  -15725.498  -16399.613\n",
      " -16709.154  -16531.404  -16009.621  -14927.06   -13527.029  -12549.189\n",
      " -12109.494  -12414.312  -13324.586  -14346.705  -15733.497  -16481.164\n",
      " -16671.605  -16608.225  -15930.56   -14780.053  -13467.511  -12468.44\n",
      " -11978.662  -12121.579  -13145.943  -14543.692  -15581.99   -16448.137\n",
      " -16719.871  -16610.51   -16007.539  -14900.241  -13524.145  -12216.841\n",
      " -11898.172  -12410.97   -13424.945  -14443.64   -15744.619  -16398.5\n",
      " -16714.07   -16620.416  -15952.409  -14707.919  -13405.991  -12411.069\n",
      " -12024.3545 -12425.079  -13349.274  -14491.33   -15790.977  -16451.822\n",
      " -16822.518  -16623.42   -16071.353  -14842.868  -13483.926  -12432.782\n",
      " -12065.935  -12484.361  -13227.721  -14638.292  -15796.419  -16400.004\n",
      " -16707.822  -16536.889  -15851.115  -14880.285  -13510.198  -12616.449\n",
      " -12163.657  -12598.123  -13248.358  -14648.034  -15752.531  -16465.38\n",
      " -16764.354  -16620.639  -15957.009  -14840.152  -13440.39   -12357.964\n",
      " -12069.233  -12369.876  -13084.755  -14458.881  -15672.953  -16382.043\n",
      " -16720.275  -16499.285  -15969.792  -14719.016  -13659.007  -12329.129\n",
      " -11943.25   -12464.516  -13226.541  -14478.691  -15620.046  -16438.578\n",
      " -16755.705  -16519.129  -15921.059  -14804.264  -13465.893  -12299.895\n",
      " -12018.139  -12459.03   -13523.337  -14324.76   -15764.185  -16364.314\n",
      " -16627.762  -16554.438  -16014.485  -14850.38   -13572.67   -12279.873\n",
      " -12134.018  -12557.2705 -13339.374  -14487.541  -15637.931  -16396.775\n",
      " -16777.445  -16571.373  -16093.592  -15030.618  -13331.759  -12400.736\n",
      " -12296.422  -12514.851  -13314.983  -14310.405  -15670.188  -16524.52\n",
      " -16585.87   -16551.486  -15972.237  -14877.002  -13340.371  -12458.342\n",
      " -12070.143  -12404.638  -13405.509  -14544.914  -15685.777  -16497.066\n",
      " -16588.729  -16569.469  -16021.817  -14811.208  -13484.849  -12520.356\n",
      " -12261.839  -12446.332  -13492.32   -14711.966  -15773.284  -16430.209\n",
      " -16655.293  -16544.562  -16007.527  -14762.891  -13429.314  -12467.377\n",
      " -12114.011  -12575.89   -13362.107  -14659.175  -15740.164  -16449.805\n",
      " -16678.516  -16530.916  -16050.314  -14906.822  -13540.747  -12605.518\n",
      " -12137.357  -12414.271  -13335.712  -14447.105  -15763.506  -16542.426\n",
      " -16641.447  -16650.549  -16217.162  -14928.81   -13596.895  -12510.913\n",
      " -12323.197  -12663.322  -13281.04   -14557.569  -15597.864  -16340.112\n",
      " -16712.562  -16692.334  -16127.03   -14912.865  -13537.691  -12579.121\n",
      " -12113.986  -12629.516  -13373.39   -14649.278  -15692.315  -16295.527\n",
      " -16653.283  -16504.54   -15968.531  -14948.812  -13497.777  -12349.165\n",
      " -12061.744  -12393.264  -13388.503  -14348.62   -15745.164  -16386.842\n",
      " -16718.986  -16643.762  -16119.348  -14917.289  -13497.026  -12522.401\n",
      " -12238.278  -12442.428  -13364.431  -14432.573  -15677.1045 -16417.88\n",
      " -16633.158  -16518.684  -15955.806  -14754.697  -13543.137  -12726.844\n",
      " -12392.659  -12386.284  -13235.264  -14415.949  -15592.195  -16375.638\n",
      " -16674.746  -16506.012  -16010.121  -14895.258  -13532.921  -12232.729\n",
      " -11944.811  -12323.857  -13408.321  -14469.816  -15768.065  -16358.9375\n",
      " -16728.215  -16612.775  -16114.359  -14813.575  -13376.852  -12393.326\n",
      " -11854.077  -12458.685  -13133.0205 -14605.913 ] + shape: (1000,) + type: <class 'numpy.ndarray'>\n",
      "Testing parameter combination 1/6: (32, 0.001, 1000, 3, 5)\n",
      "Epoch [1/1000], Loss: 213604955.5\n",
      "Epoch [101/1000], Loss: 210415573.0\n",
      "Epoch [201/1000], Loss: 207389797.0\n",
      "Epoch [301/1000], Loss: 204387711.5\n",
      "Epoch [401/1000], Loss: 201407929.0\n",
      "Epoch [501/1000], Loss: 198450335.0\n",
      "Epoch [601/1000], Loss: 195515006.0\n",
      "Epoch [701/1000], Loss: 192601866.0\n",
      "Epoch [801/1000], Loss: 189710870.5\n",
      "Epoch [901/1000], Loss: 186842096.5\n",
      "Saved model and hyperparameters to ./model_trained_lstm/fnn_model_1.pt\n",
      "\n",
      "Testing parameter combination 2/6: (32, 0.001, 1000, 3, 10)\n",
      "Epoch [1/1000], Loss: 214842385.03225806\n",
      "Epoch [101/1000], Loss: 211737261.41935483\n",
      "Epoch [201/1000], Loss: 208795534.96774194\n",
      "Epoch [301/1000], Loss: 205875979.87096775\n",
      "Epoch [401/1000], Loss: 202977355.3548387\n",
      "Epoch [501/1000], Loss: 200099626.32258064\n",
      "Epoch [601/1000], Loss: 197242679.7419355\n",
      "Epoch [701/1000], Loss: 194406616.2580645\n",
      "Epoch [801/1000], Loss: 191591487.48387095\n",
      "Epoch [901/1000], Loss: 188797328.51612905\n",
      "Saved model and hyperparameters to ./model_trained_lstm/fnn_model_2.pt\n",
      "\n",
      "Testing parameter combination 3/6: (32, 0.001, 1000, 3, 15)\n",
      "Epoch [1/1000], Loss: 215091581.41935483\n",
      "Epoch [101/1000], Loss: 211996274.58064517\n",
      "Epoch [201/1000], Loss: 209052805.67741936\n",
      "Epoch [301/1000], Loss: 206131355.3548387\n",
      "Epoch [401/1000], Loss: 203230709.67741936\n",
      "Epoch [501/1000], Loss: 200350897.5483871\n",
      "Epoch [601/1000], Loss: 197491921.03225806\n",
      "Epoch [701/1000], Loss: 194653778.06451613\n",
      "Epoch [801/1000], Loss: 191836464.0\n",
      "Epoch [901/1000], Loss: 189040089.80645162\n",
      "Saved model and hyperparameters to ./model_trained_lstm/fnn_model_3.pt\n",
      "\n",
      "Testing parameter combination 4/6: (32, 0.002, 1000, 3, 5)\n",
      "Epoch [1/1000], Loss: 213587009.5\n",
      "Epoch [101/1000], Loss: 207543458.5\n",
      "Epoch [201/1000], Loss: 201749460.5\n",
      "Epoch [301/1000], Loss: 195946632.0\n",
      "Epoch [401/1000], Loss: 190133914.5\n",
      "Epoch [501/1000], Loss: 184425755.0\n",
      "Epoch [601/1000], Loss: 178806817.5\n",
      "Epoch [701/1000], Loss: 173277122.0\n",
      "Epoch [801/1000], Loss: 167836230.25\n",
      "Epoch [901/1000], Loss: 162484040.0\n",
      "Saved model and hyperparameters to ./model_trained_lstm/fnn_model_4.pt\n",
      "\n",
      "Testing parameter combination 5/6: (32, 0.002, 1000, 3, 10)\n",
      "Epoch [1/1000], Loss: 214824083.0967742\n",
      "Epoch [101/1000], Loss: 208787374.96774194\n",
      "Epoch [201/1000], Loss: 202979868.9032258\n",
      "Epoch [301/1000], Loss: 197257913.29032257\n",
      "Epoch [401/1000], Loss: 191619317.67741936\n",
      "Epoch [501/1000], Loss: 186064328.2580645\n",
      "Epoch [601/1000], Loss: 180591951.48387095\n",
      "Epoch [701/1000], Loss: 175203225.80645162\n",
      "Epoch [801/1000], Loss: 169897716.6451613\n",
      "Epoch [901/1000], Loss: 164675308.38709676\n",
      "Saved model and hyperparameters to ./model_trained_lstm/fnn_model_5.pt\n",
      "\n",
      "Testing parameter combination 6/6: (32, 0.002, 1000, 3, 15)\n",
      "Epoch [1/1000], Loss: 215078062.4516129\n",
      "Epoch [101/1000], Loss: 209027193.29032257\n",
      "Epoch [201/1000], Loss: 203216226.06451613\n",
      "Epoch [301/1000], Loss: 197490909.93548387\n",
      "Epoch [401/1000], Loss: 191849082.83870968\n",
      "Epoch [501/1000], Loss: 186290930.58064517\n",
      "Epoch [601/1000], Loss: 180815676.9032258\n",
      "Epoch [701/1000], Loss: 175423830.19354838\n",
      "Epoch [801/1000], Loss: 170114995.0967742\n",
      "Epoch [901/1000], Loss: 164889772.38709676\n",
      "Saved model and hyperparameters to ./model_trained_lstm/fnn_model_6.pt\n",
      "\n",
      "Best model: {'hidden_size': 32, 'learning_rate': 0.002, 'num_epochs': 1000, 'loss': 162484040.0, 'losses': [213587009.5, 207543458.5, 201749460.5, 195946632.0, 190133914.5, 184425755.0, 178806817.5, 173277122.0, 167836230.25, 162484040.0]}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [25], line 148\u001B[0m\n\u001B[0;32m    145\u001B[0m best_model \u001B[38;5;241m=\u001B[39m hyperparameter_training_loop(data, hyperparams)\n\u001B[0;32m    147\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest model: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_model\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 148\u001B[0m \u001B[43mplot_loss_evolution\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbest_model\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlosses\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marange\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mbest_model\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnum_epochs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbest_model\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlearning_rate\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbest_model\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhidden_size\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [25], line 117\u001B[0m, in \u001B[0;36mplot_loss_evolution\u001B[1;34m(losses, num_epochs, learning_rate, hidden_size)\u001B[0m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplot_loss_evolution\u001B[39m(losses, num_epochs, learning_rate, hidden_size):\n\u001B[1;32m--> 117\u001B[0m     epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    118\u001B[0m     plt\u001B[38;5;241m.\u001B[39mplot(epochs, losses, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLoss\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    119\u001B[0m     plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "from LSTM_model import *\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def hyperparameter_training_loop(data, hyperparams):\n",
    "\n",
    "    # Generate all possible combinations of hyperparameters\n",
    "    combinations = list(itertools.product(*hyperparams.values()))\n",
    "\n",
    "    overall_best_model = {\"hidden_size\": None,\n",
    "                         \"learning_rate\": None,\n",
    "                         \"num_epochs\": None,\n",
    "                         \"loss\": float('inf')}\n",
    "\n",
    "    # Iterate over each hyperparameter combination\n",
    "    for i, params in enumerate(combinations):\n",
    "        print(f\"Testing parameter combination {i+1}/{len(combinations)}: {params}\")\n",
    "\n",
    "        # Create a new model with the current hyperparameters\n",
    "        hidden_size = params[0]\n",
    "        learning_rate = params[1]\n",
    "        num_epochs = params[2]\n",
    "        num_layers = params[3]\n",
    "        sequence_length = params[4]\n",
    "\n",
    "        dataloader = create_dataloader(data, sequence_length=sequence_length, batch_size=32, shuffle=False)\n",
    "\n",
    "        model = LSTMNetwork(1, hidden_size, num_layers)\n",
    "        losses = train(model, dataloader, num_epochs, learning_rate)\n",
    "\n",
    "        # Save the model and hyperparameters to a file\n",
    "        result = {\n",
    "            'hyperparameters': {\n",
    "                'hidden_size': hidden_size,\n",
    "                'learning_rate': learning_rate,\n",
    "                'num_epochs': num_epochs,\n",
    "                'num_layers': num_layers,\n",
    "                'sequence_length': sequence_length,\n",
    "                'best_loss': losses[-1],\n",
    "                \"losses\": losses\n",
    "            }\n",
    "        }\n",
    "        if losses[-1] < overall_best_model[\"loss\"]:\n",
    "            overall_best_model[\"hidden_size\"] = params[0]\n",
    "            overall_best_model[\"learning_rate\"] = params[1]\n",
    "            overall_best_model[\"num_epochs\"] = params[2]\n",
    "            overall_best_model[\"loss\"] = losses[-1]\n",
    "            overall_best_model[\"losses\"] = losses\n",
    "\n",
    "        filename = f\"./model_trained_lstm/fnn_model_{i+1}.pt\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(result, f)\n",
    "\n",
    "        print(f\"Saved model and hyperparameters to {filename}\\n\")\n",
    "\n",
    "    return overall_best_model\n",
    "\n",
    "\n",
    "# Create the DataLoader for first principal component\n",
    "data = np.array(data)[0]\n",
    "print(\"Data : {} + shape: {} + type: {}\".format(data, data.shape, type(data)))\n",
    "\n",
    "# Hyperparameter search space\n",
    "hyperparams = {\n",
    "    'hidden_size': [32],\n",
    "    'learning_rate': [0.001, 0.002],\n",
    "    'num_epochs': [1000],\n",
    "    'num_layers': [3],\n",
    "    'sequence_length': [5, 10, 15]\n",
    "}\n",
    "\n",
    "#Train the model\n",
    "#train(model, dataloader, num_epochs, learning_rate)\n",
    "best_model = hyperparameter_training_loop(data, hyperparams)\n",
    "\n",
    "print(f\"Best model: {best_model}\")\n",
    "plot_loss_evolution(best_model[\"losses\"], np.arange(0,best_model[\"num_epochs\"], 100, dtype=int), best_model[\"learning_rate\"], best_model[\"hidden_size\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}