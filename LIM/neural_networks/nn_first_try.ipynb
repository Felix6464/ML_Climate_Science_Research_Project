{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from LIM import utils as ut\n",
    "from LIM import LIM_class\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import itertools\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "#plt.style.use(\"../plotting.mplstyle\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\n",
    "data = xr.open_dataset(\"C:/Users/felix/PycharmProjects/ML_Climate_Science_Research_Project/LIM/data/ts_Amon_CESM2_piControl_r1i1p1f1.nc\")[\"ts\"]\n",
    "#data = xr.open_dataset(\"./data/zos_Amon_CESM2_piControl_r1i1p1f1.nc\")[\"zos\"]\n",
    "#data_old = xr.open_dataset(\"./data/ssta_1950_2021.nc\")[\"ssta\"]\n",
    "mask = xr.open_dataset(\"C:/Users/felix/PycharmProjects/ML_Climate_Science_Research_Project/LIM/data/sftlf_fx_CESM2_historical_r1i1p1f1.nc\")[\"sftlf\"]\n",
    "\n",
    "#14400 orginial size\n",
    "data = data[:200, :, :]\n",
    "\n",
    "data = ut.apply_mask(mask, data)\n",
    "#print(\"Data : {} + shape {}\".format(data, data.shape))\n",
    "\n",
    "data_anomalies = ut.calculate_monthly_anomalies(data)\n",
    "#print(\"Month mean : {} + shape : {}\".format(data_anomalies, data_anomalies.shape))\n",
    "\n",
    "data_cropped =ut.crop_xarray(data_anomalies)\n",
    "#print(\"Data cropped : {} + shape : {}\".format(data_cropped, data_cropped.shape))\n",
    "\n",
    "\n",
    "pca_10 = ut.SpatioTemporalPCA(data_cropped, n_components=20)\n",
    "#pca_10 = ut.SpatioTemporalPCA(data_anomalies, n_components=20)\n",
    "eof_10 = pca_10.eofs()\n",
    "pc_10 = pca_10.principal_components()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix has negative values!\n"
     ]
    }
   ],
   "source": [
    "# Create training and test data\n",
    "data = pca_10.principal_components()\n",
    "index_train = int(0.8 * len(data[\"time\"]))\n",
    "data_train = data[:, :index_train]\n",
    "data_test = data[:, index_train:]\n",
    "\n",
    "# Creating an example LIM object\n",
    "tau = 1\n",
    "model = LIM_class.LIM(tau)\n",
    "model.fit(data_train.data)\n",
    "#print(\"Data train : {} + shape: {} + type: {}\".format(data_train.data[:5], data_train.data.shape, type(data_train.data)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data : [[  5.3271937    8.385421    11.919489   ...   0.54701316 -13.858885\n",
      "  -13.140837  ]\n",
      " [ 14.023859    14.754921     9.171422   ...  10.982141    13.614319\n",
      "   15.255447  ]\n",
      " [ -3.981345     5.4285164    4.3772383  ... -14.726523    -5.7204304\n",
      "   -2.127743  ]\n",
      " ...\n",
      " [ -6.0170465   -2.3966491   -0.2913321  ...   0.37381372   2.4381244\n",
      "    2.961527  ]\n",
      " [  0.9753295    4.08532     -0.08006045 ...  -4.477956    -1.5456988\n",
      "   -2.2537868 ]\n",
      " [  0.7908709   -1.8077799   -1.2615875  ...   1.1779681   -1.9372445\n",
      "   -1.7934109 ]] + shape: (20, 200) + type: <class 'numpy.ndarray'>\n",
      "Testing parameter combination 1/4: (32, 0.01, 3000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: Error detected in AddmmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 982, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2885, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3139, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3318, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3378, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Temp\\ipykernel_6552\\1361453568.py\", line 76, in <module>\n",
      "    best_model = hyperparameter_training_loop(dataloader, input_size, output_size)\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Temp\\ipykernel_6552\\1361453568.py\", line 26, in hyperparameter_training_loop\n",
      "    losses = train(model, dataloader, num_epochs, learning_rate, input_size, target_len)\n",
      "  File \"C:\\Users\\felix\\PycharmProjects\\ML_Climate_Science_Research_Project\\LIM\\neural_networks\\FNN_model.py\", line 65, in train\n",
      "    outputs = model(inputs)\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\felix\\PycharmProjects\\ML_Climate_Science_Research_Project\\LIM\\neural_networks\\FNN_model.py\", line 34, in forward\n",
      "    x = self.fc1(x)\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\fx\\traceback.py\", line 57, in format_stack\n",
      "    return traceback.format_stack()\n",
      " (Triggered internally at ..\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:119.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 80]], which is output 0 of AsStridedBackward0, is at version 80; expected version 79 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [12], line 76\u001B[0m\n\u001B[0;32m     72\u001B[0m output_size \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;66;03m#print(\"Input size : {}\".format(input_size))\u001B[39;00m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;66;03m#train(model, dataloader, num_epochs, learning_rate)\u001B[39;00m\n\u001B[1;32m---> 76\u001B[0m best_model \u001B[38;5;241m=\u001B[39m \u001B[43mhyperparameter_training_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest model: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_model\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn [12], line 26\u001B[0m, in \u001B[0;36mhyperparameter_training_loop\u001B[1;34m(dataloader, input_size, output_size, target_len)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m     25\u001B[0m model \u001B[38;5;241m=\u001B[39m FeedforwardNetwork(input_size, hidden_size, output_size)\n\u001B[1;32m---> 26\u001B[0m losses \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_len\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# Save the model and hyperparameters to a file\u001B[39;00m\n\u001B[0;32m     29\u001B[0m result \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhyperparameters\u001B[39m\u001B[38;5;124m'\u001B[39m: {\n\u001B[0;32m     31\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhidden_size\u001B[39m\u001B[38;5;124m'\u001B[39m: params[\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     36\u001B[0m     }\n\u001B[0;32m     37\u001B[0m }\n",
      "File \u001B[1;32m~\\PycharmProjects\\ML_Climate_Science_Research_Project\\LIM\\neural_networks\\FNN_model.py:72\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, dataloader, num_epochs, learning_rate, target_len, criterion)\u001B[0m\n\u001B[0;32m     70\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, targets)\n\u001B[0;32m     71\u001B[0m \u001B[38;5;66;03m#print(\"Loss : {} + type : {} \".format(loss, type(loss)))\u001B[39;00m\n\u001B[1;32m---> 72\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     73\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     74\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 80]], which is output 0 of AsStridedBackward0, is at version 80; expected version 79 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "from FNN_model import *\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def hyperparameter_training_loop(dataloader, input_size, output_size, target_len=4):\n",
    "\n",
    "    # Generate all possible combinations of hyperparameters\n",
    "    combinations = list(itertools.product(*hyperparams.values()))\n",
    "\n",
    "    overall_best_model = {\"hidden_size\": None,\n",
    "                         \"learning_rate\": None,\n",
    "                         \"num_epochs\": None,\n",
    "                         \"loss\": float('inf')}\n",
    "\n",
    "    # Iterate over each hyperparameter combination\n",
    "    for i, params in enumerate(combinations):\n",
    "        print(f\"Testing parameter combination {i+1}/{len(combinations)}: {params}\")\n",
    "\n",
    "        hidden_size = params[0]\n",
    "        learning_rate = params[1]\n",
    "        num_epochs = params[2]\n",
    "\n",
    "        # Train the model\n",
    "        model = FeedforwardNetwork(input_size, hidden_size, output_size)\n",
    "        losses = train(model, dataloader, num_epochs, learning_rate, input_size, target_len)\n",
    "\n",
    "        # Save the model and hyperparameters to a file\n",
    "        result = {\n",
    "            'hyperparameters': {\n",
    "                'hidden_size': params[0],\n",
    "                'learning_rate': params[1],\n",
    "                'num_epochs': params[2],\n",
    "                'best_loss': losses[-1],\n",
    "                \"losses\": losses,\n",
    "            }\n",
    "        }\n",
    "        if losses[-1] < overall_best_model[\"loss\"]:\n",
    "            overall_best_model[\"hidden_size\"] = params[0]\n",
    "            overall_best_model[\"learning_rate\"] = params[1]\n",
    "            overall_best_model[\"num_epochs\"] = params[2]\n",
    "            overall_best_model[\"loss\"] = losses[-1]\n",
    "            overall_best_model[\"losses\"] = losses\n",
    "\n",
    "        filename = f\"./model_trained/fnn_model_{i+1}.pt\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(result, f)\n",
    "\n",
    "        print(f\"Saved model and hyperparameters to {filename}\\n\")\n",
    "\n",
    "    return overall_best_model\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "sequence_length = 4\n",
    "target_len = 4\n",
    "\n",
    "# Create the DataLoader for first principal component\n",
    "data = np.array(data)[:, :1000]\n",
    "print(\"Data : {} + shape: {} + type: {}\".format(data, data.shape, type(data)))\n",
    "dataloader = create_dataloader(data, batch_size, sequence_length, target_len)\n",
    "\n",
    "# Hyperparameter search space\n",
    "hyperparams = {\n",
    "    'hidden_size': [32, 64],\n",
    "    'learning_rate': [0.01, 0.001],\n",
    "    'num_epochs': [3000]\n",
    "}\n",
    "\n",
    "#Train the model\n",
    "input_size =  data.shape[0] * sequence_length\n",
    "output_size = data.shape[0] * 1\n",
    "\n",
    "#print(\"Input size : {}\".format(input_size))\n",
    "#train(model, dataloader, num_epochs, learning_rate)\n",
    "best_model = hyperparameter_training_loop(dataloader, input_size, output_size)\n",
    "\n",
    "print(f\"Best model: {best_model}\")\n",
    "#plot_loss_evolution(best_model[\"losses\"], np.arange(0,best_model[\"num_epochs\"], dtype=int), best_model[\"learning_rate\"], best_model[\"hidden_size\"])\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
