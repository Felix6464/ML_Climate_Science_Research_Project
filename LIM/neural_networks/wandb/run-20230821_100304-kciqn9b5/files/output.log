
 17%|████████████████▋                                                                                 | 17/100 [00:01<00:07, 11.69it/s, loss_test=1.097]
Epoch: 00, Training Loss: 0.9950, Test Loss: 1.0784
Epoch: 01, Training Loss: 0.9894, Test Loss: 1.0874
Epoch: 02, Training Loss: 0.9882, Test Loss: 1.0878
Epoch: 03, Training Loss: 0.9926, Test Loss: 1.0962
Epoch: 04, Training Loss: 0.9908, Test Loss: 1.0775
Epoch: 05, Training Loss: 0.9905, Test Loss: 1.0805
Epoch: 06, Training Loss: 0.9928, Test Loss: 1.0676
Epoch: 07, Training Loss: 0.9916, Test Loss: 1.0722
Epoch: 08, Training Loss: 0.9870, Test Loss: 1.0676
Epoch: 09, Training Loss: 0.9894, Test Loss: 1.0872
Epoch: 10, Training Loss: 0.9881, Test Loss: 1.0767
Epoch: 11, Training Loss: 0.9858, Test Loss: 1.0971
Epoch: 12, Training Loss: 0.9879, Test Loss: 1.0900
Epoch: 13, Training Loss: 0.9897, Test Loss: 1.0655
Epoch: 14, Training Loss: 0.9873, Test Loss: 1.0673
Epoch: 15, Training Loss: 0.9914, Test Loss: 1.0670
Epoch: 16, Training Loss: 0.9890, Test Loss: 1.0972
Epoch: 17, Training Loss: 0.9850, Test Loss: 1.0938

 41%|████████████████████████████████████████▏                                                         | 41/100 [00:03<00:04, 12.84it/s, loss_test=0.973]
Epoch: 19, Training Loss: 0.9820, Test Loss: 1.0835
Epoch: 20, Training Loss: 0.9822, Test Loss: 1.0877
Epoch: 21, Training Loss: 0.9826, Test Loss: 1.0875
Epoch: 22, Training Loss: 0.9815, Test Loss: 1.0692
Epoch: 23, Training Loss: 0.9818, Test Loss: 1.0693
Epoch: 24, Training Loss: 0.9877, Test Loss: 1.0874
Epoch: 25, Training Loss: 0.9816, Test Loss: 1.0740
Epoch: 26, Training Loss: 0.9810, Test Loss: 1.0846
Epoch: 27, Training Loss: 0.9735, Test Loss: 1.0818
Epoch: 28, Training Loss: 0.9750, Test Loss: 1.0862
Epoch: 29, Training Loss: 0.9772, Test Loss: 1.0801
Epoch: 30, Training Loss: 0.9752, Test Loss: 1.0595
Epoch: 31, Training Loss: 0.9668, Test Loss: 1.0685
Epoch: 32, Training Loss: 0.9642, Test Loss: 1.0696
Epoch: 33, Training Loss: 0.9542, Test Loss: 1.0604
Epoch: 34, Training Loss: 0.9431, Test Loss: 1.0450
Epoch: 35, Training Loss: 0.9331, Test Loss: 1.0418
Epoch: 36, Training Loss: 0.9273, Test Loss: 1.0231
Epoch: 37, Training Loss: 0.9202, Test Loss: 1.0121
Epoch: 38, Training Loss: 0.9094, Test Loss: 1.0086
Epoch: 39, Training Loss: 0.9056, Test Loss: 0.9981
Epoch: 40, Training Loss: 0.8989, Test Loss: 0.9736
Epoch: 41, Training Loss: 0.8939, Test Loss: 0.9732
Epoch: 42, Training Loss: 0.8871, Test Loss: 0.9584

 65%|███████████████████████████████████████████████████████████████▋                                  | 65/100 [00:05<00:02, 12.00it/s, loss_test=0.921]
Epoch: 44, Training Loss: 0.8788, Test Loss: 0.9628
Epoch: 45, Training Loss: 0.8717, Test Loss: 0.9604
Epoch: 46, Training Loss: 0.8689, Test Loss: 0.9607
Epoch: 47, Training Loss: 0.8630, Test Loss: 0.9472
Epoch: 48, Training Loss: 0.8610, Test Loss: 0.9784
Epoch: 49, Training Loss: 0.8535, Test Loss: 0.9595
Epoch: 50, Training Loss: 0.8508, Test Loss: 0.9629
Epoch: 51, Training Loss: 0.8439, Test Loss: 0.9678
Epoch: 52, Training Loss: 0.8432, Test Loss: 0.9466
Epoch: 53, Training Loss: 0.8362, Test Loss: 0.9476
Epoch: 54, Training Loss: 0.8300, Test Loss: 0.9583
Epoch: 55, Training Loss: 0.8286, Test Loss: 0.9380
Epoch: 56, Training Loss: 0.8198, Test Loss: 0.9436
Epoch: 57, Training Loss: 0.8173, Test Loss: 0.9415
Epoch: 58, Training Loss: 0.8117, Test Loss: 0.9611
Epoch: 59, Training Loss: 0.8135, Test Loss: 0.9336
Epoch: 60, Training Loss: 0.8080, Test Loss: 0.9351
Epoch: 61, Training Loss: 0.8026, Test Loss: 0.9461
Epoch: 62, Training Loss: 0.7993, Test Loss: 0.9347
Epoch: 63, Training Loss: 0.7964, Test Loss: 0.9392
Epoch: 64, Training Loss: 0.7938, Test Loss: 0.9324
Epoch: 65, Training Loss: 0.7856, Test Loss: 0.9210
Epoch: 66, Training Loss: 0.7833, Test Loss: 0.9358

 89%|███████████████████████████████████████████████████████████████████████████████████████▏          | 89/100 [00:07<00:00, 12.20it/s, loss_test=0.935]
Epoch: 68, Training Loss: 0.7822, Test Loss: 0.9121
Epoch: 69, Training Loss: 0.7690, Test Loss: 0.9254
Epoch: 70, Training Loss: 0.7711, Test Loss: 0.9141
Epoch: 71, Training Loss: 0.7712, Test Loss: 0.9254
Epoch: 72, Training Loss: 0.7693, Test Loss: 0.9126
Epoch: 73, Training Loss: 0.7656, Test Loss: 0.9175
Epoch: 74, Training Loss: 0.7590, Test Loss: 0.9133
Epoch: 75, Training Loss: 0.7528, Test Loss: 0.9201
Epoch: 76, Training Loss: 0.7529, Test Loss: 0.9264
Epoch: 77, Training Loss: 0.7527, Test Loss: 0.9187
Epoch: 78, Training Loss: 0.7475, Test Loss: 0.9063
Epoch: 79, Training Loss: 0.7467, Test Loss: 0.9168
Epoch: 80, Training Loss: 0.7420, Test Loss: 0.9216
Epoch: 81, Training Loss: 0.7371, Test Loss: 0.9101
Epoch: 82, Training Loss: 0.7352, Test Loss: 0.9282
Epoch: 83, Training Loss: 0.7295, Test Loss: 0.9171
Epoch: 84, Training Loss: 0.7254, Test Loss: 0.9261
Epoch: 85, Training Loss: 0.7233, Test Loss: 0.9225
Epoch: 86, Training Loss: 0.7210, Test Loss: 0.9147
Epoch: 87, Training Loss: 0.7175, Test Loss: 0.9248
Epoch: 88, Training Loss: 0.7143, Test Loss: 0.9168
Epoch: 89, Training Loss: 0.7104, Test Loss: 0.9348
Epoch: 90, Training Loss: 0.7041, Test Loss: 0.9071

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 11.66it/s, loss_test=0.908]
Epoch: 92, Training Loss: 0.7013, Test Loss: 0.9252
Epoch: 93, Training Loss: 0.6980, Test Loss: 0.9242
Epoch: 94, Training Loss: 0.6929, Test Loss: 0.9087
Epoch: 95, Training Loss: 0.6930, Test Loss: 0.9176
Epoch: 96, Training Loss: 0.6883, Test Loss: 0.9133
Epoch: 97, Training Loss: 0.6841, Test Loss: 0.9170
Epoch: 98, Training Loss: 0.6797, Test Loss: 0.9240
Epoch: 99, Training Loss: 0.6775, Test Loss: 0.9079
Model saved as model_4583595np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-121000-4583595np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 100, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9949690461158752, 0.9893866181373596, 0.9882249951362609, 0.992621374130249, 0.9908491492271423, 0.9904714226722717, 0.9927602052688599, 0.9916163086891174, 0.9870028257369995, 0.9894015669822693, 0.9881227135658264, 0.9857850670814514, 0.9878727674484253, 0.9896508097648621, 0.9872767210006714, 0.9913647294044494, 0.9890307068824769, 0.9849765419960022, 0.9920185089111329, 0.9820198893547059, 0.9821928024291993, 0.9826315641403198, 0.9815065622329712, 0.9818293213844299, 0.987658953666687, 0.9815888643264771, 0.98095144033432, 0.9735477447509766, 0.9750221252441407, 0.9771694540977478, 0.9752045750617981, 0.9667712926864624, 0.964170503616333, 0.9541960954666138, 0.9430818796157837, 0.9330962538719177, 0.9272526860237121, 0.9202026486396789, 0.9094478964805603, 0.9056418180465698, 0.8989448785781861, 0.893909502029419, 0.887138032913208, 0.8819503307342529, 0.8787754058837891, 0.8716820359230042, 0.8689269185066223, 0.8629885792732239, 0.8609981656074523, 0.8535441994667053, 0.8508116841316223, 0.8438585877418519, 0.8432270646095276, 0.8362366199493408, 0.8300322651863098, 0.8285754799842835, 0.8197614908218384, 0.8173462986946106, 0.8117408633232117, 0.8134518623352051, 0.8080239176750184, 0.8025980710983276, 0.7993427634239196, 0.7963788151741028, 0.793777072429657, 0.7855641126632691, 0.7833433151245117, 0.7818390846252441, 0.7821846961975097, 0.7690369367599488, 0.7710723638534546, 0.7711738586425781, 0.7693100214004517, 0.7656005144119262, 0.7590196967124939, 0.7528486847877502, 0.7528509736061096, 0.752688181400299, 0.747516393661499, 0.7467444777488709, 0.7419965863227844, 0.7370572924613953, 0.7351989388465882, 0.7295300245285035, 0.7254041790962219, 0.7232928037643432, 0.7209747314453125, 0.7175129294395447, 0.7142887711524963, 0.7104361653327942, 0.704111659526825, 0.7025948643684388, 0.701347041130066, 0.6980336785316468, 0.6929491758346558, 0.69297114610672, 0.6883156895637512, 0.6841313481330872, 0.6796878337860107, 0.6774762630462646], 'loss_test': [1.0783826112747192, 1.0874322652816772, 1.0878427028656006, 1.0962234735488892, 1.0775341987609863, 1.0805211067199707, 1.0676339864730835, 1.0721945762634277, 1.0676289796829224, 1.0871614217758179, 1.0767425298690796, 1.097099781036377, 1.0900473594665527, 1.065497636795044, 1.0673489570617676, 1.0670499801635742, 1.0971689224243164, 1.0938009023666382, 1.0868613719940186, 1.0834684371948242, 1.0877189636230469, 1.0874991416931152, 1.0691707134246826, 1.0693254470825195, 1.0874491930007935, 1.0740294456481934, 1.084592580795288, 1.0817670822143555, 1.0862444639205933, 1.0801411867141724, 1.0595437288284302, 1.0685051679611206, 1.0695769786834717, 1.060410737991333, 1.0450031757354736, 1.0418269634246826, 1.02311110496521, 1.0120735168457031, 1.008620262145996, 0.9980909824371338, 0.9735663533210754, 0.9732311964035034, 0.9583574533462524, 0.981448233127594, 0.9628126621246338, 0.9604197144508362, 0.9606884717941284, 0.947187066078186, 0.9783669710159302, 0.9595448970794678, 0.9629411101341248, 0.9677746295928955, 0.9465658664703369, 0.9475905895233154, 0.9582729935646057, 0.937993586063385, 0.9435630440711975, 0.9415473937988281, 0.9610921740531921, 0.9336252212524414, 0.9350706338882446, 0.9460720419883728, 0.934711217880249, 0.9391880035400391, 0.9323645830154419, 0.920975923538208, 0.9357578158378601, 0.9217873811721802, 0.9120787978172302, 0.9254260659217834, 0.9140861630439758, 0.9254019260406494, 0.9126202464103699, 0.9174832105636597, 0.9132711887359619, 0.9201438426971436, 0.9264419674873352, 0.9186683893203735, 0.906344473361969, 0.9167770743370056, 0.9216305613517761, 0.9100803136825562, 0.9281854033470154, 0.9170690178871155, 0.9260694980621338, 0.9225133657455444, 0.9147351980209351, 0.9247636795043945, 0.9168252348899841, 0.9347772002220154, 0.9070746898651123, 0.9202038645744324, 0.9251871705055237, 0.9242159724235535, 0.9087241291999817, 0.917631983757019, 0.9132797718048096, 0.9170055389404297, 0.9239879250526428, 0.9078575372695923], 'identifier': '4583595np'}