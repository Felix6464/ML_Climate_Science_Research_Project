
 20%|███████████████████▌                                                                              | 20/100 [00:01<00:06, 12.18it/s, loss_test=1.089]
Epoch: 00, Training Loss: 0.9929, Test Loss: 1.0852
Epoch: 01, Training Loss: 0.9914, Test Loss: 1.0933
Epoch: 02, Training Loss: 0.9916, Test Loss: 1.0993
Epoch: 03, Training Loss: 0.9903, Test Loss: 1.0924
Epoch: 04, Training Loss: 0.9888, Test Loss: 1.0942
Epoch: 05, Training Loss: 0.9953, Test Loss: 1.0724
Epoch: 06, Training Loss: 0.9900, Test Loss: 1.1038
Epoch: 07, Training Loss: 0.9870, Test Loss: 1.0878
Epoch: 08, Training Loss: 0.9900, Test Loss: 1.0957
Epoch: 09, Training Loss: 0.9883, Test Loss: 1.0831
Epoch: 10, Training Loss: 0.9906, Test Loss: 1.0997
Epoch: 11, Training Loss: 0.9948, Test Loss: 1.0768
Epoch: 12, Training Loss: 0.9879, Test Loss: 1.0825
Epoch: 13, Training Loss: 0.9889, Test Loss: 1.0681
Epoch: 14, Training Loss: 0.9840, Test Loss: 1.0946
Epoch: 15, Training Loss: 0.9846, Test Loss: 1.0876
Epoch: 16, Training Loss: 0.9914, Test Loss: 1.0808
Epoch: 17, Training Loss: 0.9878, Test Loss: 1.0859
Epoch: 18, Training Loss: 0.9869, Test Loss: 1.0813
Epoch: 19, Training Loss: 0.9863, Test Loss: 1.0887

 46%|█████████████████████████████████████████████                                                     | 46/100 [00:03<00:04, 11.98it/s, loss_test=1.013]
Epoch: 21, Training Loss: 0.9838, Test Loss: 1.0863
Epoch: 22, Training Loss: 0.9841, Test Loss: 1.0768
Epoch: 23, Training Loss: 0.9865, Test Loss: 1.0692
Epoch: 24, Training Loss: 0.9861, Test Loss: 1.0949
Epoch: 25, Training Loss: 0.9830, Test Loss: 1.0788
Epoch: 26, Training Loss: 0.9801, Test Loss: 1.0596
Epoch: 27, Training Loss: 0.9833, Test Loss: 1.0826
Epoch: 28, Training Loss: 0.9815, Test Loss: 1.0787
Epoch: 29, Training Loss: 0.9826, Test Loss: 1.0925
Epoch: 30, Training Loss: 0.9777, Test Loss: 1.0859
Epoch: 31, Training Loss: 0.9739, Test Loss: 1.0756
Epoch: 32, Training Loss: 0.9695, Test Loss: 1.0726
Epoch: 33, Training Loss: 0.9709, Test Loss: 1.0783
Epoch: 34, Training Loss: 0.9604, Test Loss: 1.0635
Epoch: 35, Training Loss: 0.9569, Test Loss: 1.0560
Epoch: 36, Training Loss: 0.9521, Test Loss: 1.0512
Epoch: 37, Training Loss: 0.9412, Test Loss: 1.0626
Epoch: 38, Training Loss: 0.9415, Test Loss: 1.0508
Epoch: 39, Training Loss: 0.9276, Test Loss: 1.0518
Epoch: 40, Training Loss: 0.9256, Test Loss: 1.0396
Epoch: 41, Training Loss: 0.9136, Test Loss: 1.0326
Epoch: 42, Training Loss: 0.9090, Test Loss: 1.0332
Epoch: 43, Training Loss: 0.9033, Test Loss: 1.0133
Epoch: 44, Training Loss: 0.8952, Test Loss: 1.0188
Epoch: 45, Training Loss: 0.8871, Test Loss: 1.0126
Epoch: 46, Training Loss: 0.8834, Test Loss: 1.0022
Epoch: 47, Training Loss: 0.8780, Test Loss: 1.0041
Epoch: 48, Training Loss: 0.8671, Test Loss: 0.9864
Epoch: 49, Training Loss: 0.8615, Test Loss: 0.9833
Epoch: 50, Training Loss: 0.8634, Test Loss: 0.9865
Epoch: 51, Training Loss: 0.8553, Test Loss: 0.9851
Epoch: 52, Training Loss: 0.8490, Test Loss: 0.9649
Epoch: 53, Training Loss: 0.8487, Test Loss: 0.9704
Epoch: 54, Training Loss: 0.8418, Test Loss: 0.9635
Epoch: 55, Training Loss: 0.8329, Test Loss: 0.9670
Epoch: 56, Training Loss: 0.8311, Test Loss: 0.9707
Epoch: 57, Training Loss: 0.8293, Test Loss: 0.9501
Epoch: 58, Training Loss: 0.8270, Test Loss: 0.9543
Epoch: 59, Training Loss: 0.8239, Test Loss: 0.9555
Epoch: 60, Training Loss: 0.8143, Test Loss: 0.9452
Epoch: 61, Training Loss: 0.8112, Test Loss: 0.9531
Epoch: 62, Training Loss: 0.8068, Test Loss: 0.9518
Epoch: 63, Training Loss: 0.8066, Test Loss: 0.9448
Epoch: 64, Training Loss: 0.7960, Test Loss: 0.9494
Epoch: 65, Training Loss: 0.7970, Test Loss: 0.9522
Epoch: 66, Training Loss: 0.7928, Test Loss: 0.9487
Epoch: 67, Training Loss: 0.7883, Test Loss: 0.9331
Epoch: 68, Training Loss: 0.7844, Test Loss: 0.9432
Epoch: 69, Training Loss: 0.7808, Test Loss: 0.9307

 70%|████████████████████████████████████████████████████████████████████▌                             | 70/100 [00:05<00:02, 12.21it/s, loss_test=0.940]
Epoch: 71, Training Loss: 0.7696, Test Loss: 0.9416
Epoch: 72, Training Loss: 0.7692, Test Loss: 0.9178
Epoch: 73, Training Loss: 0.7624, Test Loss: 0.9122
Epoch: 74, Training Loss: 0.7600, Test Loss: 0.9297
Epoch: 75, Training Loss: 0.7568, Test Loss: 0.9147
Epoch: 76, Training Loss: 0.7531, Test Loss: 0.9342
Epoch: 77, Training Loss: 0.7479, Test Loss: 0.9156
Epoch: 78, Training Loss: 0.7440, Test Loss: 0.9169
Epoch: 79, Training Loss: 0.7392, Test Loss: 0.9156
Epoch: 80, Training Loss: 0.7307, Test Loss: 0.9124
Epoch: 81, Training Loss: 0.7328, Test Loss: 0.9189
Epoch: 82, Training Loss: 0.7272, Test Loss: 0.9113
Epoch: 83, Training Loss: 0.7200, Test Loss: 0.9141
Epoch: 84, Training Loss: 0.7192, Test Loss: 0.9159
Epoch: 85, Training Loss: 0.7141, Test Loss: 0.9188
Epoch: 86, Training Loss: 0.7130, Test Loss: 0.9007
Epoch: 87, Training Loss: 0.7054, Test Loss: 0.9159
Epoch: 88, Training Loss: 0.7026, Test Loss: 0.9188
Epoch: 89, Training Loss: 0.6953, Test Loss: 0.9184
Epoch: 90, Training Loss: 0.6931, Test Loss: 0.9154
Epoch: 91, Training Loss: 0.6891, Test Loss: 0.9160
Epoch: 92, Training Loss: 0.6822, Test Loss: 0.9073
Epoch: 93, Training Loss: 0.6781, Test Loss: 0.9209


100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.29it/s, loss_test=0.919]
Epoch: 95, Training Loss: 0.6735, Test Loss: 0.9213
Epoch: 96, Training Loss: 0.6704, Test Loss: 0.9160
Epoch: 97, Training Loss: 0.6655, Test Loss: 0.9222
Epoch: 98, Training Loss: 0.6615, Test Loss: 0.9121
Epoch: 99, Training Loss: 0.6604, Test Loss: 0.9189
Model saved as model_1618852np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-12150000-1618852np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 100, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9929290175437927, 0.9913535237312316, 0.991570794582367, 0.9902982831001281, 0.9888396024703979, 0.9952574491500854, 0.9899695754051209, 0.9869741916656494, 0.9899885058403015, 0.9883204221725463, 0.9906384348869324, 0.994814932346344, 0.9878814697265625, 0.9888539910316467, 0.9840019583702088, 0.9846220374107361, 0.9913882970809936, 0.9878325223922729, 0.9868807792663574, 0.986255145072937, 0.9843432545661926, 0.9837917208671569, 0.9840914487838746, 0.9864518165588378, 0.9860687255859375, 0.9829843044281006, 0.9801191449165344, 0.98330157995224, 0.9815445899963379, 0.9826351642608643, 0.9776936411857605, 0.9739399671554565, 0.9694615244865418, 0.9709391832351685, 0.9604225516319275, 0.9568645000457764, 0.9520771265029907, 0.9411764979362488, 0.9414888262748718, 0.9275565862655639, 0.9255866408348083, 0.9135891795158386, 0.9089858412742615, 0.9033273220062256, 0.8952340841293335, 0.8870870471000671, 0.8834444642066955, 0.8779652953147888, 0.8670719027519226, 0.8615481376647949, 0.8634313225746155, 0.855279016494751, 0.8490310549736023, 0.8486977219581604, 0.8417903304100036, 0.8329043626785279, 0.8311463713645935, 0.8293205857276916, 0.8269815921783448, 0.8239201188087464, 0.8142922878265381, 0.8112038493156433, 0.8068438291549682, 0.80656898021698, 0.7959604740142823, 0.7970242142677307, 0.7927805781364441, 0.7883481025695801, 0.7843531608581543, 0.7808157920837402, 0.777381432056427, 0.7696417927742004, 0.7691598653793335, 0.7624481320381165, 0.7599552869796753, 0.7567736864089966, 0.7531003355979919, 0.747877299785614, 0.7440162777900696, 0.7391625165939331, 0.7307344794273376, 0.7327935457229614, 0.727234137058258, 0.7199975848197937, 0.7191777348518371, 0.7140602111816406, 0.7130298972129822, 0.7053836941719055, 0.7026279211044312, 0.6953348636627197, 0.6930739998817443, 0.689066219329834, 0.6822358846664429, 0.678113317489624, 0.6749653816223145, 0.6734689593315124, 0.6703949332237243, 0.6654780030250549, 0.6614538908004761, 0.6604370594024658], 'loss_test': [1.0851837396621704, 1.093332052230835, 1.0992708206176758, 1.0923937559127808, 1.0942394733428955, 1.0724393129348755, 1.1037706136703491, 1.0878307819366455, 1.0957077741622925, 1.083134412765503, 1.0996545553207397, 1.0768111944198608, 1.0824804306030273, 1.068079948425293, 1.094576120376587, 1.0875946283340454, 1.0808005332946777, 1.0859395265579224, 1.0812647342681885, 1.0887213945388794, 1.0889164209365845, 1.0863286256790161, 1.0768423080444336, 1.0692418813705444, 1.094864845275879, 1.0787564516067505, 1.059601068496704, 1.0826194286346436, 1.0787262916564941, 1.0924596786499023, 1.0859450101852417, 1.075628638267517, 1.072576642036438, 1.0783056020736694, 1.0634769201278687, 1.0560238361358643, 1.0511586666107178, 1.0625691413879395, 1.0508474111557007, 1.051804780960083, 1.0395914316177368, 1.0326064825057983, 1.033171534538269, 1.0133132934570312, 1.0188437700271606, 1.0126031637191772, 1.0021538734436035, 1.004064917564392, 0.9863574504852295, 0.9832897186279297, 0.9864946603775024, 0.9851117134094238, 0.9648687243461609, 0.9704149961471558, 0.9635225534439087, 0.9669766426086426, 0.9706982970237732, 0.9500647783279419, 0.9542852640151978, 0.9554955363273621, 0.9452468156814575, 0.953064501285553, 0.9518246054649353, 0.9448044300079346, 0.949397623538971, 0.9522148966789246, 0.9487406611442566, 0.9331285357475281, 0.943152666091919, 0.9307410717010498, 0.9404942989349365, 0.9416047930717468, 0.9178444743156433, 0.9121802449226379, 0.9297027587890625, 0.9147257804870605, 0.9342074990272522, 0.9155704379081726, 0.9168640375137329, 0.9156477451324463, 0.9123538136482239, 0.9189023375511169, 0.9112899303436279, 0.9141335487365723, 0.915945291519165, 0.9187588095664978, 0.9007307291030884, 0.9159343242645264, 0.9188446402549744, 0.9184049963951111, 0.9154000282287598, 0.9159647822380066, 0.9072548151016235, 0.9208880066871643, 0.9200252890586853, 0.9212633967399597, 0.9159888029098511, 0.9222425818443298, 0.9121459126472473, 0.9188693165779114], 'identifier': '1618852np'}