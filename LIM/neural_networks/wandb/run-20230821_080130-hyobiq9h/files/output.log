
  8%|███████▊                                                                                          | 20/250 [00:01<00:19, 11.83it/s, loss_test=1.080]
Epoch: 00, Training Loss: 0.9898, Test Loss: 1.0896
Epoch: 01, Training Loss: 0.9924, Test Loss: 1.0798
Epoch: 02, Training Loss: 0.9951, Test Loss: 1.0840
Epoch: 03, Training Loss: 0.9965, Test Loss: 1.0777
Epoch: 04, Training Loss: 0.9917, Test Loss: 1.0673
Epoch: 05, Training Loss: 0.9908, Test Loss: 1.0773
Epoch: 06, Training Loss: 0.9898, Test Loss: 1.0868
Epoch: 07, Training Loss: 0.9934, Test Loss: 1.0894
Epoch: 08, Training Loss: 0.9879, Test Loss: 1.0771
Epoch: 09, Training Loss: 0.9866, Test Loss: 1.0850
Epoch: 10, Training Loss: 0.9899, Test Loss: 1.0703
Epoch: 11, Training Loss: 0.9915, Test Loss: 1.0921
Epoch: 12, Training Loss: 0.9810, Test Loss: 1.0811
Epoch: 13, Training Loss: 0.9833, Test Loss: 1.0786
Epoch: 14, Training Loss: 0.9891, Test Loss: 1.0835
Epoch: 15, Training Loss: 0.9882, Test Loss: 1.0782
Epoch: 16, Training Loss: 0.9891, Test Loss: 1.0912
Epoch: 17, Training Loss: 0.9896, Test Loss: 1.0862
Epoch: 18, Training Loss: 0.9925, Test Loss: 1.0768

 18%|█████████████████▏                                                                                | 44/250 [00:03<00:16, 12.44it/s, loss_test=0.991]
Epoch: 20, Training Loss: 0.9863, Test Loss: 1.0674
Epoch: 21, Training Loss: 0.9876, Test Loss: 1.0897
Epoch: 22, Training Loss: 0.9836, Test Loss: 1.0709
Epoch: 23, Training Loss: 0.9801, Test Loss: 1.0936
Epoch: 24, Training Loss: 0.9842, Test Loss: 1.0830
Epoch: 25, Training Loss: 0.9804, Test Loss: 1.0831
Epoch: 26, Training Loss: 0.9790, Test Loss: 1.0863
Epoch: 27, Training Loss: 0.9807, Test Loss: 1.0896
Epoch: 28, Training Loss: 0.9713, Test Loss: 1.0678
Epoch: 29, Training Loss: 0.9682, Test Loss: 1.0903
Epoch: 30, Training Loss: 0.9692, Test Loss: 1.0698
Epoch: 31, Training Loss: 0.9633, Test Loss: 1.0613
Epoch: 32, Training Loss: 0.9523, Test Loss: 1.0657
Epoch: 33, Training Loss: 0.9469, Test Loss: 1.0393
Epoch: 34, Training Loss: 0.9363, Test Loss: 1.0681
Epoch: 35, Training Loss: 0.9317, Test Loss: 1.0464
Epoch: 36, Training Loss: 0.9195, Test Loss: 1.0284
Epoch: 37, Training Loss: 0.9165, Test Loss: 1.0399
Epoch: 38, Training Loss: 0.9109, Test Loss: 1.0211
Epoch: 39, Training Loss: 0.9053, Test Loss: 1.0240
Epoch: 40, Training Loss: 0.8965, Test Loss: 0.9976
Epoch: 41, Training Loss: 0.8964, Test Loss: 0.9928
Epoch: 42, Training Loss: 0.8925, Test Loss: 0.9960

 27%|██████████████████████████▋                                                                       | 68/250 [00:05<00:15, 11.95it/s, loss_test=0.926]
Epoch: 44, Training Loss: 0.8793, Test Loss: 0.9832
Epoch: 45, Training Loss: 0.8791, Test Loss: 0.9837
Epoch: 46, Training Loss: 0.8731, Test Loss: 0.9605
Epoch: 47, Training Loss: 0.8676, Test Loss: 0.9742
Epoch: 48, Training Loss: 0.8632, Test Loss: 0.9619
Epoch: 49, Training Loss: 0.8571, Test Loss: 0.9844
Epoch: 50, Training Loss: 0.8520, Test Loss: 0.9605
Epoch: 51, Training Loss: 0.8498, Test Loss: 0.9724
Epoch: 52, Training Loss: 0.8437, Test Loss: 0.9559
Epoch: 53, Training Loss: 0.8421, Test Loss: 0.9607
Epoch: 54, Training Loss: 0.8345, Test Loss: 0.9654
Epoch: 55, Training Loss: 0.8305, Test Loss: 0.9477
Epoch: 56, Training Loss: 0.8236, Test Loss: 0.9599
Epoch: 57, Training Loss: 0.8224, Test Loss: 0.9512
Epoch: 58, Training Loss: 0.8160, Test Loss: 0.9501
Epoch: 59, Training Loss: 0.8144, Test Loss: 0.9598
Epoch: 60, Training Loss: 0.8094, Test Loss: 0.9551
Epoch: 61, Training Loss: 0.8068, Test Loss: 0.9217
Epoch: 62, Training Loss: 0.8042, Test Loss: 0.9577
Epoch: 63, Training Loss: 0.8004, Test Loss: 0.9390
Epoch: 64, Training Loss: 0.7940, Test Loss: 0.9522
Epoch: 65, Training Loss: 0.7938, Test Loss: 0.9262
Epoch: 66, Training Loss: 0.7879, Test Loss: 0.9389
Epoch: 67, Training Loss: 0.7895, Test Loss: 0.9540

 37%|████████████████████████████████████                                                              | 92/250 [00:07<00:13, 11.47it/s, loss_test=0.908]
Epoch: 69, Training Loss: 0.7791, Test Loss: 0.9313
Epoch: 70, Training Loss: 0.7774, Test Loss: 0.9468
Epoch: 71, Training Loss: 0.7731, Test Loss: 0.9216
Epoch: 72, Training Loss: 0.7714, Test Loss: 0.9340
Epoch: 73, Training Loss: 0.7656, Test Loss: 0.9370
Epoch: 74, Training Loss: 0.7624, Test Loss: 0.9327
Epoch: 75, Training Loss: 0.7617, Test Loss: 0.9300
Epoch: 76, Training Loss: 0.7562, Test Loss: 0.9218
Epoch: 77, Training Loss: 0.7557, Test Loss: 0.9280
Epoch: 78, Training Loss: 0.7525, Test Loss: 0.9275
Epoch: 79, Training Loss: 0.7459, Test Loss: 0.9316
Epoch: 80, Training Loss: 0.7468, Test Loss: 0.9386
Epoch: 81, Training Loss: 0.7426, Test Loss: 0.9188
Epoch: 82, Training Loss: 0.7374, Test Loss: 0.9250
Epoch: 83, Training Loss: 0.7310, Test Loss: 0.9286
Epoch: 84, Training Loss: 0.7288, Test Loss: 0.9256
Epoch: 85, Training Loss: 0.7278, Test Loss: 0.9173
Epoch: 86, Training Loss: 0.7217, Test Loss: 0.9134
Epoch: 87, Training Loss: 0.7169, Test Loss: 0.9278
Epoch: 88, Training Loss: 0.7185, Test Loss: 0.9233
Epoch: 89, Training Loss: 0.7146, Test Loss: 0.9047
Epoch: 90, Training Loss: 0.7095, Test Loss: 0.9152
Epoch: 91, Training Loss: 0.7063, Test Loss: 0.9040

 46%|█████████████████████████████████████████████                                                    | 116/250 [00:09<00:11, 12.10it/s, loss_test=0.905]
Epoch: 93, Training Loss: 0.6970, Test Loss: 0.8883
Epoch: 94, Training Loss: 0.6972, Test Loss: 0.9057
Epoch: 95, Training Loss: 0.6901, Test Loss: 0.9069
Epoch: 96, Training Loss: 0.6883, Test Loss: 0.9214
Epoch: 97, Training Loss: 0.6858, Test Loss: 0.9011
Epoch: 98, Training Loss: 0.6810, Test Loss: 0.9071
Epoch: 99, Training Loss: 0.6794, Test Loss: 0.8900
Epoch: 100, Training Loss: 0.6759, Test Loss: 0.9004
Epoch: 101, Training Loss: 0.6727, Test Loss: 0.9037
Epoch: 102, Training Loss: 0.6640, Test Loss: 0.8964
Epoch: 103, Training Loss: 0.6669, Test Loss: 0.8879
Epoch: 104, Training Loss: 0.6592, Test Loss: 0.8980
Epoch: 105, Training Loss: 0.6588, Test Loss: 0.9052
Epoch: 106, Training Loss: 0.6553, Test Loss: 0.9038
Epoch: 107, Training Loss: 0.6491, Test Loss: 0.8993
Epoch: 108, Training Loss: 0.6471, Test Loss: 0.9095
Epoch: 109, Training Loss: 0.6440, Test Loss: 0.9054
Epoch: 110, Training Loss: 0.6401, Test Loss: 0.8940
Epoch: 111, Training Loss: 0.6343, Test Loss: 0.8914
Epoch: 112, Training Loss: 0.6339, Test Loss: 0.8996
Epoch: 113, Training Loss: 0.6310, Test Loss: 0.9001
Epoch: 114, Training Loss: 0.6253, Test Loss: 0.9008
Epoch: 115, Training Loss: 0.6226, Test Loss: 0.8926
Epoch: 116, Training Loss: 0.6197, Test Loss: 0.9007

 56%|██████████████████████████████████████████████████████▎                                          | 140/250 [00:11<00:09, 11.97it/s, loss_test=0.898]
Epoch: 118, Training Loss: 0.6124, Test Loss: 0.8952
Epoch: 119, Training Loss: 0.6081, Test Loss: 0.8932
Epoch: 120, Training Loss: 0.6068, Test Loss: 0.9026
Epoch: 121, Training Loss: 0.6028, Test Loss: 0.8937
Epoch: 122, Training Loss: 0.5979, Test Loss: 0.8949
Epoch: 123, Training Loss: 0.5942, Test Loss: 0.8917
Epoch: 124, Training Loss: 0.5921, Test Loss: 0.8958
Epoch: 125, Training Loss: 0.5885, Test Loss: 0.9038
Epoch: 126, Training Loss: 0.5857, Test Loss: 0.8950
Epoch: 127, Training Loss: 0.5801, Test Loss: 0.9116
Epoch: 128, Training Loss: 0.5787, Test Loss: 0.9033
Epoch: 129, Training Loss: 0.5750, Test Loss: 0.9052
Epoch: 130, Training Loss: 0.5718, Test Loss: 0.9053
Epoch: 131, Training Loss: 0.5685, Test Loss: 0.8957
Epoch: 132, Training Loss: 0.5645, Test Loss: 0.9025
Epoch: 133, Training Loss: 0.5616, Test Loss: 0.9044
Epoch: 134, Training Loss: 0.5544, Test Loss: 0.8918
Epoch: 135, Training Loss: 0.5563, Test Loss: 0.8877
Epoch: 136, Training Loss: 0.5510, Test Loss: 0.8912
Epoch: 137, Training Loss: 0.5506, Test Loss: 0.8946
Epoch: 138, Training Loss: 0.5451, Test Loss: 0.8961
Epoch: 139, Training Loss: 0.5445, Test Loss: 0.8945

 66%|███████████████████████████████████████████████████████████████▋                                 | 164/250 [00:13<00:07, 12.16it/s, loss_test=0.912]
Epoch: 141, Training Loss: 0.5361, Test Loss: 0.9032
Epoch: 142, Training Loss: 0.5326, Test Loss: 0.8964
Epoch: 143, Training Loss: 0.5325, Test Loss: 0.9013
Epoch: 144, Training Loss: 0.5278, Test Loss: 0.9038
Epoch: 145, Training Loss: 0.5246, Test Loss: 0.8952
Epoch: 146, Training Loss: 0.5226, Test Loss: 0.9037
Epoch: 147, Training Loss: 0.5193, Test Loss: 0.9106
Epoch: 148, Training Loss: 0.5171, Test Loss: 0.9149
Epoch: 149, Training Loss: 0.5148, Test Loss: 0.9139
Epoch: 150, Training Loss: 0.5108, Test Loss: 0.9128
Epoch: 151, Training Loss: 0.5088, Test Loss: 0.9147
Epoch: 152, Training Loss: 0.5054, Test Loss: 0.9117
Epoch: 153, Training Loss: 0.5023, Test Loss: 0.9048
Epoch: 154, Training Loss: 0.5002, Test Loss: 0.9024
Epoch: 155, Training Loss: 0.4969, Test Loss: 0.9050
Epoch: 156, Training Loss: 0.4953, Test Loss: 0.9250
Epoch: 157, Training Loss: 0.4915, Test Loss: 0.9107
Epoch: 158, Training Loss: 0.4899, Test Loss: 0.9159
Epoch: 159, Training Loss: 0.4874, Test Loss: 0.9121
Epoch: 160, Training Loss: 0.4843, Test Loss: 0.9165
Epoch: 161, Training Loss: 0.4812, Test Loss: 0.9137
Epoch: 162, Training Loss: 0.4790, Test Loss: 0.9109
Epoch: 163, Training Loss: 0.4772, Test Loss: 0.9202

 76%|█████████████████████████████████████████████████████████████████████████▋                       | 190/250 [00:15<00:04, 12.21it/s, loss_test=0.925]
Epoch: 165, Training Loss: 0.4708, Test Loss: 0.9030
Epoch: 166, Training Loss: 0.4700, Test Loss: 0.9139
Epoch: 167, Training Loss: 0.4640, Test Loss: 0.9195
Epoch: 168, Training Loss: 0.4649, Test Loss: 0.9186
Epoch: 169, Training Loss: 0.4626, Test Loss: 0.9120
Epoch: 170, Training Loss: 0.4597, Test Loss: 0.9166
Epoch: 171, Training Loss: 0.4545, Test Loss: 0.9312
Epoch: 172, Training Loss: 0.4542, Test Loss: 0.9177
Epoch: 173, Training Loss: 0.4525, Test Loss: 0.9130
Epoch: 174, Training Loss: 0.4490, Test Loss: 0.9140
Epoch: 175, Training Loss: 0.4467, Test Loss: 0.9207
Epoch: 176, Training Loss: 0.4449, Test Loss: 0.9198
Epoch: 177, Training Loss: 0.4425, Test Loss: 0.9246
Epoch: 178, Training Loss: 0.4397, Test Loss: 0.9196
Epoch: 179, Training Loss: 0.4370, Test Loss: 0.9216
Epoch: 180, Training Loss: 0.4365, Test Loss: 0.9142
Epoch: 181, Training Loss: 0.4338, Test Loss: 0.9097
Epoch: 182, Training Loss: 0.4334, Test Loss: 0.9303
Epoch: 183, Training Loss: 0.4285, Test Loss: 0.9252
Epoch: 184, Training Loss: 0.4276, Test Loss: 0.9296
Epoch: 185, Training Loss: 0.4228, Test Loss: 0.9389
Epoch: 186, Training Loss: 0.4217, Test Loss: 0.9290
Epoch: 187, Training Loss: 0.4201, Test Loss: 0.9192
Epoch: 188, Training Loss: 0.4193, Test Loss: 0.9295

 86%|███████████████████████████████████████████████████████████████████████████████████              | 214/250 [00:17<00:02, 12.06it/s, loss_test=0.964]
Epoch: 190, Training Loss: 0.4131, Test Loss: 0.9221
Epoch: 191, Training Loss: 0.4106, Test Loss: 0.9414
Epoch: 192, Training Loss: 0.4102, Test Loss: 0.9271
Epoch: 193, Training Loss: 0.4077, Test Loss: 0.9476
Epoch: 194, Training Loss: 0.4063, Test Loss: 0.9288
Epoch: 195, Training Loss: 0.4037, Test Loss: 0.9351
Epoch: 196, Training Loss: 0.4024, Test Loss: 0.9497
Epoch: 197, Training Loss: 0.3982, Test Loss: 0.9388
Epoch: 198, Training Loss: 0.3980, Test Loss: 0.9443
Epoch: 199, Training Loss: 0.3968, Test Loss: 0.9336
Epoch: 200, Training Loss: 0.3939, Test Loss: 0.9543
Epoch: 201, Training Loss: 0.3918, Test Loss: 0.9400
Epoch: 202, Training Loss: 0.3915, Test Loss: 0.9380
Epoch: 203, Training Loss: 0.3885, Test Loss: 0.9220
Epoch: 204, Training Loss: 0.3864, Test Loss: 0.9258
Epoch: 205, Training Loss: 0.3843, Test Loss: 0.9495
Epoch: 206, Training Loss: 0.3826, Test Loss: 0.9357
Epoch: 207, Training Loss: 0.3795, Test Loss: 0.9456
Epoch: 208, Training Loss: 0.3777, Test Loss: 0.9426
Epoch: 209, Training Loss: 0.3756, Test Loss: 0.9355
Epoch: 210, Training Loss: 0.3744, Test Loss: 0.9329
Epoch: 211, Training Loss: 0.3742, Test Loss: 0.9467
Epoch: 212, Training Loss: 0.3706, Test Loss: 0.9580

 95%|████████████████████████████████████████████████████████████████████████████████████████████▎    | 238/250 [00:19<00:00, 12.50it/s, loss_test=0.990]
Epoch: 214, Training Loss: 0.3654, Test Loss: 0.9481
Epoch: 215, Training Loss: 0.3660, Test Loss: 0.9556
Epoch: 216, Training Loss: 0.3630, Test Loss: 0.9579
Epoch: 217, Training Loss: 0.3613, Test Loss: 0.9573
Epoch: 218, Training Loss: 0.3595, Test Loss: 0.9533
Epoch: 219, Training Loss: 0.3584, Test Loss: 0.9695
Epoch: 220, Training Loss: 0.3553, Test Loss: 0.9652
Epoch: 221, Training Loss: 0.3533, Test Loss: 0.9620
Epoch: 222, Training Loss: 0.3532, Test Loss: 0.9771
Epoch: 223, Training Loss: 0.3517, Test Loss: 0.9649
Epoch: 224, Training Loss: 0.3497, Test Loss: 0.9612
Epoch: 225, Training Loss: 0.3483, Test Loss: 0.9557
Epoch: 226, Training Loss: 0.3475, Test Loss: 0.9624
Epoch: 227, Training Loss: 0.3453, Test Loss: 0.9659
Epoch: 228, Training Loss: 0.3425, Test Loss: 0.9719
Epoch: 229, Training Loss: 0.3413, Test Loss: 0.9740
Epoch: 230, Training Loss: 0.3395, Test Loss: 0.9674
Epoch: 231, Training Loss: 0.3373, Test Loss: 0.9691
Epoch: 232, Training Loss: 0.3368, Test Loss: 0.9609
Epoch: 233, Training Loss: 0.3341, Test Loss: 0.9615
Epoch: 234, Training Loss: 0.3318, Test Loss: 0.9667
Epoch: 235, Training Loss: 0.3307, Test Loss: 0.9713
Epoch: 236, Training Loss: 0.3298, Test Loss: 0.9776
Epoch: 237, Training Loss: 0.3283, Test Loss: 0.9819

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.04it/s, loss_test=0.973]
Epoch: 239, Training Loss: 0.3252, Test Loss: 0.9556
Epoch: 240, Training Loss: 0.3244, Test Loss: 0.9762
Epoch: 241, Training Loss: 0.3221, Test Loss: 0.9789
Epoch: 242, Training Loss: 0.3210, Test Loss: 0.9716
Epoch: 243, Training Loss: 0.3205, Test Loss: 0.9827
Epoch: 244, Training Loss: 0.3187, Test Loss: 0.9808
Epoch: 245, Training Loss: 0.3155, Test Loss: 0.9685
Epoch: 246, Training Loss: 0.3151, Test Loss: 0.9771
Epoch: 247, Training Loss: 0.3126, Test Loss: 0.9942
Epoch: 248, Training Loss: 0.3108, Test Loss: 0.9928
Epoch: 249, Training Loss: 0.3117, Test Loss: 0.9726
Model saved as model_133216np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-12160000-133216np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.989788007736206, 0.9924022436141968, 0.995098614692688, 0.9964800357818604, 0.9917433023452759, 0.9907581686973572, 0.989806878566742, 0.9933850646018982, 0.987941038608551, 0.9866077661514282, 0.9899348735809326, 0.9915442824363708, 0.9809856534004211, 0.9832891941070556, 0.9891347885131836, 0.9882379293441772, 0.9891158223152161, 0.9896445870399475, 0.9924915432929993, 0.9844202160835266, 0.9862901926040649, 0.9876005291938782, 0.9835723042488098, 0.9800690412521362, 0.9842439651489258, 0.9804209232330322, 0.9790265083312988, 0.9806620478630066, 0.9712672710418702, 0.9682031989097595, 0.969159722328186, 0.9633005619049072, 0.952277410030365, 0.9469227075576783, 0.9363394737243652, 0.9316903233528138, 0.9195067882537842, 0.9165433406829834, 0.9108759641647339, 0.9052916526794433, 0.8964674830436706, 0.8963517785072327, 0.8925170779228211, 0.884542977809906, 0.8793381571769714, 0.8790857315063476, 0.8730520963668823, 0.8676441669464111, 0.8632364630699157, 0.8571159839630127, 0.851976478099823, 0.8498330116271973, 0.8436779499053955, 0.8421485304832459, 0.8345257759094238, 0.8304713010787964, 0.8235604643821717, 0.8223561406135559, 0.8159786820411682, 0.8143629193305969, 0.8094214916229248, 0.8068397283554077, 0.8041562080383301, 0.800351369380951, 0.7940032124519348, 0.7937500476837158, 0.7879461646080017, 0.7894772171974183, 0.7844641923904419, 0.7790514826774597, 0.7773521780967713, 0.7731150031089783, 0.7714069843292236, 0.7655769348144531, 0.7623510122299194, 0.7616634607315064, 0.7561697006225586, 0.7557433605194092, 0.7525473475456238, 0.7459429740905762, 0.7467910289764405, 0.742611825466156, 0.7374035477638244, 0.7310072660446167, 0.7288291931152344, 0.7278014898300171, 0.7217373490333557, 0.7168880343437195, 0.7184839487075806, 0.714596438407898, 0.7094578385353089, 0.7063239216804504, 0.7037375092506408, 0.6969940423965454, 0.6972028970718384, 0.69011971950531, 0.688315498828888, 0.6857679963111878, 0.6809845328330993, 0.6793782830238342, 0.6759122967720032, 0.6726610541343689, 0.6639986038208008, 0.666892671585083, 0.6592342138290406, 0.658841359615326, 0.655288302898407, 0.6490901350975037, 0.6470874547958374, 0.6439851284027099, 0.6400811553001404, 0.634276294708252, 0.6339138507843017, 0.6309610724449157, 0.6252594232559204, 0.6225569128990174, 0.6196585655212402, 0.6173502802848816, 0.6123823165893555, 0.608063530921936, 0.6068387031555176, 0.6027984142303466, 0.597912073135376, 0.5942429065704345, 0.5920669078826905, 0.5885170459747314, 0.5856820344924927, 0.580141294002533, 0.578740394115448, 0.574952495098114, 0.5717789173126221, 0.5684839248657226, 0.5644881963729859, 0.5616258025169373, 0.5543985843658448, 0.5562699437141418, 0.5509503483772278, 0.5505851030349731, 0.5450554728507996, 0.5445018887519837, 0.5387973189353943, 0.5360881209373474, 0.5326377868652343, 0.5325233697891235, 0.5278356909751892, 0.5246351003646851, 0.5226027965545654, 0.5192653894424438, 0.5170756101608276, 0.514794921875, 0.5107623159885406, 0.5087585687637329, 0.5053923010826111, 0.5023384034633637, 0.5002379179000854, 0.4968685984611511, 0.4953393578529358, 0.49152691960334777, 0.48988271355628965, 0.4874271035194397, 0.48425145745277404, 0.481236058473587, 0.4790214002132416, 0.4771639883518219, 0.4714984893798828, 0.47076529264450073, 0.4700358331203461, 0.46398247480392457, 0.46491732001304625, 0.46262031197547915, 0.45974661111831666, 0.45447691679000857, 0.4541689336299896, 0.4525037705898285, 0.44901983737945556, 0.4466945052146912, 0.4449082612991333, 0.4424731910228729, 0.43967894911766053, 0.4370439171791077, 0.43646361827850344, 0.4338338851928711, 0.4333544969558716, 0.42854554653167726, 0.4275874435901642, 0.42281281352043154, 0.4217035710811615, 0.42006675004959104, 0.4192878842353821, 0.4153605461120605, 0.41312687993049624, 0.4106473684310913, 0.41017374992370603, 0.4076991736888885, 0.40629830956459045, 0.4037380337715149, 0.40236295461654664, 0.3982404410839081, 0.3979815483093262, 0.39680007100105286, 0.3938521146774292, 0.3917999804019928, 0.391473650932312, 0.3885402798652649, 0.3863945245742798, 0.3843372941017151, 0.3826434910297394, 0.379477721452713, 0.37768843173980715, 0.37562755346298216, 0.3744488418102264, 0.37421164512634275, 0.37055298686027527, 0.369255393743515, 0.3654410421848297, 0.3659904956817627, 0.36295061111450194, 0.3613264083862305, 0.3594791293144226, 0.358391147851944, 0.3553291618824005, 0.3533112287521362, 0.35318618416786196, 0.3516893148422241, 0.3497407019138336, 0.34825239777565004, 0.3474806547164917, 0.34533199667930603, 0.3424612462520599, 0.3412907004356384, 0.3395178198814392, 0.33734241127967834, 0.33682324886322024, 0.3341392457485199, 0.3318370580673218, 0.33067975640296937, 0.3298120439052582, 0.32834903001785276, 0.3270002841949463, 0.32515950202941896, 0.32435148358345034, 0.32209216356277465, 0.3209968090057373, 0.3204787850379944, 0.318724524974823, 0.3155476152896881, 0.3151206374168396, 0.3126359462738037, 0.31082996129989626, 0.3116722762584686], 'loss_test': [1.0896095037460327, 1.0798193216323853, 1.084000587463379, 1.077706217765808, 1.0672677755355835, 1.0772985219955444, 1.0868244171142578, 1.0894420146942139, 1.0770747661590576, 1.0849647521972656, 1.070260763168335, 1.0920673608779907, 1.0811289548873901, 1.0786453485488892, 1.0834813117980957, 1.0782133340835571, 1.091151237487793, 1.0861892700195312, 1.0767710208892822, 1.0798500776290894, 1.0673962831497192, 1.0897432565689087, 1.07089364528656, 1.0935944318771362, 1.0829575061798096, 1.0830951929092407, 1.0862663984298706, 1.0895822048187256, 1.0678128004074097, 1.090320110321045, 1.069751501083374, 1.0613198280334473, 1.0657482147216797, 1.039273977279663, 1.0681203603744507, 1.046399474143982, 1.028385043144226, 1.0398979187011719, 1.0210577249526978, 1.0240230560302734, 0.9975504875183105, 0.9927819967269897, 0.9960136413574219, 0.99134761095047, 0.9831833839416504, 0.9836776852607727, 0.9604670405387878, 0.9741994738578796, 0.9619132280349731, 0.9843952059745789, 0.9604858756065369, 0.9724492430686951, 0.9558789134025574, 0.9606562256813049, 0.9654184579849243, 0.9477291703224182, 0.9599367380142212, 0.9512336254119873, 0.9500553011894226, 0.9598331451416016, 0.9550539255142212, 0.9216871857643127, 0.9576762318611145, 0.9389858841896057, 0.9521572589874268, 0.9262109994888306, 0.9388527274131775, 0.9540066123008728, 0.9256775379180908, 0.9313451647758484, 0.9468011856079102, 0.9216366410255432, 0.9339800477027893, 0.9369518756866455, 0.9327000975608826, 0.9300289750099182, 0.9217617511749268, 0.9280358552932739, 0.9274625182151794, 0.931571364402771, 0.9386401772499084, 0.9188308715820312, 0.9249515533447266, 0.9285849928855896, 0.9256380200386047, 0.9173097610473633, 0.9134353995323181, 0.9277734756469727, 0.9233357310295105, 0.9047247171401978, 0.9151809215545654, 0.9040122628211975, 0.9075605869293213, 0.8882958292961121, 0.9057081937789917, 0.9069193601608276, 0.9214015007019043, 0.9011495113372803, 0.9070650935173035, 0.8899577260017395, 0.9003915190696716, 0.9037024974822998, 0.8964072465896606, 0.887875497341156, 0.8980342745780945, 0.9052468538284302, 0.9038298726081848, 0.899260401725769, 0.9094765782356262, 0.9054247736930847, 0.8939814567565918, 0.8914270997047424, 0.8996065855026245, 0.9000936150550842, 0.9008079171180725, 0.8926467895507812, 0.9006824493408203, 0.9054877758026123, 0.8952361345291138, 0.8932086825370789, 0.902592122554779, 0.8937389850616455, 0.8949235677719116, 0.8917356729507446, 0.8958262205123901, 0.9037908315658569, 0.8950082063674927, 0.9115920662879944, 0.9033234119415283, 0.9052490592002869, 0.9053214192390442, 0.8957258462905884, 0.9024763703346252, 0.9044159054756165, 0.8917894959449768, 0.8876585364341736, 0.891231119632721, 0.8946153521537781, 0.8960944414138794, 0.8945017457008362, 0.8984616994857788, 0.9031550288200378, 0.8964498043060303, 0.9013167023658752, 0.9037688374519348, 0.8952316641807556, 0.9037123322486877, 0.9106119275093079, 0.9149144887924194, 0.9139235019683838, 0.9128023386001587, 0.9147390127182007, 0.9116722345352173, 0.9047935009002686, 0.9023841023445129, 0.9050325155258179, 0.9249810576438904, 0.9106777906417847, 0.9159404635429382, 0.9121487140655518, 0.9164861440658569, 0.9137337803840637, 0.910866379737854, 0.9202346205711365, 0.9119362831115723, 0.9029890298843384, 0.9138741493225098, 0.9194620847702026, 0.9186175465583801, 0.9119764566421509, 0.9165958166122437, 0.9311567544937134, 0.9176534414291382, 0.9130359292030334, 0.9139543771743774, 0.9206610321998596, 0.919800341129303, 0.924591600894928, 0.9196391105651855, 0.9215807914733887, 0.9141790270805359, 0.9097076654434204, 0.9302949905395508, 0.9252128601074219, 0.9296274781227112, 0.9388965368270874, 0.9289612174034119, 0.9192165732383728, 0.9294975399971008, 0.9253683686256409, 0.9221000075340271, 0.9414413571357727, 0.927137553691864, 0.9475787281990051, 0.9288008213043213, 0.9350757598876953, 0.9497268795967102, 0.9387839436531067, 0.944302499294281, 0.933612585067749, 0.9543211460113525, 0.9399921298027039, 0.9379984736442566, 0.9220475554466248, 0.9258396029472351, 0.9494829177856445, 0.9357331991195679, 0.9455881714820862, 0.9426495432853699, 0.9355405569076538, 0.9329098463058472, 0.9467138051986694, 0.9579756259918213, 0.9641650915145874, 0.948100209236145, 0.9556441903114319, 0.9578714966773987, 0.9572640657424927, 0.9533433318138123, 0.9694790840148926, 0.9651730060577393, 0.9619516134262085, 0.9771182537078857, 0.9649037718772888, 0.9612444043159485, 0.9557041525840759, 0.9623607397079468, 0.9658762812614441, 0.9719021320343018, 0.9739617705345154, 0.967350959777832, 0.969075620174408, 0.9609137773513794, 0.9615104794502258, 0.9667221903800964, 0.9713265299797058, 0.9775753617286682, 0.9819353222846985, 0.9897116422653198, 0.9556098580360413, 0.9761608839035034, 0.9788572192192078, 0.9716252088546753, 0.9827272295951843, 0.9808341264724731, 0.9685094356536865, 0.9771155714988708, 0.9941616654396057, 0.992811918258667, 0.972612202167511], 'identifier': '133216np'}