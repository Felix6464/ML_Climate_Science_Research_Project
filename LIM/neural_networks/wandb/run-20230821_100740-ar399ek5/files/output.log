
 22%|█████████████████████▌                                                                            | 22/100 [00:01<00:06, 12.08it/s, loss_test=1.075]
Epoch: 00, Training Loss: 0.9917, Test Loss: 1.0809
Epoch: 01, Training Loss: 0.9938, Test Loss: 1.0840
Epoch: 02, Training Loss: 0.9902, Test Loss: 1.0881
Epoch: 03, Training Loss: 0.9896, Test Loss: 1.0875
Epoch: 04, Training Loss: 0.9903, Test Loss: 1.0746
Epoch: 05, Training Loss: 0.9933, Test Loss: 1.0789
Epoch: 06, Training Loss: 0.9922, Test Loss: 1.0828
Epoch: 07, Training Loss: 0.9851, Test Loss: 1.0955
Epoch: 08, Training Loss: 0.9882, Test Loss: 1.0968
Epoch: 09, Training Loss: 0.9864, Test Loss: 1.0821
Epoch: 10, Training Loss: 0.9895, Test Loss: 1.0722
Epoch: 11, Training Loss: 0.9915, Test Loss: 1.0744
Epoch: 12, Training Loss: 0.9894, Test Loss: 1.0802
Epoch: 13, Training Loss: 0.9848, Test Loss: 1.0893
Epoch: 14, Training Loss: 0.9867, Test Loss: 1.0725
Epoch: 15, Training Loss: 0.9856, Test Loss: 1.0759
Epoch: 16, Training Loss: 0.9820, Test Loss: 1.0976
Epoch: 17, Training Loss: 0.9859, Test Loss: 1.0699
Epoch: 18, Training Loss: 0.9857, Test Loss: 1.0705
Epoch: 19, Training Loss: 0.9820, Test Loss: 1.0740
Epoch: 20, Training Loss: 0.9756, Test Loss: 1.0777
Epoch: 21, Training Loss: 0.9802, Test Loss: 1.0753
Epoch: 22, Training Loss: 0.9844, Test Loss: 1.0847
Epoch: 23, Training Loss: 0.9789, Test Loss: 1.0931
Epoch: 24, Training Loss: 0.9803, Test Loss: 1.0878
Epoch: 25, Training Loss: 0.9804, Test Loss: 1.0621
Epoch: 26, Training Loss: 0.9747, Test Loss: 1.0835
Epoch: 27, Training Loss: 0.9747, Test Loss: 1.0908
Epoch: 28, Training Loss: 0.9724, Test Loss: 1.0619
Epoch: 29, Training Loss: 0.9648, Test Loss: 1.0598
Epoch: 30, Training Loss: 0.9598, Test Loss: 1.0474
Epoch: 31, Training Loss: 0.9563, Test Loss: 1.0793
Epoch: 32, Training Loss: 0.9452, Test Loss: 1.0510
Epoch: 33, Training Loss: 0.9324, Test Loss: 1.0445
Epoch: 34, Training Loss: 0.9270, Test Loss: 1.0460
Epoch: 35, Training Loss: 0.9256, Test Loss: 1.0338
Epoch: 36, Training Loss: 0.9165, Test Loss: 1.0377
Epoch: 37, Training Loss: 0.9095, Test Loss: 1.0169
Epoch: 38, Training Loss: 0.9025, Test Loss: 1.0305
Epoch: 39, Training Loss: 0.8976, Test Loss: 1.0056
Epoch: 40, Training Loss: 0.8970, Test Loss: 0.9857
Epoch: 41, Training Loss: 0.8946, Test Loss: 0.9965
Epoch: 42, Training Loss: 0.8906, Test Loss: 0.9872
Epoch: 43, Training Loss: 0.8942, Test Loss: 0.9959
Epoch: 44, Training Loss: 0.8857, Test Loss: 0.9899
Epoch: 45, Training Loss: 0.8845, Test Loss: 0.9792


 72%|██████████████████████████████████████████████████████████████████████▌                           | 72/100 [00:05<00:02, 12.40it/s, loss_test=0.957]
Epoch: 47, Training Loss: 0.8772, Test Loss: 0.9864
Epoch: 48, Training Loss: 0.8739, Test Loss: 0.9745
Epoch: 49, Training Loss: 0.8752, Test Loss: 0.9792
Epoch: 50, Training Loss: 0.8696, Test Loss: 0.9766
Epoch: 51, Training Loss: 0.8674, Test Loss: 0.9612
Epoch: 52, Training Loss: 0.8642, Test Loss: 0.9657
Epoch: 53, Training Loss: 0.8591, Test Loss: 0.9469
Epoch: 54, Training Loss: 0.8576, Test Loss: 0.9657
Epoch: 55, Training Loss: 0.8558, Test Loss: 0.9823
Epoch: 56, Training Loss: 0.8518, Test Loss: 0.9546
Epoch: 57, Training Loss: 0.8513, Test Loss: 0.9718
Epoch: 58, Training Loss: 0.8491, Test Loss: 0.9692
Epoch: 59, Training Loss: 0.8432, Test Loss: 0.9645
Epoch: 60, Training Loss: 0.8437, Test Loss: 0.9580
Epoch: 61, Training Loss: 0.8359, Test Loss: 0.9563
Epoch: 62, Training Loss: 0.8376, Test Loss: 0.9481
Epoch: 63, Training Loss: 0.8324, Test Loss: 0.9386
Epoch: 64, Training Loss: 0.8312, Test Loss: 0.9543
Epoch: 65, Training Loss: 0.8295, Test Loss: 0.9613
Epoch: 66, Training Loss: 0.8294, Test Loss: 0.9446
Epoch: 67, Training Loss: 0.8226, Test Loss: 0.9472
Epoch: 68, Training Loss: 0.8216, Test Loss: 0.9539
Epoch: 69, Training Loss: 0.8201, Test Loss: 0.9188
Epoch: 70, Training Loss: 0.8172, Test Loss: 0.9299

 96%|██████████████████████████████████████████████████████████████████████████████████████████████    | 96/100 [00:07<00:00, 12.29it/s, loss_test=0.912]
Epoch: 72, Training Loss: 0.8116, Test Loss: 0.9339
Epoch: 73, Training Loss: 0.8058, Test Loss: 0.9368
Epoch: 74, Training Loss: 0.8060, Test Loss: 0.9419
Epoch: 75, Training Loss: 0.8060, Test Loss: 0.9318
Epoch: 76, Training Loss: 0.7990, Test Loss: 0.9317
Epoch: 77, Training Loss: 0.7977, Test Loss: 0.9303
Epoch: 78, Training Loss: 0.7966, Test Loss: 0.9359
Epoch: 79, Training Loss: 0.7906, Test Loss: 0.9304
Epoch: 80, Training Loss: 0.7892, Test Loss: 0.9220
Epoch: 81, Training Loss: 0.7881, Test Loss: 0.9348
Epoch: 82, Training Loss: 0.7860, Test Loss: 0.9243
Epoch: 83, Training Loss: 0.7785, Test Loss: 0.9300
Epoch: 84, Training Loss: 0.7791, Test Loss: 0.9365
Epoch: 85, Training Loss: 0.7767, Test Loss: 0.9297
Epoch: 86, Training Loss: 0.7694, Test Loss: 0.9390
Epoch: 87, Training Loss: 0.7671, Test Loss: 0.9329
Epoch: 88, Training Loss: 0.7645, Test Loss: 0.9277
Epoch: 89, Training Loss: 0.7593, Test Loss: 0.9263
Epoch: 90, Training Loss: 0.7603, Test Loss: 0.9271
Epoch: 91, Training Loss: 0.7553, Test Loss: 0.9256
Epoch: 92, Training Loss: 0.7513, Test Loss: 0.9169
Epoch: 93, Training Loss: 0.7529, Test Loss: 0.9170
Epoch: 94, Training Loss: 0.7470, Test Loss: 0.9213

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.27it/s, loss_test=0.916]
Epoch: 96, Training Loss: 0.7343, Test Loss: 0.9383
Epoch: 97, Training Loss: 0.7327, Test Loss: 0.9074
Epoch: 98, Training Loss: 0.7311, Test Loss: 0.9244
Epoch: 99, Training Loss: 0.7287, Test Loss: 0.9165
Model saved as model_1272788np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-1260000-1272788np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 100, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9916766881942749, 0.9937826633453369, 0.9901664972305297, 0.9896293759346009, 0.990287697315216, 0.9932728290557862, 0.9921899676322937, 0.9850656986236572, 0.9882336378097534, 0.9864173293113708, 0.9894812226295471, 0.9915200233459472, 0.9893726944923401, 0.984805703163147, 0.9867349982261657, 0.9856306552886963, 0.9820292115211486, 0.985910177230835, 0.9856731653213501, 0.982037341594696, 0.9756048917770386, 0.9801989674568177, 0.9843614816665649, 0.9788600444793701, 0.9803424596786499, 0.9803954243659974, 0.9746676445007324, 0.9746925473213196, 0.9723984479904175, 0.9648088335990905, 0.9598101735115051, 0.9562561869621277, 0.9451793551445007, 0.9323799848556519, 0.9270168304443359, 0.9255887985229492, 0.9165481209754944, 0.9095056414604187, 0.9024831056594849, 0.8975651860237122, 0.8970440626144409, 0.8945989847183228, 0.8905931234359741, 0.8941646933555603, 0.8856768846511841, 0.8844650149345398, 0.8817080378532409, 0.8772106885910034, 0.8739022612571716, 0.8751638293266296, 0.8695657134056092, 0.8674031257629394, 0.8642093300819397, 0.8591402053833008, 0.8575573563575745, 0.8558122515678406, 0.8517993330955506, 0.8512506008148193, 0.8491000533103943, 0.8432170033454895, 0.8436881542205811, 0.8358743548393249, 0.8376302719116211, 0.8323854804039001, 0.8311501026153565, 0.8294730544090271, 0.829356563091278, 0.8226466655731202, 0.8215630531311036, 0.8201311111450196, 0.8172428011894226, 0.8128424286842346, 0.8116137146949768, 0.8057846069335938, 0.8060277342796326, 0.805954110622406, 0.7989940881729126, 0.7976590156555176, 0.7966462016105652, 0.7905528783798218, 0.7891757130622864, 0.7880792140960693, 0.7860225319862366, 0.7784941911697387, 0.7791229963302613, 0.7766536116600037, 0.7694067239761353, 0.7671319603919983, 0.7645347595214844, 0.7593419671058654, 0.7602571368217468, 0.7553306102752686, 0.7512680888175964, 0.7529468059539794, 0.7470389842987061, 0.7427542209625244, 0.734345555305481, 0.7327436327934265, 0.7310672879219056, 0.7286894798278809], 'loss_test': [1.0808730125427246, 1.0839906930923462, 1.088092565536499, 1.087536334991455, 1.0746002197265625, 1.0788848400115967, 1.0827940702438354, 1.095475673675537, 1.0967634916305542, 1.0820704698562622, 1.0721793174743652, 1.0744109153747559, 1.0802165269851685, 1.0893216133117676, 1.0724960565567017, 1.075932502746582, 1.0975691080093384, 1.0698742866516113, 1.0704814195632935, 1.0739655494689941, 1.0776904821395874, 1.0753003358840942, 1.0847136974334717, 1.0931236743927002, 1.0877701044082642, 1.0621193647384644, 1.0834521055221558, 1.0908458232879639, 1.0619087219238281, 1.059754490852356, 1.0474315881729126, 1.079295039176941, 1.0509971380233765, 1.0444755554199219, 1.0459946393966675, 1.033794641494751, 1.0376818180084229, 1.0168761014938354, 1.0305231809616089, 1.0055900812149048, 0.9857293963432312, 0.9964956045150757, 0.9872487783432007, 0.9959490895271301, 0.9899088144302368, 0.9792236685752869, 0.9761199951171875, 0.9863975644111633, 0.9744685888290405, 0.9792409539222717, 0.9765632152557373, 0.9612195491790771, 0.9657032489776611, 0.946867048740387, 0.9656726717948914, 0.9823117256164551, 0.9545789957046509, 0.9717707633972168, 0.969155490398407, 0.9645074009895325, 0.9580371379852295, 0.9562572836875916, 0.9480715394020081, 0.938635528087616, 0.9542856216430664, 0.9613100290298462, 0.9445788860321045, 0.9471579194068909, 0.9539085626602173, 0.9188404679298401, 0.9299228191375732, 0.9565874338150024, 0.9338599443435669, 0.9368070363998413, 0.9418525695800781, 0.9318233132362366, 0.9317254424095154, 0.9303278923034668, 0.9358819127082825, 0.9304206371307373, 0.9220129251480103, 0.9348252415657043, 0.9242764115333557, 0.9299871921539307, 0.9365128874778748, 0.9297181963920593, 0.9389828443527222, 0.9328886270523071, 0.9276841878890991, 0.926307201385498, 0.9270992875099182, 0.9255794882774353, 0.9168537855148315, 0.9169800877571106, 0.9212656617164612, 0.9123917818069458, 0.9383288025856018, 0.9074104428291321, 0.9244155883789062, 0.9164783954620361], 'identifier': '1272788np'}