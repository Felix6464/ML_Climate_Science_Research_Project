
  8%|███████▊                                                                                          | 20/250 [00:01<00:19, 11.92it/s, loss_test=1.079]
Epoch: 00, Training Loss: 0.9927, Test Loss: 1.1067
Epoch: 01, Training Loss: 0.9945, Test Loss: 1.0859
Epoch: 02, Training Loss: 0.9900, Test Loss: 1.0838
Epoch: 03, Training Loss: 0.9908, Test Loss: 1.0799
Epoch: 04, Training Loss: 0.9916, Test Loss: 1.0695
Epoch: 05, Training Loss: 0.9893, Test Loss: 1.0804
Epoch: 06, Training Loss: 0.9896, Test Loss: 1.0792
Epoch: 07, Training Loss: 0.9850, Test Loss: 1.0748
Epoch: 08, Training Loss: 0.9899, Test Loss: 1.0790
Epoch: 09, Training Loss: 0.9858, Test Loss: 1.0841
Epoch: 10, Training Loss: 0.9858, Test Loss: 1.0994
Epoch: 11, Training Loss: 0.9878, Test Loss: 1.0925
Epoch: 12, Training Loss: 0.9919, Test Loss: 1.0736
Epoch: 13, Training Loss: 0.9862, Test Loss: 1.0838
Epoch: 14, Training Loss: 0.9835, Test Loss: 1.0828
Epoch: 15, Training Loss: 0.9844, Test Loss: 1.0877
Epoch: 16, Training Loss: 0.9868, Test Loss: 1.0851
Epoch: 17, Training Loss: 0.9853, Test Loss: 1.0732
Epoch: 18, Training Loss: 0.9855, Test Loss: 1.0839
Epoch: 19, Training Loss: 0.9785, Test Loss: 1.0830

 18%|█████████████████▏                                                                                | 44/250 [00:03<00:17, 11.91it/s, loss_test=0.985]
Epoch: 21, Training Loss: 0.9873, Test Loss: 1.0722
Epoch: 22, Training Loss: 0.9808, Test Loss: 1.0737
Epoch: 23, Training Loss: 0.9798, Test Loss: 1.0829
Epoch: 24, Training Loss: 0.9815, Test Loss: 1.0801
Epoch: 25, Training Loss: 0.9876, Test Loss: 1.0783
Epoch: 26, Training Loss: 0.9828, Test Loss: 1.0641
Epoch: 27, Training Loss: 0.9827, Test Loss: 1.0682
Epoch: 28, Training Loss: 0.9766, Test Loss: 1.0580
Epoch: 29, Training Loss: 0.9760, Test Loss: 1.0707
Epoch: 30, Training Loss: 0.9726, Test Loss: 1.0795
Epoch: 31, Training Loss: 0.9689, Test Loss: 1.0739
Epoch: 32, Training Loss: 0.9667, Test Loss: 1.0533
Epoch: 33, Training Loss: 0.9620, Test Loss: 1.0626
Epoch: 34, Training Loss: 0.9497, Test Loss: 1.0580
Epoch: 35, Training Loss: 0.9426, Test Loss: 1.0351
Epoch: 36, Training Loss: 0.9319, Test Loss: 1.0413
Epoch: 37, Training Loss: 0.9262, Test Loss: 1.0305
Epoch: 38, Training Loss: 0.9153, Test Loss: 1.0139
Epoch: 39, Training Loss: 0.9071, Test Loss: 1.0002
Epoch: 40, Training Loss: 0.9048, Test Loss: 0.9895
Epoch: 41, Training Loss: 0.8989, Test Loss: 1.0005
Epoch: 42, Training Loss: 0.8956, Test Loss: 0.9891
Epoch: 43, Training Loss: 0.8860, Test Loss: 0.9992

 28%|███████████████████████████▍                                                                      | 70/250 [00:05<00:14, 12.12it/s, loss_test=0.900]
Epoch: 45, Training Loss: 0.8833, Test Loss: 0.9925
Epoch: 46, Training Loss: 0.8727, Test Loss: 0.9627
Epoch: 47, Training Loss: 0.8749, Test Loss: 0.9664
Epoch: 48, Training Loss: 0.8658, Test Loss: 0.9722
Epoch: 49, Training Loss: 0.8652, Test Loss: 0.9583
Epoch: 50, Training Loss: 0.8569, Test Loss: 0.9628
Epoch: 51, Training Loss: 0.8600, Test Loss: 0.9535
Epoch: 52, Training Loss: 0.8558, Test Loss: 0.9298
Epoch: 53, Training Loss: 0.8463, Test Loss: 0.9546
Epoch: 54, Training Loss: 0.8472, Test Loss: 0.9381
Epoch: 55, Training Loss: 0.8429, Test Loss: 0.9262
Epoch: 56, Training Loss: 0.8365, Test Loss: 0.9385
Epoch: 57, Training Loss: 0.8322, Test Loss: 0.9288
Epoch: 58, Training Loss: 0.8278, Test Loss: 0.9257
Epoch: 59, Training Loss: 0.8211, Test Loss: 0.9371
Epoch: 60, Training Loss: 0.8180, Test Loss: 0.9351
Epoch: 61, Training Loss: 0.8136, Test Loss: 0.9289
Epoch: 62, Training Loss: 0.8089, Test Loss: 0.9170
Epoch: 63, Training Loss: 0.8034, Test Loss: 0.9162
Epoch: 64, Training Loss: 0.8017, Test Loss: 0.8950
Epoch: 65, Training Loss: 0.7979, Test Loss: 0.9007
Epoch: 66, Training Loss: 0.7939, Test Loss: 0.9070
Epoch: 67, Training Loss: 0.7895, Test Loss: 0.9028
Epoch: 68, Training Loss: 0.7831, Test Loss: 0.8903
Epoch: 69, Training Loss: 0.7774, Test Loss: 0.9000
Epoch: 70, Training Loss: 0.7744, Test Loss: 0.8945
Epoch: 71, Training Loss: 0.7677, Test Loss: 0.8998
Epoch: 72, Training Loss: 0.7657, Test Loss: 0.8889
Epoch: 73, Training Loss: 0.7634, Test Loss: 0.8792
Epoch: 74, Training Loss: 0.7558, Test Loss: 0.8957
Epoch: 75, Training Loss: 0.7550, Test Loss: 0.8798
Epoch: 76, Training Loss: 0.7514, Test Loss: 0.8871
Epoch: 77, Training Loss: 0.7445, Test Loss: 0.8784
Epoch: 78, Training Loss: 0.7422, Test Loss: 0.8999
Epoch: 79, Training Loss: 0.7380, Test Loss: 0.8795
Epoch: 80, Training Loss: 0.7296, Test Loss: 0.8854
Epoch: 81, Training Loss: 0.7278, Test Loss: 0.8912
Epoch: 82, Training Loss: 0.7259, Test Loss: 0.8882
Epoch: 83, Training Loss: 0.7203, Test Loss: 0.8974
Epoch: 84, Training Loss: 0.7173, Test Loss: 0.8789
Epoch: 85, Training Loss: 0.7129, Test Loss: 0.8875
Epoch: 86, Training Loss: 0.7119, Test Loss: 0.8841
Epoch: 87, Training Loss: 0.7071, Test Loss: 0.8838
Epoch: 88, Training Loss: 0.7046, Test Loss: 0.8779
Epoch: 89, Training Loss: 0.7013, Test Loss: 0.8863
Epoch: 90, Training Loss: 0.6983, Test Loss: 0.8788
Epoch: 91, Training Loss: 0.6912, Test Loss: 0.8818
Epoch: 92, Training Loss: 0.6861, Test Loss: 0.8782
Epoch: 93, Training Loss: 0.6820, Test Loss: 0.8789

 38%|████████████████████████████████████▊                                                             | 94/250 [00:07<00:12, 12.72it/s, loss_test=0.875]
Epoch: 95, Training Loss: 0.6784, Test Loss: 0.8815
Epoch: 96, Training Loss: 0.6743, Test Loss: 0.8817
Epoch: 97, Training Loss: 0.6716, Test Loss: 0.8933
Epoch: 98, Training Loss: 0.6664, Test Loss: 0.8909
Epoch: 99, Training Loss: 0.6628, Test Loss: 0.8911
Epoch: 100, Training Loss: 0.6580, Test Loss: 0.8878
Epoch: 101, Training Loss: 0.6573, Test Loss: 0.8966
Epoch: 102, Training Loss: 0.6520, Test Loss: 0.8859
Epoch: 103, Training Loss: 0.6489, Test Loss: 0.8893
Epoch: 104, Training Loss: 0.6447, Test Loss: 0.8889
Epoch: 105, Training Loss: 0.6420, Test Loss: 0.8977
Epoch: 106, Training Loss: 0.6381, Test Loss: 0.8964
Epoch: 107, Training Loss: 0.6332, Test Loss: 0.8915
Epoch: 108, Training Loss: 0.6317, Test Loss: 0.8905
Epoch: 109, Training Loss: 0.6273, Test Loss: 0.8925
Epoch: 110, Training Loss: 0.6248, Test Loss: 0.9030
Epoch: 111, Training Loss: 0.6217, Test Loss: 0.8890
Epoch: 112, Training Loss: 0.6168, Test Loss: 0.9085
Epoch: 113, Training Loss: 0.6151, Test Loss: 0.9003
Epoch: 114, Training Loss: 0.6093, Test Loss: 0.9051
Epoch: 115, Training Loss: 0.6096, Test Loss: 0.9084
Epoch: 116, Training Loss: 0.6043, Test Loss: 0.9068
Epoch: 117, Training Loss: 0.6031, Test Loss: 0.9019
Epoch: 118, Training Loss: 0.5966, Test Loss: 0.9017


 58%|███████████████████████████████████████████████████████▊                                         | 144/250 [00:11<00:08, 11.96it/s, loss_test=0.941]
Epoch: 120, Training Loss: 0.5922, Test Loss: 0.9002
Epoch: 121, Training Loss: 0.5898, Test Loss: 0.9094
Epoch: 122, Training Loss: 0.5858, Test Loss: 0.9208
Epoch: 123, Training Loss: 0.5834, Test Loss: 0.9242
Epoch: 124, Training Loss: 0.5811, Test Loss: 0.9161
Epoch: 125, Training Loss: 0.5779, Test Loss: 0.9108
Epoch: 126, Training Loss: 0.5740, Test Loss: 0.9164
Epoch: 127, Training Loss: 0.5699, Test Loss: 0.9236
Epoch: 128, Training Loss: 0.5672, Test Loss: 0.9185
Epoch: 129, Training Loss: 0.5655, Test Loss: 0.9300
Epoch: 130, Training Loss: 0.5630, Test Loss: 0.9158
Epoch: 131, Training Loss: 0.5580, Test Loss: 0.9221
Epoch: 132, Training Loss: 0.5555, Test Loss: 0.9255
Epoch: 133, Training Loss: 0.5556, Test Loss: 0.9302
Epoch: 134, Training Loss: 0.5507, Test Loss: 0.9286
Epoch: 135, Training Loss: 0.5470, Test Loss: 0.9314
Epoch: 136, Training Loss: 0.5448, Test Loss: 0.9325
Epoch: 137, Training Loss: 0.5441, Test Loss: 0.9318
Epoch: 138, Training Loss: 0.5403, Test Loss: 0.9244
Epoch: 139, Training Loss: 0.5369, Test Loss: 0.9244
Epoch: 140, Training Loss: 0.5336, Test Loss: 0.9294
Epoch: 141, Training Loss: 0.5289, Test Loss: 0.9415
Epoch: 142, Training Loss: 0.5302, Test Loss: 0.9320

 67%|█████████████████████████████████████████████████████████████████▏                               | 168/250 [00:13<00:07, 11.70it/s, loss_test=0.948]
Epoch: 144, Training Loss: 0.5236, Test Loss: 0.9412
Epoch: 145, Training Loss: 0.5194, Test Loss: 0.9418
Epoch: 146, Training Loss: 0.5177, Test Loss: 0.9399
Epoch: 147, Training Loss: 0.5160, Test Loss: 0.9300
Epoch: 148, Training Loss: 0.5111, Test Loss: 0.9478
Epoch: 149, Training Loss: 0.5087, Test Loss: 0.9402
Epoch: 150, Training Loss: 0.5078, Test Loss: 0.9424
Epoch: 151, Training Loss: 0.5030, Test Loss: 0.9406
Epoch: 152, Training Loss: 0.5006, Test Loss: 0.9398
Epoch: 153, Training Loss: 0.4990, Test Loss: 0.9470
Epoch: 154, Training Loss: 0.4962, Test Loss: 0.9515
Epoch: 155, Training Loss: 0.4934, Test Loss: 0.9488
Epoch: 156, Training Loss: 0.4923, Test Loss: 0.9451
Epoch: 157, Training Loss: 0.4880, Test Loss: 0.9470
Epoch: 158, Training Loss: 0.4856, Test Loss: 0.9491
Epoch: 159, Training Loss: 0.4825, Test Loss: 0.9449
Epoch: 160, Training Loss: 0.4798, Test Loss: 0.9366
Epoch: 161, Training Loss: 0.4770, Test Loss: 0.9339
Epoch: 162, Training Loss: 0.4747, Test Loss: 0.9364
Epoch: 163, Training Loss: 0.4728, Test Loss: 0.9635
Epoch: 164, Training Loss: 0.4708, Test Loss: 0.9598
Epoch: 165, Training Loss: 0.4687, Test Loss: 0.9464
Epoch: 166, Training Loss: 0.4662, Test Loss: 0.9575

 77%|██████████████████████████████████████████████████████████████████████████▍                      | 192/250 [00:15<00:04, 11.95it/s, loss_test=0.962]
Epoch: 168, Training Loss: 0.4622, Test Loss: 0.9611
Epoch: 169, Training Loss: 0.4583, Test Loss: 0.9540
Epoch: 170, Training Loss: 0.4572, Test Loss: 0.9510
Epoch: 171, Training Loss: 0.4549, Test Loss: 0.9496
Epoch: 172, Training Loss: 0.4521, Test Loss: 0.9570
Epoch: 173, Training Loss: 0.4508, Test Loss: 0.9459
Epoch: 174, Training Loss: 0.4472, Test Loss: 0.9586
Epoch: 175, Training Loss: 0.4447, Test Loss: 0.9542
Epoch: 176, Training Loss: 0.4423, Test Loss: 0.9583
Epoch: 177, Training Loss: 0.4400, Test Loss: 0.9563
Epoch: 178, Training Loss: 0.4369, Test Loss: 0.9564
Epoch: 179, Training Loss: 0.4361, Test Loss: 0.9594
Epoch: 180, Training Loss: 0.4345, Test Loss: 0.9491
Epoch: 181, Training Loss: 0.4319, Test Loss: 0.9629
Epoch: 182, Training Loss: 0.4298, Test Loss: 0.9697
Epoch: 183, Training Loss: 0.4264, Test Loss: 0.9542
Epoch: 184, Training Loss: 0.4254, Test Loss: 0.9480
Epoch: 185, Training Loss: 0.4220, Test Loss: 0.9517
Epoch: 186, Training Loss: 0.4213, Test Loss: 0.9457
Epoch: 187, Training Loss: 0.4183, Test Loss: 0.9627
Epoch: 188, Training Loss: 0.4158, Test Loss: 0.9487
Epoch: 189, Training Loss: 0.4126, Test Loss: 0.9605
Epoch: 190, Training Loss: 0.4118, Test Loss: 0.9566

 86%|███████████████████████████████████████████████████████████████████████████████████▊             | 216/250 [00:17<00:02, 11.79it/s, loss_test=0.973]
Epoch: 192, Training Loss: 0.4073, Test Loss: 0.9618
Epoch: 193, Training Loss: 0.4060, Test Loss: 0.9614
Epoch: 194, Training Loss: 0.4049, Test Loss: 0.9724
Epoch: 195, Training Loss: 0.4010, Test Loss: 0.9744
Epoch: 196, Training Loss: 0.3979, Test Loss: 0.9622
Epoch: 197, Training Loss: 0.3976, Test Loss: 0.9574
Epoch: 198, Training Loss: 0.3953, Test Loss: 0.9608
Epoch: 199, Training Loss: 0.3928, Test Loss: 0.9724
Epoch: 200, Training Loss: 0.3915, Test Loss: 0.9727
Epoch: 201, Training Loss: 0.3896, Test Loss: 0.9602
Epoch: 202, Training Loss: 0.3891, Test Loss: 0.9641
Epoch: 203, Training Loss: 0.3857, Test Loss: 0.9720
Epoch: 204, Training Loss: 0.3836, Test Loss: 0.9850
Epoch: 205, Training Loss: 0.3819, Test Loss: 0.9823
Epoch: 206, Training Loss: 0.3811, Test Loss: 0.9784
Epoch: 207, Training Loss: 0.3797, Test Loss: 0.9689
Epoch: 208, Training Loss: 0.3772, Test Loss: 0.9591
Epoch: 209, Training Loss: 0.3748, Test Loss: 0.9700
Epoch: 210, Training Loss: 0.3718, Test Loss: 0.9685
Epoch: 211, Training Loss: 0.3724, Test Loss: 0.9643
Epoch: 212, Training Loss: 0.3692, Test Loss: 0.9801
Epoch: 213, Training Loss: 0.3677, Test Loss: 0.9712
Epoch: 214, Training Loss: 0.3652, Test Loss: 0.9848

 96%|█████████████████████████████████████████████████████████████████████████████████████████████    | 240/250 [00:19<00:00, 12.14it/s, loss_test=0.969]
Epoch: 216, Training Loss: 0.3625, Test Loss: 0.9731
Epoch: 217, Training Loss: 0.3592, Test Loss: 0.9739
Epoch: 218, Training Loss: 0.3595, Test Loss: 0.9622
Epoch: 219, Training Loss: 0.3581, Test Loss: 0.9916
Epoch: 220, Training Loss: 0.3564, Test Loss: 0.9846
Epoch: 221, Training Loss: 0.3538, Test Loss: 0.9980
Epoch: 222, Training Loss: 0.3522, Test Loss: 0.9852
Epoch: 223, Training Loss: 0.3501, Test Loss: 0.9755
Epoch: 224, Training Loss: 0.3488, Test Loss: 0.9807
Epoch: 225, Training Loss: 0.3470, Test Loss: 0.9765
Epoch: 226, Training Loss: 0.3439, Test Loss: 0.9868
Epoch: 227, Training Loss: 0.3433, Test Loss: 0.9924
Epoch: 228, Training Loss: 0.3425, Test Loss: 0.9798
Epoch: 229, Training Loss: 0.3389, Test Loss: 0.9894
Epoch: 230, Training Loss: 0.3393, Test Loss: 0.9844
Epoch: 231, Training Loss: 0.3383, Test Loss: 0.9590
Epoch: 232, Training Loss: 0.3345, Test Loss: 0.9849
Epoch: 233, Training Loss: 0.3345, Test Loss: 0.9877
Epoch: 234, Training Loss: 0.3318, Test Loss: 0.9900
Epoch: 235, Training Loss: 0.3311, Test Loss: 0.9748
Epoch: 236, Training Loss: 0.3290, Test Loss: 0.9782
Epoch: 237, Training Loss: 0.3276, Test Loss: 0.9994
Epoch: 238, Training Loss: 0.3266, Test Loss: 0.9955

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.11it/s, loss_test=0.989]
Epoch: 240, Training Loss: 0.3235, Test Loss: 0.9686
Epoch: 241, Training Loss: 0.3223, Test Loss: 0.9845
Epoch: 242, Training Loss: 0.3203, Test Loss: 1.0093
Epoch: 243, Training Loss: 0.3185, Test Loss: 1.0061
Epoch: 244, Training Loss: 0.3178, Test Loss: 1.0151
Epoch: 245, Training Loss: 0.3168, Test Loss: 0.9942
Epoch: 246, Training Loss: 0.3146, Test Loss: 1.0048
Epoch: 247, Training Loss: 0.3138, Test Loss: 1.0062
Epoch: 248, Training Loss: 0.3120, Test Loss: 0.9996
Epoch: 249, Training Loss: 0.3115, Test Loss: 0.9894
Model saved as model_3735136np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-1250000-3735136np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9927036762237549, 0.9945165991783143, 0.9900335669517517, 0.9908288478851318, 0.9915536999702453, 0.9892988801002502, 0.9895735025405884, 0.9849781036376953, 0.9899136781692505, 0.9858349919319153, 0.9858191728591919, 0.9877704858779908, 0.991884458065033, 0.9861740708351135, 0.9834824442863465, 0.9844055294990539, 0.9867567300796509, 0.9853111863136291, 0.9855478286743165, 0.9785272598266601, 0.9835705995559693, 0.987250280380249, 0.9808178901672363, 0.9797973275184632, 0.9814536929130554, 0.9876326799392701, 0.9827600359916687, 0.9826685309410095, 0.976626992225647, 0.9759540438652039, 0.9726020455360412, 0.9688971996307373, 0.9666605114936828, 0.9619962692260742, 0.9496541857719422, 0.9426450252532959, 0.9318935155868531, 0.9261699080467224, 0.91528480052948, 0.9070999264717102, 0.9048369288444519, 0.8989202857017518, 0.8956486940383911, 0.8859650611877441, 0.8817068934440613, 0.8833439946174622, 0.872731626033783, 0.8748994827270508, 0.8658208131790162, 0.8652212500572205, 0.856908094882965, 0.8600098848342895, 0.8557617783546447, 0.8463434100151062, 0.8472389221191406, 0.8429113149642944, 0.8365098595619201, 0.832223653793335, 0.8277905344963074, 0.8211440801620483, 0.8179585695266723, 0.8135984897613525, 0.8089153170585632, 0.8034275293350219, 0.8017164826393127, 0.7978531002998352, 0.7938998103141784, 0.7894958138465882, 0.7830819845199585, 0.7774453401565552, 0.7743858814239502, 0.7676651000976562, 0.7657146453857422, 0.763371741771698, 0.755847442150116, 0.7550195813179016, 0.7514236330986023, 0.7445267319679261, 0.7421869754791259, 0.7380154371261597, 0.7296398162841797, 0.7277662873268127, 0.7259260773658752, 0.7203258156776429, 0.7173070669174194, 0.7129376292228699, 0.7118557929992676, 0.7070789217948914, 0.7045603156089782, 0.7013006687164307, 0.698291289806366, 0.6911701083183288, 0.6860695004463195, 0.6819741606712342, 0.6786260485649109, 0.6783835172653199, 0.6743427872657776, 0.6716209888458252, 0.6664451122283935, 0.6627769589424133, 0.6579558253288269, 0.6573133349418641, 0.6520062565803528, 0.6488606810569764, 0.6447186350822449, 0.6420069336891174, 0.6380796194076538, 0.6331515789031983, 0.6317168235778808, 0.6273383855819702, 0.6247564911842346, 0.621748948097229, 0.6167564034461975, 0.6151144146919251, 0.6093020915985108, 0.6095941662788391, 0.6042761087417603, 0.6031266570091247, 0.5966452360153198, 0.5951800346374512, 0.592196261882782, 0.5898214340209961, 0.5857813000679016, 0.5834349155426025, 0.5810701966285705, 0.5778856158256531, 0.5740416288375855, 0.5698896765708923, 0.5672083139419556, 0.5654869318008423, 0.5630115747451783, 0.5580214381217956, 0.555538010597229, 0.5555648922920227, 0.5506778120994568, 0.5469651579856872, 0.5447674036026001, 0.544104278087616, 0.5403427958488465, 0.5368557333946228, 0.533604371547699, 0.5288783073425293, 0.5302375078201294, 0.5263149261474609, 0.5236288547515869, 0.5193817615509033, 0.517713975906372, 0.515982162952423, 0.5111332774162293, 0.5086912512779236, 0.507794839143753, 0.5030377984046936, 0.5006115078926087, 0.49904707074165344, 0.4962255835533142, 0.4933596670627594, 0.49231205582618714, 0.48804118633270266, 0.4856020987033844, 0.4824774503707886, 0.47981377840042116, 0.4770136773586273, 0.47474595308303835, 0.4727840065956116, 0.4708061158657074, 0.4687274992465973, 0.46619633436203, 0.46329823732376096, 0.46224241852760317, 0.45827739834785464, 0.4571528255939484, 0.4548609972000122, 0.45208709836006167, 0.45081427693367004, 0.4471846044063568, 0.4447398006916046, 0.4422809243202209, 0.43997070789337156, 0.43692723512649534, 0.436144095659256, 0.4345299184322357, 0.43191720843315123, 0.42981119751930236, 0.42637708187103274, 0.42537841796875, 0.42196165919303896, 0.4213168442249298, 0.41826810836791994, 0.41582942605018614, 0.41260687112808225, 0.41184295415878297, 0.40903862118721007, 0.4073369801044464, 0.40600656867027285, 0.4048628568649292, 0.40101121068000795, 0.3979067087173462, 0.3976381063461304, 0.39525442719459536, 0.3928239166736603, 0.3915253818035126, 0.38955518007278445, 0.38905600309371946, 0.3857461094856262, 0.3835717737674713, 0.38189302682876586, 0.3810716450214386, 0.3797320544719696, 0.3772351622581482, 0.3748078107833862, 0.37175039649009706, 0.3723827004432678, 0.369189989566803, 0.3677435040473938, 0.3652396142482758, 0.3646017014980316, 0.3624873459339142, 0.35917454957962036, 0.35946509838104246, 0.3581061363220215, 0.35639736652374265, 0.3537696897983551, 0.3521809995174408, 0.3500626146793365, 0.3488034188747406, 0.34697699546813965, 0.3439319670200348, 0.3433267652988434, 0.3424762487411499, 0.338913357257843, 0.3392793297767639, 0.33828629851341246, 0.334483927488327, 0.33450632691383364, 0.3318190693855286, 0.3311274409294128, 0.32903451919555665, 0.3276344120502472, 0.3266142189502716, 0.3243566155433655, 0.32348752617835996, 0.32234674096107485, 0.3203400671482086, 0.31846567392349245, 0.3177819728851318, 0.31675225496292114, 0.31459362506866456, 0.31380451321601865, 0.31203908324241636, 0.3114731013774872], 'loss_test': [1.1067074537277222, 1.085940957069397, 1.0837695598602295, 1.0799111127853394, 1.0694925785064697, 1.08041250705719, 1.079249382019043, 1.0747939348220825, 1.078991174697876, 1.0840766429901123, 1.0994377136230469, 1.0925021171569824, 1.073593020439148, 1.0837990045547485, 1.0828369855880737, 1.0877091884613037, 1.085113763809204, 1.0731983184814453, 1.0839413404464722, 1.0830358266830444, 1.0791740417480469, 1.0722315311431885, 1.0737254619598389, 1.0828516483306885, 1.0801000595092773, 1.078317403793335, 1.0640662908554077, 1.0681887865066528, 1.0579696893692017, 1.070709466934204, 1.0794591903686523, 1.0739293098449707, 1.0532668828964233, 1.0625946521759033, 1.0580273866653442, 1.0351221561431885, 1.0413472652435303, 1.0305062532424927, 1.0138877630233765, 1.00017511844635, 0.9894501566886902, 1.0005040168762207, 0.9891074299812317, 0.999229371547699, 0.9847742319107056, 0.9925267100334167, 0.962680995464325, 0.966424822807312, 0.9721642732620239, 0.9583116769790649, 0.9628351926803589, 0.9534624218940735, 0.9297822713851929, 0.9545992016792297, 0.9380800127983093, 0.9262134432792664, 0.9384709000587463, 0.9287919402122498, 0.9256821870803833, 0.9370739459991455, 0.9351479411125183, 0.9288575053215027, 0.9169860482215881, 0.9162253141403198, 0.8949764966964722, 0.900655210018158, 0.9070353507995605, 0.9028106331825256, 0.8903254270553589, 0.8999940156936646, 0.8944727182388306, 0.8997865915298462, 0.8889386653900146, 0.8791797161102295, 0.8956581950187683, 0.8797782063484192, 0.8871266841888428, 0.8784219026565552, 0.8998594880104065, 0.8794758319854736, 0.8853980302810669, 0.8911980986595154, 0.8881849646568298, 0.8973813652992249, 0.8789272308349609, 0.8874910473823547, 0.8841342926025391, 0.8838417530059814, 0.8779436349868774, 0.8862579464912415, 0.878831148147583, 0.8817893266677856, 0.8781875967979431, 0.8789191842079163, 0.8746046423912048, 0.8815100789070129, 0.8816882967948914, 0.8933436870574951, 0.8909090161323547, 0.891111433506012, 0.887847900390625, 0.8965883851051331, 0.8858863115310669, 0.8892669081687927, 0.8888951539993286, 0.8977041840553284, 0.8964245319366455, 0.8915169835090637, 0.8904773592948914, 0.8924862742424011, 0.9029600620269775, 0.8889954090118408, 0.9084689617156982, 0.9002647399902344, 0.9051223993301392, 0.9084166884422302, 0.9067922234535217, 0.9018672108650208, 0.9016833901405334, 0.9196723103523254, 0.9001663327217102, 0.9094420075416565, 0.9207804799079895, 0.924178421497345, 0.9161118865013123, 0.9107638001441956, 0.9164015054702759, 0.9236042499542236, 0.9185053706169128, 0.9300071001052856, 0.9158386588096619, 0.9220649003982544, 0.9254569411277771, 0.9301767349243164, 0.928621768951416, 0.9313914775848389, 0.9325278401374817, 0.9317581653594971, 0.9244241118431091, 0.9244140982627869, 0.929435670375824, 0.9415318965911865, 0.9320309162139893, 0.9352296590805054, 0.9411980509757996, 0.9418061375617981, 0.939929723739624, 0.9300108551979065, 0.9477730989456177, 0.9401552081108093, 0.9424228668212891, 0.9406333565711975, 0.9398256540298462, 0.9470328092575073, 0.9515479207038879, 0.9488403797149658, 0.9451313018798828, 0.9469994902610779, 0.949110209941864, 0.9448540806770325, 0.9366163611412048, 0.9338625073432922, 0.9363881945610046, 0.9635152220726013, 0.9597788453102112, 0.946429967880249, 0.9574832916259766, 0.9478539824485779, 0.9610504508018494, 0.9540212154388428, 0.9509848356246948, 0.9496139883995056, 0.9570002555847168, 0.9458844065666199, 0.9586460590362549, 0.9541854858398438, 0.9583024978637695, 0.9562720656394958, 0.9563812613487244, 0.9594129920005798, 0.9490600824356079, 0.9628680348396301, 0.9696633219718933, 0.954197883605957, 0.9479731321334839, 0.9517134428024292, 0.9456853270530701, 0.9627084732055664, 0.9487141966819763, 0.9604836702346802, 0.9565679430961609, 0.9847992062568665, 0.9618411660194397, 0.9614227414131165, 0.9724479913711548, 0.9743577837944031, 0.9621930122375488, 0.9573819041252136, 0.9607871174812317, 0.9723923802375793, 0.9727284908294678, 0.9601741433143616, 0.9640704989433289, 0.9720401763916016, 0.9850485324859619, 0.9823460578918457, 0.9783658385276794, 0.9688546061515808, 0.9591315388679504, 0.9700120687484741, 0.9685120582580566, 0.9643034338951111, 0.9800506234169006, 0.9711739420890808, 0.9848005771636963, 0.9620770812034607, 0.973088264465332, 0.973899245262146, 0.9621711373329163, 0.9915636777877808, 0.9845842123031616, 0.9980396032333374, 0.9852021932601929, 0.9754684567451477, 0.9806854724884033, 0.9764853119850159, 0.9868372678756714, 0.9924365878105164, 0.9797850847244263, 0.9894195199012756, 0.9844458699226379, 0.9589754343032837, 0.9848964214324951, 0.9876741170883179, 0.9900000691413879, 0.9747779369354248, 0.9782041907310486, 0.9993598461151123, 0.9954530596733093, 0.9811891317367554, 0.9685977697372437, 0.9844645261764526, 1.0092647075653076, 1.0060882568359375, 1.015053629875183, 0.9941509962081909, 1.0047507286071777, 1.0062119960784912, 0.9995749592781067, 0.9894073605537415], 'identifier': '3735136np'}