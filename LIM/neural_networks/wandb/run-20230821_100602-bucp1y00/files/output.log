
 20%|███████████████████▌                                                                              | 20/100 [00:01<00:06, 12.71it/s, loss_test=1.083]
Epoch: 00, Training Loss: 0.9961, Test Loss: 1.0689
Epoch: 01, Training Loss: 0.9931, Test Loss: 1.1003
Epoch: 02, Training Loss: 0.9952, Test Loss: 1.0864
Epoch: 03, Training Loss: 0.9946, Test Loss: 1.0860
Epoch: 04, Training Loss: 0.9862, Test Loss: 1.0851
Epoch: 05, Training Loss: 0.9925, Test Loss: 1.0900
Epoch: 06, Training Loss: 0.9934, Test Loss: 1.0771
Epoch: 07, Training Loss: 0.9957, Test Loss: 1.0883
Epoch: 08, Training Loss: 0.9872, Test Loss: 1.0793
Epoch: 09, Training Loss: 0.9947, Test Loss: 1.0819
Epoch: 10, Training Loss: 0.9891, Test Loss: 1.0891
Epoch: 11, Training Loss: 0.9865, Test Loss: 1.0889
Epoch: 12, Training Loss: 0.9842, Test Loss: 1.0899
Epoch: 13, Training Loss: 0.9850, Test Loss: 1.0766
Epoch: 14, Training Loss: 0.9873, Test Loss: 1.0763
Epoch: 15, Training Loss: 0.9877, Test Loss: 1.0637
Epoch: 16, Training Loss: 0.9854, Test Loss: 1.0939
Epoch: 17, Training Loss: 0.9868, Test Loss: 1.0847
Epoch: 18, Training Loss: 0.9876, Test Loss: 1.0819
Epoch: 19, Training Loss: 0.9801, Test Loss: 1.0896

 44%|███████████████████████████████████████████                                                       | 44/100 [00:03<00:04, 11.77it/s, loss_test=0.995]
Epoch: 21, Training Loss: 0.9865, Test Loss: 1.0895
Epoch: 22, Training Loss: 0.9842, Test Loss: 1.0912
Epoch: 23, Training Loss: 0.9822, Test Loss: 1.0708
Epoch: 24, Training Loss: 0.9841, Test Loss: 1.0705
Epoch: 25, Training Loss: 0.9826, Test Loss: 1.0888
Epoch: 26, Training Loss: 0.9806, Test Loss: 1.0692
Epoch: 27, Training Loss: 0.9790, Test Loss: 1.0806
Epoch: 28, Training Loss: 0.9770, Test Loss: 1.0876
Epoch: 29, Training Loss: 0.9717, Test Loss: 1.0665
Epoch: 30, Training Loss: 0.9724, Test Loss: 1.0855
Epoch: 31, Training Loss: 0.9684, Test Loss: 1.0674
Epoch: 32, Training Loss: 0.9573, Test Loss: 1.0554
Epoch: 33, Training Loss: 0.9543, Test Loss: 1.0591
Epoch: 34, Training Loss: 0.9505, Test Loss: 1.0554
Epoch: 35, Training Loss: 0.9464, Test Loss: 1.0177
Epoch: 36, Training Loss: 0.9360, Test Loss: 1.0207
Epoch: 37, Training Loss: 0.9289, Test Loss: 1.0268
Epoch: 38, Training Loss: 0.9255, Test Loss: 1.0177
Epoch: 39, Training Loss: 0.9178, Test Loss: 0.9918
Epoch: 40, Training Loss: 0.9133, Test Loss: 0.9966
Epoch: 41, Training Loss: 0.9019, Test Loss: 1.0064
Epoch: 42, Training Loss: 0.8988, Test Loss: 0.9896
Epoch: 43, Training Loss: 0.8985, Test Loss: 0.9805

 68%|██████████████████████████████████████████████████████████████████▋                               | 68/100 [00:05<00:02, 12.24it/s, loss_test=0.925]
Epoch: 45, Training Loss: 0.8850, Test Loss: 0.9828
Epoch: 46, Training Loss: 0.8824, Test Loss: 0.9571
Epoch: 47, Training Loss: 0.8765, Test Loss: 0.9741
Epoch: 48, Training Loss: 0.8766, Test Loss: 0.9489
Epoch: 49, Training Loss: 0.8705, Test Loss: 0.9692
Epoch: 50, Training Loss: 0.8673, Test Loss: 0.9572
Epoch: 51, Training Loss: 0.8602, Test Loss: 0.9652
Epoch: 52, Training Loss: 0.8602, Test Loss: 0.9501
Epoch: 53, Training Loss: 0.8531, Test Loss: 0.9475
Epoch: 54, Training Loss: 0.8557, Test Loss: 0.9536
Epoch: 55, Training Loss: 0.8498, Test Loss: 0.9504
Epoch: 56, Training Loss: 0.8440, Test Loss: 0.9657
Epoch: 57, Training Loss: 0.8404, Test Loss: 0.9727
Epoch: 58, Training Loss: 0.8350, Test Loss: 0.9575
Epoch: 59, Training Loss: 0.8312, Test Loss: 0.9324
Epoch: 60, Training Loss: 0.8288, Test Loss: 0.9341
Epoch: 61, Training Loss: 0.8269, Test Loss: 0.9333
Epoch: 62, Training Loss: 0.8200, Test Loss: 0.9416
Epoch: 63, Training Loss: 0.8173, Test Loss: 0.9585
Epoch: 64, Training Loss: 0.8139, Test Loss: 0.9305
Epoch: 65, Training Loss: 0.8067, Test Loss: 0.9345
Epoch: 66, Training Loss: 0.8095, Test Loss: 0.9296
Epoch: 67, Training Loss: 0.8025, Test Loss: 0.9322

 94%|████████████████████████████████████████████████████████████████████████████████████████████      | 94/100 [00:07<00:00, 12.09it/s, loss_test=0.909]
Epoch: 69, Training Loss: 0.7881, Test Loss: 0.9424
Epoch: 70, Training Loss: 0.7875, Test Loss: 0.9287
Epoch: 71, Training Loss: 0.7839, Test Loss: 0.9383
Epoch: 72, Training Loss: 0.7802, Test Loss: 0.9241
Epoch: 73, Training Loss: 0.7747, Test Loss: 0.9153
Epoch: 74, Training Loss: 0.7693, Test Loss: 0.9305
Epoch: 75, Training Loss: 0.7684, Test Loss: 0.9114
Epoch: 76, Training Loss: 0.7639, Test Loss: 0.9256
Epoch: 77, Training Loss: 0.7601, Test Loss: 0.9153
Epoch: 78, Training Loss: 0.7503, Test Loss: 0.9196
Epoch: 79, Training Loss: 0.7484, Test Loss: 0.9228
Epoch: 80, Training Loss: 0.7458, Test Loss: 0.9157
Epoch: 81, Training Loss: 0.7444, Test Loss: 0.9235
Epoch: 82, Training Loss: 0.7362, Test Loss: 0.9136
Epoch: 83, Training Loss: 0.7347, Test Loss: 0.8996
Epoch: 84, Training Loss: 0.7310, Test Loss: 0.9127
Epoch: 85, Training Loss: 0.7243, Test Loss: 0.9105
Epoch: 86, Training Loss: 0.7240, Test Loss: 0.9235
Epoch: 87, Training Loss: 0.7165, Test Loss: 0.9134
Epoch: 88, Training Loss: 0.7124, Test Loss: 0.8999
Epoch: 89, Training Loss: 0.7121, Test Loss: 0.8967
Epoch: 90, Training Loss: 0.7061, Test Loss: 0.9092
Epoch: 91, Training Loss: 0.7008, Test Loss: 0.9011
Epoch: 92, Training Loss: 0.6984, Test Loss: 0.9066

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.17it/s, loss_test=0.914]
Epoch: 94, Training Loss: 0.6909, Test Loss: 0.9112
Epoch: 95, Training Loss: 0.6844, Test Loss: 0.9164
Epoch: 96, Training Loss: 0.6829, Test Loss: 0.9122
Epoch: 97, Training Loss: 0.6752, Test Loss: 0.9025
Epoch: 98, Training Loss: 0.6746, Test Loss: 0.9077
Epoch: 99, Training Loss: 0.6700, Test Loss: 0.9136
Model saved as model_8235611np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-1210000-8235611np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 100, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.996055257320404, 0.9930917620658875, 0.9951526999473572, 0.994568943977356, 0.9861802458763123, 0.9925094246864319, 0.9934325695037842, 0.9956523299217224, 0.9871927618980407, 0.9947061777114868, 0.9890653610229492, 0.9864992618560791, 0.98415447473526, 0.985023033618927, 0.987295925617218, 0.9877037405967712, 0.9853812098503113, 0.9868303418159485, 0.9876455783843994, 0.9800626516342164, 0.9819392681121826, 0.9865324854850769, 0.9841582536697387, 0.9822137475013732, 0.9840859293937683, 0.9826250910758972, 0.9806441068649292, 0.978990912437439, 0.976966667175293, 0.9716915369033814, 0.9724288702011108, 0.9684451699256897, 0.9572983741760254, 0.9542515397071838, 0.9505313396453857, 0.9464463233947754, 0.9359764933586121, 0.9288922309875488, 0.9255051374435425, 0.9177679538726806, 0.9132715463638306, 0.9019492149353028, 0.8987954020500183, 0.8984822750091552, 0.8898717641830445, 0.8850138545036316, 0.8823664665222168, 0.8765017628669739, 0.8765838980674744, 0.8705445408821106, 0.8672644972801209, 0.8601662158966065, 0.8601893067359925, 0.8530731558799743, 0.8557054877281189, 0.8498065352439881, 0.844008207321167, 0.8403625845909118, 0.834970498085022, 0.8311570405960083, 0.8288327097892761, 0.8269080638885498, 0.8199783325195312, 0.8172567844390869, 0.8138788104057312, 0.8067133307456971, 0.809465491771698, 0.8024981856346131, 0.7968953013420105, 0.7881075501441955, 0.7874841094017029, 0.7839120626449585, 0.780170452594757, 0.7746761679649353, 0.7693137526512146, 0.7683714985847473, 0.7638959646224975, 0.7601284742355346, 0.7502991914749145, 0.748418641090393, 0.7458230376243591, 0.7443735122680664, 0.7362105131149292, 0.7346814870834351, 0.730987012386322, 0.7242588758468628, 0.7240131139755249, 0.7164610385894775, 0.7123845815658569, 0.7120647311210633, 0.7060815930366516, 0.7007787108421326, 0.6984021663665771, 0.6936171770095825, 0.690917682647705, 0.6843808770179749, 0.6828575968742371, 0.6752191781997681, 0.6746066808700562, 0.6700028538703918], 'loss_test': [1.068874478340149, 1.100325584411621, 1.0864448547363281, 1.086016058921814, 1.0851057767868042, 1.0899872779846191, 1.0770838260650635, 1.0883427858352661, 1.0792893171310425, 1.081870436668396, 1.0891374349594116, 1.0888558626174927, 1.089902639389038, 1.0765591859817505, 1.0763471126556396, 1.0637297630310059, 1.0938998460769653, 1.0846879482269287, 1.081876516342163, 1.0896228551864624, 1.0830198526382446, 1.0894955396652222, 1.091210126876831, 1.0708457231521606, 1.070501446723938, 1.0887700319290161, 1.0692249536514282, 1.0806224346160889, 1.0875569581985474, 1.0664572715759277, 1.0854556560516357, 1.0674285888671875, 1.05535089969635, 1.0590803623199463, 1.0553584098815918, 1.0177197456359863, 1.0207064151763916, 1.0267994403839111, 1.0176500082015991, 0.9917711019515991, 0.9966005086898804, 1.0064250230789185, 0.9896460771560669, 0.9804908037185669, 0.9954233765602112, 0.9827865958213806, 0.9571128487586975, 0.9741433262825012, 0.9489060640335083, 0.9691777229309082, 0.9572383761405945, 0.965195894241333, 0.9500693678855896, 0.9474923014640808, 0.9535914063453674, 0.9503899812698364, 0.9656978249549866, 0.9726622104644775, 0.9575268626213074, 0.9324294328689575, 0.9340500831604004, 0.9333087801933289, 0.9416462779045105, 0.9584624767303467, 0.9305018186569214, 0.9344596266746521, 0.9296483397483826, 0.9321736097335815, 0.9249144792556763, 0.9423545002937317, 0.9287105202674866, 0.9383189082145691, 0.924072265625, 0.915275514125824, 0.930458664894104, 0.9113909006118774, 0.9256119728088379, 0.9152727127075195, 0.9195978045463562, 0.9228039383888245, 0.9157289266586304, 0.9234707355499268, 0.9136113524436951, 0.8995886445045471, 0.9127385020256042, 0.9105175733566284, 0.9234839081764221, 0.9134005308151245, 0.8999079465866089, 0.8966742753982544, 0.9091784954071045, 0.9011155366897583, 0.9066272377967834, 0.9086598753929138, 0.9112269282341003, 0.9163655638694763, 0.9122216105461121, 0.9025179743766785, 0.9077408313751221, 0.9135618805885315], 'identifier': '8235611np'}