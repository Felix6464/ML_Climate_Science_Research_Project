

  0%|▍                                                                                                  | 1/250 [00:09<37:31,  9.04s/it, loss_test=1.005]

  1%|▊                                                                                                  | 2/250 [00:17<36:31,  8.84s/it, loss_test=0.792]
Epoch: 01, Training Loss: 0.7560, Test Loss: 0.7919


  2%|█▌                                                                                                 | 4/250 [00:35<36:15,  8.84s/it, loss_test=0.675]

  2%|█▉                                                                                                 | 5/250 [00:44<35:54,  8.79s/it, loss_test=0.651]

  2%|██▍                                                                                                | 6/250 [00:52<35:41,  8.78s/it, loss_test=0.638]

  3%|██▊                                                                                                | 7/250 [01:01<35:40,  8.81s/it, loss_test=0.628]
Epoch: 06, Training Loss: 0.6183, Test Loss: 0.6282

  3%|███▏                                                                                               | 8/250 [01:10<35:23,  8.77s/it, loss_test=0.622]


  4%|███▉                                                                                              | 10/250 [01:27<34:56,  8.73s/it, loss_test=0.612]
Epoch: 09, Training Loss: 0.6016, Test Loss: 0.6121


  5%|████▋                                                                                             | 12/250 [01:45<34:32,  8.71s/it, loss_test=0.605]

  5%|█████                                                                                             | 13/250 [01:53<34:21,  8.70s/it, loss_test=0.602]

  6%|█████▍                                                                                            | 14/250 [02:02<34:23,  8.74s/it, loss_test=0.601]

  6%|█████▉                                                                                            | 15/250 [02:11<33:59,  8.68s/it, loss_test=0.599]

  6%|██████▎                                                                                           | 16/250 [02:20<34:03,  8.73s/it, loss_test=0.597]
Epoch: 15, Training Loss: 0.5854, Test Loss: 0.5968


  7%|███████                                                                                           | 18/250 [02:37<34:06,  8.82s/it, loss_test=0.595]
Epoch: 17, Training Loss: 0.5823, Test Loss: 0.5945


  8%|███████▊                                                                                          | 20/250 [02:55<33:30,  8.74s/it, loss_test=0.593]

  8%|████████▏                                                                                         | 21/250 [03:04<33:24,  8.75s/it, loss_test=0.592]
Epoch: 20, Training Loss: 0.5787, Test Loss: 0.5921


  9%|█████████                                                                                         | 23/250 [03:21<33:00,  8.72s/it, loss_test=0.590]

 10%|█████████▍                                                                                        | 24/250 [03:30<32:49,  8.71s/it, loss_test=0.590]
Epoch: 23, Training Loss: 0.5759, Test Loss: 0.5904


 10%|██████████▏                                                                                       | 26/250 [03:47<32:32,  8.72s/it, loss_test=0.590]

 11%|██████████▌                                                                                       | 27/250 [03:56<32:12,  8.67s/it, loss_test=0.589]
Epoch: 26, Training Loss: 0.5739, Test Loss: 0.5893

 11%|██████████▉                                                                                       | 28/250 [04:06<32:37,  8.82s/it, loss_test=0.589]
Traceback (most recent call last):
  File "lstm_training.py", line 142, in <module>
    loss, loss_test = model.train_model(train_dataloader,
  File "/mnt/qb/home/goswami/gkd235/ML_Climate_Science_Research_Project/LIM/neural_networks/models/LSTM_enc_dec.py", line 328, in train_model
    outputs, decoder_hidden = self.decoder(decoder_input,
  File "/mnt/qb/work/goswami/gkd235/path/to/conda/envs/felix_c/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/qb/home/goswami/gkd235/ML_Climate_Science_Research_Project/LIM/neural_networks/models/LSTM_enc_dec.py", line 189, in forward
    lstm_out, decoder_hidden = self.lstms[i](decoder_input, decoder_hidden)
  File "/mnt/qb/work/goswami/gkd235/path/to/conda/envs/felix_c/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/qb/work/goswami/gkd235/path/to/conda/envs/felix_c/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 774, in forward
    is_batched = input.dim() == 3
KeyboardInterrupt