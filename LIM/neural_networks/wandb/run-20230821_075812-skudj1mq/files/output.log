
  8%|███████▊                                                                                          | 20/250 [00:01<00:19, 11.87it/s, loss_test=1.083]
Epoch: 00, Training Loss: 0.9942, Test Loss: 1.0780
Epoch: 01, Training Loss: 0.9981, Test Loss: 1.0765
Epoch: 02, Training Loss: 0.9925, Test Loss: 1.0778
Epoch: 03, Training Loss: 0.9878, Test Loss: 1.0582
Epoch: 04, Training Loss: 0.9964, Test Loss: 1.0565
Epoch: 05, Training Loss: 0.9912, Test Loss: 1.0802
Epoch: 06, Training Loss: 0.9903, Test Loss: 1.0721
Epoch: 07, Training Loss: 0.9923, Test Loss: 1.0651
Epoch: 08, Training Loss: 0.9923, Test Loss: 1.0745
Epoch: 09, Training Loss: 0.9893, Test Loss: 1.0615
Epoch: 10, Training Loss: 0.9886, Test Loss: 1.0685
Epoch: 11, Training Loss: 0.9905, Test Loss: 1.0829
Epoch: 12, Training Loss: 0.9886, Test Loss: 1.0623
Epoch: 13, Training Loss: 0.9862, Test Loss: 1.0922
Epoch: 14, Training Loss: 0.9930, Test Loss: 1.0722
Epoch: 15, Training Loss: 0.9845, Test Loss: 1.0791
Epoch: 16, Training Loss: 0.9865, Test Loss: 1.0858
Epoch: 17, Training Loss: 0.9895, Test Loss: 1.0771
Epoch: 18, Training Loss: 0.9867, Test Loss: 1.0776
Epoch: 19, Training Loss: 0.9892, Test Loss: 1.0835
Epoch: 20, Training Loss: 0.9854, Test Loss: 1.0764
Epoch: 21, Training Loss: 0.9885, Test Loss: 1.0883
Epoch: 22, Training Loss: 0.9833, Test Loss: 1.0781
Epoch: 23, Training Loss: 0.9871, Test Loss: 1.0831
Epoch: 24, Training Loss: 0.9814, Test Loss: 1.0817
Epoch: 25, Training Loss: 0.9780, Test Loss: 1.0856
Epoch: 26, Training Loss: 0.9807, Test Loss: 1.0733
Epoch: 27, Training Loss: 0.9809, Test Loss: 1.0709
Epoch: 28, Training Loss: 0.9809, Test Loss: 1.0795
Epoch: 29, Training Loss: 0.9777, Test Loss: 1.0993
Epoch: 30, Training Loss: 0.9811, Test Loss: 1.0819
Epoch: 31, Training Loss: 0.9732, Test Loss: 1.0874
Epoch: 32, Training Loss: 0.9717, Test Loss: 1.0638
Epoch: 33, Training Loss: 0.9678, Test Loss: 1.0762
Epoch: 34, Training Loss: 0.9593, Test Loss: 1.0646
Epoch: 35, Training Loss: 0.9507, Test Loss: 1.0747
Epoch: 36, Training Loss: 0.9458, Test Loss: 1.0543
Epoch: 37, Training Loss: 0.9366, Test Loss: 1.0490
Epoch: 38, Training Loss: 0.9267, Test Loss: 1.0399
Epoch: 39, Training Loss: 0.9223, Test Loss: 1.0247
Epoch: 40, Training Loss: 0.9094, Test Loss: 1.0100
Epoch: 41, Training Loss: 0.9058, Test Loss: 1.0133
Epoch: 42, Training Loss: 0.8976, Test Loss: 1.0073
Epoch: 43, Training Loss: 0.8975, Test Loss: 0.9822

 18%|█████████████████▏                                                                                | 44/250 [00:03<00:16, 12.50it/s, loss_test=0.994]
Epoch: 45, Training Loss: 0.8781, Test Loss: 0.9976
Epoch: 46, Training Loss: 0.8760, Test Loss: 0.9943
Epoch: 47, Training Loss: 0.8727, Test Loss: 0.9833
Epoch: 48, Training Loss: 0.8666, Test Loss: 0.9856
Epoch: 49, Training Loss: 0.8619, Test Loss: 0.9688
Epoch: 50, Training Loss: 0.8522, Test Loss: 0.9602
Epoch: 51, Training Loss: 0.8562, Test Loss: 0.9658
Epoch: 52, Training Loss: 0.8517, Test Loss: 0.9545
Epoch: 53, Training Loss: 0.8465, Test Loss: 0.9546
Epoch: 54, Training Loss: 0.8448, Test Loss: 0.9471
Epoch: 55, Training Loss: 0.8424, Test Loss: 0.9633
Epoch: 56, Training Loss: 0.8364, Test Loss: 0.9558
Epoch: 57, Training Loss: 0.8336, Test Loss: 0.9522
Epoch: 58, Training Loss: 0.8295, Test Loss: 0.9467
Epoch: 59, Training Loss: 0.8238, Test Loss: 0.9432
Epoch: 60, Training Loss: 0.8221, Test Loss: 0.9279
Epoch: 61, Training Loss: 0.8202, Test Loss: 0.9380
Epoch: 62, Training Loss: 0.8157, Test Loss: 0.9380
Epoch: 63, Training Loss: 0.8103, Test Loss: 0.9441
Epoch: 64, Training Loss: 0.8069, Test Loss: 0.9445
Epoch: 65, Training Loss: 0.8047, Test Loss: 0.9434
Epoch: 66, Training Loss: 0.7959, Test Loss: 0.9332
Epoch: 67, Training Loss: 0.7972, Test Loss: 0.9354


 38%|████████████████████████████████████▊                                                             | 94/250 [00:07<00:12, 12.11it/s, loss_test=0.905]
Epoch: 69, Training Loss: 0.7873, Test Loss: 0.9235
Epoch: 70, Training Loss: 0.7855, Test Loss: 0.9262
Epoch: 71, Training Loss: 0.7794, Test Loss: 0.9349
Epoch: 72, Training Loss: 0.7723, Test Loss: 0.9102
Epoch: 73, Training Loss: 0.7699, Test Loss: 0.9188
Epoch: 74, Training Loss: 0.7647, Test Loss: 0.9137
Epoch: 75, Training Loss: 0.7600, Test Loss: 0.9261
Epoch: 76, Training Loss: 0.7553, Test Loss: 0.9060
Epoch: 77, Training Loss: 0.7531, Test Loss: 0.9265
Epoch: 78, Training Loss: 0.7471, Test Loss: 0.9182
Epoch: 79, Training Loss: 0.7421, Test Loss: 0.9245
Epoch: 80, Training Loss: 0.7391, Test Loss: 0.8989
Epoch: 81, Training Loss: 0.7346, Test Loss: 0.9029
Epoch: 82, Training Loss: 0.7304, Test Loss: 0.9126
Epoch: 83, Training Loss: 0.7248, Test Loss: 0.9032
Epoch: 84, Training Loss: 0.7218, Test Loss: 0.8976
Epoch: 85, Training Loss: 0.7184, Test Loss: 0.9090
Epoch: 86, Training Loss: 0.7095, Test Loss: 0.9028
Epoch: 87, Training Loss: 0.7109, Test Loss: 0.8913
Epoch: 88, Training Loss: 0.7041, Test Loss: 0.9107
Epoch: 89, Training Loss: 0.7005, Test Loss: 0.9031
Epoch: 90, Training Loss: 0.6971, Test Loss: 0.9096
Epoch: 91, Training Loss: 0.6934, Test Loss: 0.9037
Epoch: 92, Training Loss: 0.6916, Test Loss: 0.9010

 47%|█████████████████████████████████████████████▊                                                   | 118/250 [00:09<00:11, 11.86it/s, loss_test=0.898]
Epoch: 94, Training Loss: 0.6836, Test Loss: 0.9072
Epoch: 95, Training Loss: 0.6793, Test Loss: 0.9109
Epoch: 96, Training Loss: 0.6742, Test Loss: 0.8980
Epoch: 97, Training Loss: 0.6700, Test Loss: 0.9023
Epoch: 98, Training Loss: 0.6648, Test Loss: 0.9059
Epoch: 99, Training Loss: 0.6621, Test Loss: 0.9080
Epoch: 100, Training Loss: 0.6604, Test Loss: 0.9057
Epoch: 101, Training Loss: 0.6591, Test Loss: 0.9076
Epoch: 102, Training Loss: 0.6552, Test Loss: 0.9126
Epoch: 103, Training Loss: 0.6493, Test Loss: 0.9071
Epoch: 104, Training Loss: 0.6470, Test Loss: 0.8927
Epoch: 105, Training Loss: 0.6436, Test Loss: 0.8870
Epoch: 106, Training Loss: 0.6382, Test Loss: 0.9033
Epoch: 107, Training Loss: 0.6366, Test Loss: 0.9054
Epoch: 108, Training Loss: 0.6315, Test Loss: 0.9124
Epoch: 109, Training Loss: 0.6276, Test Loss: 0.9006
Epoch: 110, Training Loss: 0.6262, Test Loss: 0.8975
Epoch: 111, Training Loss: 0.6232, Test Loss: 0.9041
Epoch: 112, Training Loss: 0.6190, Test Loss: 0.8967
Epoch: 113, Training Loss: 0.6150, Test Loss: 0.9066
Epoch: 114, Training Loss: 0.6098, Test Loss: 0.9111
Epoch: 115, Training Loss: 0.6074, Test Loss: 0.9071
Epoch: 116, Training Loss: 0.6047, Test Loss: 0.8958

 57%|███████████████████████████████████████████████████████                                          | 142/250 [00:11<00:08, 12.30it/s, loss_test=0.920]
Epoch: 118, Training Loss: 0.5989, Test Loss: 0.9064
Epoch: 119, Training Loss: 0.5966, Test Loss: 0.9193
Epoch: 120, Training Loss: 0.5914, Test Loss: 0.9159
Epoch: 121, Training Loss: 0.5873, Test Loss: 0.9099
Epoch: 122, Training Loss: 0.5855, Test Loss: 0.9071
Epoch: 123, Training Loss: 0.5823, Test Loss: 0.9096
Epoch: 124, Training Loss: 0.5754, Test Loss: 0.9068
Epoch: 125, Training Loss: 0.5768, Test Loss: 0.9029
Epoch: 126, Training Loss: 0.5748, Test Loss: 0.9161
Epoch: 127, Training Loss: 0.5708, Test Loss: 0.9148
Epoch: 128, Training Loss: 0.5656, Test Loss: 0.9272
Epoch: 129, Training Loss: 0.5621, Test Loss: 0.9080
Epoch: 130, Training Loss: 0.5570, Test Loss: 0.9182
Epoch: 131, Training Loss: 0.5565, Test Loss: 0.9134
Epoch: 132, Training Loss: 0.5551, Test Loss: 0.9163
Epoch: 133, Training Loss: 0.5493, Test Loss: 0.9040
Epoch: 134, Training Loss: 0.5494, Test Loss: 0.9006
Epoch: 135, Training Loss: 0.5453, Test Loss: 0.9178
Epoch: 136, Training Loss: 0.5396, Test Loss: 0.9179
Epoch: 137, Training Loss: 0.5380, Test Loss: 0.9201
Epoch: 138, Training Loss: 0.5344, Test Loss: 0.9163
Epoch: 139, Training Loss: 0.5301, Test Loss: 0.9194
Epoch: 140, Training Loss: 0.5302, Test Loss: 0.9262

 67%|█████████████████████████████████████████████████████████████████▏                               | 168/250 [00:13<00:06, 11.94it/s, loss_test=0.933]
Epoch: 142, Training Loss: 0.5236, Test Loss: 0.9199
Epoch: 143, Training Loss: 0.5208, Test Loss: 0.9159
Epoch: 144, Training Loss: 0.5197, Test Loss: 0.9217
Epoch: 145, Training Loss: 0.5155, Test Loss: 0.9206
Epoch: 146, Training Loss: 0.5147, Test Loss: 0.9183
Epoch: 147, Training Loss: 0.5108, Test Loss: 0.9203
Epoch: 148, Training Loss: 0.5074, Test Loss: 0.9153
Epoch: 149, Training Loss: 0.5046, Test Loss: 0.9168
Epoch: 150, Training Loss: 0.5015, Test Loss: 0.9233
Epoch: 151, Training Loss: 0.4995, Test Loss: 0.9156
Epoch: 152, Training Loss: 0.4938, Test Loss: 0.9245
Epoch: 153, Training Loss: 0.4941, Test Loss: 0.9227
Epoch: 154, Training Loss: 0.4904, Test Loss: 0.9273
Epoch: 155, Training Loss: 0.4873, Test Loss: 0.9440
Epoch: 156, Training Loss: 0.4855, Test Loss: 0.9166
Epoch: 157, Training Loss: 0.4831, Test Loss: 0.9307
Epoch: 158, Training Loss: 0.4800, Test Loss: 0.9314
Epoch: 159, Training Loss: 0.4774, Test Loss: 0.9393
Epoch: 160, Training Loss: 0.4752, Test Loss: 0.9364
Epoch: 161, Training Loss: 0.4726, Test Loss: 0.9376
Epoch: 162, Training Loss: 0.4691, Test Loss: 0.9369
Epoch: 163, Training Loss: 0.4666, Test Loss: 0.9510
Epoch: 164, Training Loss: 0.4645, Test Loss: 0.9360
Epoch: 165, Training Loss: 0.4622, Test Loss: 0.9315

 77%|██████████████████████████████████████████████████████████████████████████▍                      | 192/250 [00:15<00:04, 12.21it/s, loss_test=0.945]
Epoch: 167, Training Loss: 0.4547, Test Loss: 0.9334
Epoch: 168, Training Loss: 0.4576, Test Loss: 0.9469
Epoch: 169, Training Loss: 0.4523, Test Loss: 0.9357
Epoch: 170, Training Loss: 0.4507, Test Loss: 0.9343
Epoch: 171, Training Loss: 0.4487, Test Loss: 0.9240
Epoch: 172, Training Loss: 0.4476, Test Loss: 0.9377
Epoch: 173, Training Loss: 0.4435, Test Loss: 0.9458
Epoch: 174, Training Loss: 0.4410, Test Loss: 0.9420
Epoch: 175, Training Loss: 0.4392, Test Loss: 0.9404
Epoch: 176, Training Loss: 0.4363, Test Loss: 0.9417
Epoch: 177, Training Loss: 0.4346, Test Loss: 0.9549
Epoch: 178, Training Loss: 0.4338, Test Loss: 0.9552
Epoch: 179, Training Loss: 0.4295, Test Loss: 0.9620
Epoch: 180, Training Loss: 0.4278, Test Loss: 0.9393
Epoch: 181, Training Loss: 0.4262, Test Loss: 0.9547
Epoch: 182, Training Loss: 0.4249, Test Loss: 0.9500
Epoch: 183, Training Loss: 0.4221, Test Loss: 0.9441
Epoch: 184, Training Loss: 0.4197, Test Loss: 0.9537
Epoch: 185, Training Loss: 0.4189, Test Loss: 0.9302
Epoch: 186, Training Loss: 0.4148, Test Loss: 0.9405
Epoch: 187, Training Loss: 0.4126, Test Loss: 0.9546
Epoch: 188, Training Loss: 0.4088, Test Loss: 0.9533
Epoch: 189, Training Loss: 0.4092, Test Loss: 0.9426

 86%|███████████████████████████████████████████████████████████████████████████████████▊             | 216/250 [00:17<00:02, 12.15it/s, loss_test=0.976]
Epoch: 191, Training Loss: 0.4032, Test Loss: 0.9450
Epoch: 192, Training Loss: 0.4005, Test Loss: 0.9587
Epoch: 193, Training Loss: 0.4008, Test Loss: 0.9506
Epoch: 194, Training Loss: 0.3988, Test Loss: 0.9617
Epoch: 195, Training Loss: 0.3958, Test Loss: 0.9586
Epoch: 196, Training Loss: 0.3953, Test Loss: 0.9589
Epoch: 197, Training Loss: 0.3918, Test Loss: 0.9411
Epoch: 198, Training Loss: 0.3928, Test Loss: 0.9636
Epoch: 199, Training Loss: 0.3899, Test Loss: 0.9622
Epoch: 200, Training Loss: 0.3876, Test Loss: 0.9634
Epoch: 201, Training Loss: 0.3869, Test Loss: 0.9556
Epoch: 202, Training Loss: 0.3844, Test Loss: 0.9608
Epoch: 203, Training Loss: 0.3807, Test Loss: 0.9595
Epoch: 204, Training Loss: 0.3805, Test Loss: 0.9738
Epoch: 205, Training Loss: 0.3774, Test Loss: 0.9718
Epoch: 206, Training Loss: 0.3768, Test Loss: 0.9629
Epoch: 207, Training Loss: 0.3739, Test Loss: 0.9607
Epoch: 208, Training Loss: 0.3730, Test Loss: 0.9756
Epoch: 209, Training Loss: 0.3727, Test Loss: 0.9775
Epoch: 210, Training Loss: 0.3685, Test Loss: 0.9923
Epoch: 211, Training Loss: 0.3687, Test Loss: 0.9710
Epoch: 212, Training Loss: 0.3657, Test Loss: 0.9707
Epoch: 213, Training Loss: 0.3622, Test Loss: 0.9849

 97%|█████████████████████████████████████████████████████████████████████████████████████████████▉   | 242/250 [00:19<00:00, 12.83it/s, loss_test=0.997]
Epoch: 215, Training Loss: 0.3599, Test Loss: 0.9683
Epoch: 216, Training Loss: 0.3567, Test Loss: 0.9759
Epoch: 217, Training Loss: 0.3562, Test Loss: 0.9853
Epoch: 218, Training Loss: 0.3552, Test Loss: 0.9905
Epoch: 219, Training Loss: 0.3534, Test Loss: 0.9842
Epoch: 220, Training Loss: 0.3521, Test Loss: 0.9829
Epoch: 221, Training Loss: 0.3508, Test Loss: 0.9748
Epoch: 222, Training Loss: 0.3492, Test Loss: 0.9849
Epoch: 223, Training Loss: 0.3473, Test Loss: 0.9907
Epoch: 224, Training Loss: 0.3459, Test Loss: 0.9700
Epoch: 225, Training Loss: 0.3436, Test Loss: 0.9819
Epoch: 226, Training Loss: 0.3416, Test Loss: 0.9730
Epoch: 227, Training Loss: 0.3399, Test Loss: 0.9799
Epoch: 228, Training Loss: 0.3387, Test Loss: 0.9978
Epoch: 229, Training Loss: 0.3382, Test Loss: 0.9937
Epoch: 230, Training Loss: 0.3357, Test Loss: 0.9961
Epoch: 231, Training Loss: 0.3326, Test Loss: 0.9969
Epoch: 232, Training Loss: 0.3321, Test Loss: 0.9820
Epoch: 233, Training Loss: 0.3318, Test Loss: 1.0128
Epoch: 234, Training Loss: 0.3283, Test Loss: 0.9799
Epoch: 235, Training Loss: 0.3271, Test Loss: 0.9970
Epoch: 236, Training Loss: 0.3268, Test Loss: 0.9981
Epoch: 237, Training Loss: 0.3245, Test Loss: 0.9954
Epoch: 238, Training Loss: 0.3226, Test Loss: 0.9913
Epoch: 239, Training Loss: 0.3216, Test Loss: 1.0035

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.16it/s, loss_test=1.012]
Epoch: 241, Training Loss: 0.3200, Test Loss: 0.9967
Epoch: 242, Training Loss: 0.3173, Test Loss: 0.9980
Epoch: 243, Training Loss: 0.3160, Test Loss: 1.0106
Epoch: 244, Training Loss: 0.3134, Test Loss: 1.0005
Epoch: 245, Training Loss: 0.3114, Test Loss: 0.9997
Epoch: 246, Training Loss: 0.3109, Test Loss: 1.0086
Epoch: 247, Training Loss: 0.3096, Test Loss: 0.9968
Epoch: 248, Training Loss: 0.3090, Test Loss: 1.0119
Epoch: 249, Training Loss: 0.3070, Test Loss: 1.0119
Model saved as model_1961800np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-12100000-1961800np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.994233226776123, 0.9980963826179504, 0.9924557685852051, 0.9878291964530945, 0.9963983297348022, 0.9911596298217773, 0.9903171300888062, 0.9922895669937134, 0.9922607779502869, 0.9893378257751465, 0.9885630249977112, 0.9904546856880188, 0.9886263370513916, 0.9862399578094483, 0.9929915428161621, 0.9844535827636719, 0.9864667057991028, 0.9894596695899963, 0.9867326855659485, 0.9892393469810485, 0.9854493379592896, 0.9884873270988465, 0.9833235502243042, 0.9870552659034729, 0.9813743472099304, 0.9779983043670655, 0.9807347059249878, 0.9809305667877197, 0.9808893442153931, 0.9776582837104797, 0.9811178684234619, 0.9731814622879028, 0.9716562986373901, 0.9678271651268006, 0.959276819229126, 0.9506946325302124, 0.9457908511161804, 0.9365670680999756, 0.9267395377159119, 0.9222630143165589, 0.9093824863433838, 0.9058356881141663, 0.8975709915161133, 0.8975068807601929, 0.8900486469268799, 0.8780698299407959, 0.8759949922561645, 0.8726845502853393, 0.8666497707366944, 0.8619277477264404, 0.8521616339683533, 0.8562310934066772, 0.851725435256958, 0.8465288996696472, 0.8447795152664185, 0.8423766136169434, 0.8363552212715148, 0.8335702061653137, 0.8294532060623169, 0.8237582445144653, 0.8220686316490173, 0.82020663022995, 0.8156795501708984, 0.8102722764015198, 0.8069038391113281, 0.8046803712844849, 0.7958937764167786, 0.7972420454025269, 0.7924984216690063, 0.7872544765472412, 0.7854556679725647, 0.7793600916862488, 0.772331178188324, 0.7699141383171082, 0.7646828651428222, 0.7600077748298645, 0.7553303122520447, 0.7530901074409485, 0.7471202373504638, 0.7420542359352111, 0.7391471862792969, 0.7346060872077942, 0.7303991079330444, 0.724844217300415, 0.7217613101005554, 0.718424129486084, 0.7094959735870361, 0.7109071135520935, 0.7040522336959839, 0.7005107879638672, 0.6970608472824097, 0.6934032440185547, 0.6916376233100892, 0.6877761244773865, 0.6836134791374207, 0.6793127655982971, 0.6742021918296814, 0.6700286746025086, 0.6647663474082947, 0.6621037602424622, 0.660413408279419, 0.6591467022895813, 0.6552073240280152, 0.6493225574493409, 0.6470038175582886, 0.6435539722442627, 0.6381753325462342, 0.6365657567977905, 0.6314680695533752, 0.6275972366333008, 0.6262437343597412, 0.6232425451278687, 0.6189638257026673, 0.6150208830833435, 0.6097663521766663, 0.6074490904808044, 0.604710066318512, 0.6040222525596619, 0.5988751649856567, 0.5965574622154236, 0.5914137363433838, 0.5873334646224976, 0.585512912273407, 0.5823473691940307, 0.5754483461380004, 0.5767855763435363, 0.5747909545898438, 0.5708424210548401, 0.5655920624732971, 0.5620674133300781, 0.5570014476776123, 0.5564700245857239, 0.5551344633102417, 0.5493276000022889, 0.5493808627128601, 0.5452581644058228, 0.5396363735198975, 0.5379889845848084, 0.5344253420829773, 0.530078935623169, 0.5301761507987977, 0.5263032793998719, 0.5236268520355225, 0.5208045601844787, 0.519667637348175, 0.5155199527740478, 0.5147473335266113, 0.5108097195625305, 0.5074371874332428, 0.50462886095047, 0.5014862298965455, 0.49954543113708494, 0.49378188252449035, 0.49411206841468813, 0.4904459059238434, 0.4873396992683411, 0.4855215013027191, 0.48311383128166197, 0.4800261318683624, 0.4773867130279541, 0.4751791417598724, 0.4726494371891022, 0.4691396474838257, 0.46658658385276797, 0.46450633406639097, 0.46224666833877565, 0.4602244973182678, 0.45473853349685667, 0.4576275825500488, 0.45231571793556213, 0.45065240263938905, 0.44869260787963866, 0.4476152777671814, 0.4435271143913269, 0.44101659655570985, 0.43919538855552676, 0.4363358557224274, 0.4345807135105133, 0.4337681829929352, 0.42946259379386903, 0.42775052785873413, 0.4261864423751831, 0.42494879961013793, 0.42210336327552794, 0.41972734928131106, 0.41888631582260133, 0.4148037552833557, 0.41264463067054746, 0.40881956219673155, 0.4091931939125061, 0.40672531723976135, 0.4032194197177887, 0.4005406856536865, 0.4008077383041382, 0.3988227844238281, 0.39579774141311647, 0.3952809453010559, 0.39176976680755615, 0.3928315281867981, 0.38987296223640444, 0.38762347102165223, 0.38686853647232056, 0.3844003438949585, 0.3806658089160919, 0.3805493175983429, 0.3773647606372833, 0.3768140017986298, 0.3739143073558807, 0.37303197383880615, 0.3727236747741699, 0.3684700489044189, 0.36873632073402407, 0.3657138764858246, 0.36221688985824585, 0.3625990867614746, 0.3599024474620819, 0.3567322850227356, 0.3562157690525055, 0.3552308022975922, 0.3534045696258545, 0.3521477818489075, 0.3507941126823425, 0.34918210506439207, 0.3473337471485138, 0.3458504855632782, 0.34362633228302003, 0.34157270193099976, 0.33992393016815187, 0.33866828083992007, 0.3382400691509247, 0.33572548627853394, 0.3325573682785034, 0.3320726573467255, 0.33178127408027647, 0.3283082664012909, 0.3271054565906525, 0.3267898321151733, 0.3244895815849304, 0.32256410717964173, 0.3216497480869293, 0.319408655166626, 0.31997613310813905, 0.31732595562934873, 0.3159544110298157, 0.3134355962276459, 0.3114250898361206, 0.310896110534668, 0.3095927298069, 0.30900445580482483, 0.30704827308654786], 'loss_test': [1.0780497789382935, 1.0765252113342285, 1.0777933597564697, 1.058179259300232, 1.0565481185913086, 1.0802041292190552, 1.072118878364563, 1.0651006698608398, 1.0745435953140259, 1.061478614807129, 1.0685073137283325, 1.0829159021377563, 1.0623184442520142, 1.0922008752822876, 1.0722076892852783, 1.0790581703186035, 1.0858136415481567, 1.0770583152770996, 1.0776035785675049, 1.0834537744522095, 1.0763565301895142, 1.0882536172866821, 1.0781238079071045, 1.0831204652786255, 1.0817207098007202, 1.0855789184570312, 1.0732955932617188, 1.0708813667297363, 1.0795139074325562, 1.099320888519287, 1.0819059610366821, 1.0873929262161255, 1.063821792602539, 1.076195478439331, 1.0646426677703857, 1.0746747255325317, 1.0542713403701782, 1.0490188598632812, 1.0399398803710938, 1.0247211456298828, 1.0100328922271729, 1.0133225917816162, 1.0073232650756836, 0.9821630120277405, 0.9940309524536133, 0.9976460337638855, 0.994297981262207, 0.983330488204956, 0.9855709671974182, 0.9688039422035217, 0.9601569771766663, 0.9657843708992004, 0.9544692635536194, 0.9545528888702393, 0.9471222758293152, 0.9633033275604248, 0.955836832523346, 0.9521993398666382, 0.946672260761261, 0.9432436227798462, 0.9278567433357239, 0.9380497336387634, 0.9379928112030029, 0.9441441893577576, 0.9444553256034851, 0.9433737993240356, 0.9331985116004944, 0.9353895783424377, 0.9118728637695312, 0.92349773645401, 0.9261977672576904, 0.9349272847175598, 0.9102330803871155, 0.9188165664672852, 0.9137083292007446, 0.9261336922645569, 0.9060226678848267, 0.926497757434845, 0.9182161092758179, 0.9244825839996338, 0.8988966345787048, 0.9029061198234558, 0.9126011729240417, 0.9032179117202759, 0.8976219296455383, 0.9090057611465454, 0.9027512073516846, 0.8912837505340576, 0.9106749296188354, 0.903059720993042, 0.9096471667289734, 0.9036899209022522, 0.9009696245193481, 0.9048992991447449, 0.9072486162185669, 0.9109018445014954, 0.8980469107627869, 0.9023047089576721, 0.9058611989021301, 0.9080020785331726, 0.9056674838066101, 0.9075788855552673, 0.9125510454177856, 0.9070594906806946, 0.8926727771759033, 0.8869667053222656, 0.9032901525497437, 0.905398428440094, 0.9124377965927124, 0.9005681276321411, 0.8975425362586975, 0.9041012525558472, 0.8967419266700745, 0.9066388607025146, 0.9110681414604187, 0.9070592522621155, 0.8957847952842712, 0.897628128528595, 0.906386137008667, 0.9192754626274109, 0.915851354598999, 0.9099182486534119, 0.9070687294006348, 0.9095854163169861, 0.9068081378936768, 0.9029141068458557, 0.9161131978034973, 0.9148234128952026, 0.9271948933601379, 0.908022940158844, 0.9182285070419312, 0.913413941860199, 0.9163447022438049, 0.9039676189422607, 0.9006160497665405, 0.9178034663200378, 0.9179024696350098, 0.9201146960258484, 0.9162548780441284, 0.9194086194038391, 0.9262375235557556, 0.9211978912353516, 0.9199246168136597, 0.9159173369407654, 0.9216623902320862, 0.9205647110939026, 0.9182916879653931, 0.9203498959541321, 0.9152882099151611, 0.9167960286140442, 0.9232661724090576, 0.9156256318092346, 0.9244842529296875, 0.9226640462875366, 0.9272530674934387, 0.9439797401428223, 0.9166021943092346, 0.9307120442390442, 0.9314390420913696, 0.939287006855011, 0.9363767504692078, 0.9375961422920227, 0.9369098544120789, 0.9509742856025696, 0.9360417723655701, 0.9315411448478699, 0.948876142501831, 0.9334176182746887, 0.9469459056854248, 0.9356929659843445, 0.9343146681785583, 0.9239539504051208, 0.9376804232597351, 0.9457792639732361, 0.9419873952865601, 0.940422773361206, 0.9416975378990173, 0.954928457736969, 0.9551955461502075, 0.9620180726051331, 0.9393466711044312, 0.9546851515769958, 0.9500235915184021, 0.9440523982048035, 0.9537045359611511, 0.9301859140396118, 0.9404880404472351, 0.9546078443527222, 0.9532516598701477, 0.9426032304763794, 0.9609444737434387, 0.9450217485427856, 0.9587076902389526, 0.9506397247314453, 0.9616594314575195, 0.9585744738578796, 0.9589051604270935, 0.9410884380340576, 0.9635962843894958, 0.9622310400009155, 0.9633870124816895, 0.955564558506012, 0.9608438611030579, 0.9595426917076111, 0.9738174676895142, 0.9718027710914612, 0.9628741145133972, 0.9606848359107971, 0.9756464958190918, 0.9775326251983643, 0.9923092722892761, 0.9709552526473999, 0.9706824421882629, 0.9849278926849365, 0.9835447072982788, 0.9683453440666199, 0.9759476184844971, 0.9852907061576843, 0.9904527068138123, 0.9841655492782593, 0.9828526377677917, 0.9748426675796509, 0.9848880767822266, 0.9907305836677551, 0.9699932932853699, 0.9819294810295105, 0.9730364084243774, 0.9798874855041504, 0.9978172779083252, 0.9937143921852112, 0.9961035251617432, 0.9969102144241333, 0.9819945096969604, 1.0127612352371216, 0.9798609614372253, 0.997035562992096, 0.9981333613395691, 0.9954304695129395, 0.9912508130073547, 1.003501534461975, 1.0012578964233398, 0.9967444539070129, 0.9980146884918213, 1.0106289386749268, 1.0004770755767822, 0.9996584057807922, 1.0085822343826294, 0.9968428015708923, 1.0118910074234009, 1.0119410753250122], 'identifier': '1961800np'}