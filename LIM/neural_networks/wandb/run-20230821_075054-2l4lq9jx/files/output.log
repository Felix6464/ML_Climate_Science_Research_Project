
  8%|███████▊                                                                                          | 20/250 [00:01<00:19, 12.10it/s, loss_test=1.089]
Epoch: 00, Training Loss: 0.9946, Test Loss: 1.0830
Epoch: 01, Training Loss: 0.9933, Test Loss: 1.0674
Epoch: 02, Training Loss: 0.9938, Test Loss: 1.0745
Epoch: 03, Training Loss: 0.9975, Test Loss: 1.0760
Epoch: 04, Training Loss: 0.9914, Test Loss: 1.0579
Epoch: 05, Training Loss: 0.9822, Test Loss: 1.0888
Epoch: 06, Training Loss: 0.9924, Test Loss: 1.0839
Epoch: 07, Training Loss: 0.9879, Test Loss: 1.0643
Epoch: 08, Training Loss: 0.9920, Test Loss: 1.0813
Epoch: 09, Training Loss: 0.9901, Test Loss: 1.0831
Epoch: 10, Training Loss: 0.9925, Test Loss: 1.0847
Epoch: 11, Training Loss: 0.9893, Test Loss: 1.0743
Epoch: 12, Training Loss: 0.9882, Test Loss: 1.1008
Epoch: 13, Training Loss: 0.9878, Test Loss: 1.0714
Epoch: 14, Training Loss: 0.9872, Test Loss: 1.0845
Epoch: 15, Training Loss: 0.9896, Test Loss: 1.0714
Epoch: 16, Training Loss: 0.9851, Test Loss: 1.0770
Epoch: 17, Training Loss: 0.9883, Test Loss: 1.0633
Epoch: 18, Training Loss: 0.9868, Test Loss: 1.0804
Epoch: 19, Training Loss: 0.9814, Test Loss: 1.0695

 18%|██████████████████                                                                                | 46/250 [00:03<00:16, 12.35it/s, loss_test=0.967]
Epoch: 21, Training Loss: 0.9851, Test Loss: 1.0961
Epoch: 22, Training Loss: 0.9860, Test Loss: 1.0913
Epoch: 23, Training Loss: 0.9850, Test Loss: 1.0825
Epoch: 24, Training Loss: 0.9834, Test Loss: 1.0877
Epoch: 25, Training Loss: 0.9793, Test Loss: 1.0921
Epoch: 26, Training Loss: 0.9818, Test Loss: 1.0834
Epoch: 27, Training Loss: 0.9811, Test Loss: 1.0792
Epoch: 28, Training Loss: 0.9765, Test Loss: 1.0830
Epoch: 29, Training Loss: 0.9770, Test Loss: 1.0760
Epoch: 30, Training Loss: 0.9709, Test Loss: 1.0618
Epoch: 31, Training Loss: 0.9618, Test Loss: 1.0688
Epoch: 32, Training Loss: 0.9525, Test Loss: 1.0606
Epoch: 33, Training Loss: 0.9449, Test Loss: 1.0616
Epoch: 34, Training Loss: 0.9370, Test Loss: 1.0438
Epoch: 35, Training Loss: 0.9331, Test Loss: 1.0341
Epoch: 36, Training Loss: 0.9244, Test Loss: 1.0209
Epoch: 37, Training Loss: 0.9107, Test Loss: 1.0286
Epoch: 38, Training Loss: 0.9122, Test Loss: 1.0152
Epoch: 39, Training Loss: 0.9027, Test Loss: 1.0044
Epoch: 40, Training Loss: 0.9013, Test Loss: 0.9895
Epoch: 41, Training Loss: 0.8989, Test Loss: 0.9971
Epoch: 42, Training Loss: 0.8928, Test Loss: 0.9896
Epoch: 43, Training Loss: 0.8880, Test Loss: 0.9893
Epoch: 44, Training Loss: 0.8859, Test Loss: 0.9766
Epoch: 45, Training Loss: 0.8848, Test Loss: 0.9666
Epoch: 46, Training Loss: 0.8817, Test Loss: 0.9605
Epoch: 47, Training Loss: 0.8767, Test Loss: 0.9658
Epoch: 48, Training Loss: 0.8752, Test Loss: 0.9626
Epoch: 49, Training Loss: 0.8702, Test Loss: 0.9587
Epoch: 50, Training Loss: 0.8684, Test Loss: 0.9610
Epoch: 51, Training Loss: 0.8648, Test Loss: 0.9698
Epoch: 52, Training Loss: 0.8624, Test Loss: 0.9580
Epoch: 53, Training Loss: 0.8566, Test Loss: 0.9570
Epoch: 54, Training Loss: 0.8551, Test Loss: 0.9707
Epoch: 55, Training Loss: 0.8517, Test Loss: 0.9523
Epoch: 56, Training Loss: 0.8503, Test Loss: 0.9542
Epoch: 57, Training Loss: 0.8503, Test Loss: 0.9548
Epoch: 58, Training Loss: 0.8457, Test Loss: 0.9671
Epoch: 59, Training Loss: 0.8408, Test Loss: 0.9326
Epoch: 60, Training Loss: 0.8437, Test Loss: 0.9329
Epoch: 61, Training Loss: 0.8408, Test Loss: 0.9389
Epoch: 62, Training Loss: 0.8319, Test Loss: 0.9536
Epoch: 63, Training Loss: 0.8334, Test Loss: 0.9385
Epoch: 64, Training Loss: 0.8308, Test Loss: 0.9413
Epoch: 65, Training Loss: 0.8272, Test Loss: 0.9512
Epoch: 66, Training Loss: 0.8240, Test Loss: 0.9414
Epoch: 67, Training Loss: 0.8252, Test Loss: 0.9300
Epoch: 68, Training Loss: 0.8201, Test Loss: 0.9252

 28%|███████████████████████████▍                                                                      | 70/250 [00:05<00:15, 11.89it/s, loss_test=0.941]
Epoch: 70, Training Loss: 0.8156, Test Loss: 0.9349
Epoch: 71, Training Loss: 0.8115, Test Loss: 0.9271
Epoch: 72, Training Loss: 0.8065, Test Loss: 0.9434
Epoch: 73, Training Loss: 0.8031, Test Loss: 0.9171
Epoch: 74, Training Loss: 0.8015, Test Loss: 0.9218
Epoch: 75, Training Loss: 0.7977, Test Loss: 0.9377
Epoch: 76, Training Loss: 0.7949, Test Loss: 0.9371
Epoch: 77, Training Loss: 0.7904, Test Loss: 0.9279
Epoch: 78, Training Loss: 0.7863, Test Loss: 0.9176
Epoch: 79, Training Loss: 0.7860, Test Loss: 0.9416
Epoch: 80, Training Loss: 0.7820, Test Loss: 0.9285
Epoch: 81, Training Loss: 0.7778, Test Loss: 0.9207
Epoch: 82, Training Loss: 0.7736, Test Loss: 0.9266
Epoch: 83, Training Loss: 0.7704, Test Loss: 0.9152
Epoch: 84, Training Loss: 0.7660, Test Loss: 0.9076
Epoch: 85, Training Loss: 0.7611, Test Loss: 0.9170
Epoch: 86, Training Loss: 0.7606, Test Loss: 0.9198
Epoch: 87, Training Loss: 0.7549, Test Loss: 0.9162
Epoch: 88, Training Loss: 0.7519, Test Loss: 0.9113
Epoch: 89, Training Loss: 0.7468, Test Loss: 0.9210
Epoch: 90, Training Loss: 0.7442, Test Loss: 0.9262
Epoch: 91, Training Loss: 0.7390, Test Loss: 0.9286
Epoch: 92, Training Loss: 0.7340, Test Loss: 0.8943

 38%|████████████████████████████████████▊                                                             | 94/250 [00:07<00:13, 11.77it/s, loss_test=0.926]
Epoch: 94, Training Loss: 0.7291, Test Loss: 0.9258
Epoch: 95, Training Loss: 0.7276, Test Loss: 0.8999
Epoch: 96, Training Loss: 0.7192, Test Loss: 0.9221
Epoch: 97, Training Loss: 0.7162, Test Loss: 0.9138
Epoch: 98, Training Loss: 0.7150, Test Loss: 0.9084
Epoch: 99, Training Loss: 0.7083, Test Loss: 0.9100
Epoch: 100, Training Loss: 0.7043, Test Loss: 0.9035
Epoch: 101, Training Loss: 0.7008, Test Loss: 0.9103
Epoch: 102, Training Loss: 0.6947, Test Loss: 0.9136
Epoch: 103, Training Loss: 0.6951, Test Loss: 0.9036
Epoch: 104, Training Loss: 0.6872, Test Loss: 0.9263
Epoch: 105, Training Loss: 0.6860, Test Loss: 0.9176
Epoch: 106, Training Loss: 0.6820, Test Loss: 0.9210
Epoch: 107, Training Loss: 0.6768, Test Loss: 0.9113
Epoch: 108, Training Loss: 0.6714, Test Loss: 0.9140
Epoch: 109, Training Loss: 0.6679, Test Loss: 0.9192
Epoch: 110, Training Loss: 0.6648, Test Loss: 0.9136
Epoch: 111, Training Loss: 0.6586, Test Loss: 0.9139
Epoch: 112, Training Loss: 0.6550, Test Loss: 0.9128
Epoch: 113, Training Loss: 0.6484, Test Loss: 0.9252
Epoch: 114, Training Loss: 0.6466, Test Loss: 0.9124
Epoch: 115, Training Loss: 0.6447, Test Loss: 0.9198
Epoch: 116, Training Loss: 0.6397, Test Loss: 0.9098
Epoch: 117, Training Loss: 0.6355, Test Loss: 0.9265
Epoch: 118, Training Loss: 0.6311, Test Loss: 0.9092
Epoch: 119, Training Loss: 0.6264, Test Loss: 0.9160
Epoch: 120, Training Loss: 0.6244, Test Loss: 0.9172
Epoch: 121, Training Loss: 0.6207, Test Loss: 0.9252
Epoch: 122, Training Loss: 0.6155, Test Loss: 0.9219

 47%|█████████████████████████████████████████████▊                                                   | 118/250 [00:09<00:11, 11.75it/s, loss_test=0.909]
Epoch: 124, Training Loss: 0.6100, Test Loss: 0.9235
Epoch: 125, Training Loss: 0.6087, Test Loss: 0.9152
Epoch: 126, Training Loss: 0.6022, Test Loss: 0.9275
Epoch: 127, Training Loss: 0.5985, Test Loss: 0.9151
Epoch: 128, Training Loss: 0.5957, Test Loss: 0.9295
Epoch: 129, Training Loss: 0.5943, Test Loss: 0.9328
Epoch: 130, Training Loss: 0.5893, Test Loss: 0.9219
Epoch: 131, Training Loss: 0.5868, Test Loss: 0.9279
Epoch: 132, Training Loss: 0.5838, Test Loss: 0.9220
Epoch: 133, Training Loss: 0.5792, Test Loss: 0.9255
Epoch: 134, Training Loss: 0.5765, Test Loss: 0.9221
Epoch: 135, Training Loss: 0.5728, Test Loss: 0.9308
Epoch: 136, Training Loss: 0.5696, Test Loss: 0.9217
Epoch: 137, Training Loss: 0.5643, Test Loss: 0.9208
Epoch: 138, Training Loss: 0.5628, Test Loss: 0.9312
Epoch: 139, Training Loss: 0.5587, Test Loss: 0.9210
Epoch: 140, Training Loss: 0.5564, Test Loss: 0.9317
Epoch: 141, Training Loss: 0.5542, Test Loss: 0.9273
Epoch: 142, Training Loss: 0.5509, Test Loss: 0.9324
Epoch: 143, Training Loss: 0.5469, Test Loss: 0.9389
Epoch: 144, Training Loss: 0.5419, Test Loss: 0.9331
Epoch: 145, Training Loss: 0.5418, Test Loss: 0.9276


 66%|████████████████████████████████████████████████████████████████▍                                | 166/250 [00:13<00:06, 12.12it/s, loss_test=0.941]
Epoch: 147, Training Loss: 0.5354, Test Loss: 0.9379
Epoch: 148, Training Loss: 0.5329, Test Loss: 0.9401
Epoch: 149, Training Loss: 0.5304, Test Loss: 0.9315
Epoch: 150, Training Loss: 0.5254, Test Loss: 0.9362
Epoch: 151, Training Loss: 0.5232, Test Loss: 0.9335
Epoch: 152, Training Loss: 0.5212, Test Loss: 0.9495
Epoch: 153, Training Loss: 0.5194, Test Loss: 0.9419
Epoch: 154, Training Loss: 0.5163, Test Loss: 0.9384
Epoch: 155, Training Loss: 0.5116, Test Loss: 0.9527
Epoch: 156, Training Loss: 0.5092, Test Loss: 0.9362
Epoch: 157, Training Loss: 0.5047, Test Loss: 0.9437
Epoch: 158, Training Loss: 0.5031, Test Loss: 0.9466
Epoch: 159, Training Loss: 0.5010, Test Loss: 0.9409
Epoch: 160, Training Loss: 0.4989, Test Loss: 0.9401
Epoch: 161, Training Loss: 0.4966, Test Loss: 0.9398
Epoch: 162, Training Loss: 0.4934, Test Loss: 0.9368
Epoch: 163, Training Loss: 0.4905, Test Loss: 0.9364
Epoch: 164, Training Loss: 0.4877, Test Loss: 0.9478
Epoch: 165, Training Loss: 0.4857, Test Loss: 0.9410
Epoch: 166, Training Loss: 0.4847, Test Loss: 0.9584
Epoch: 167, Training Loss: 0.4797, Test Loss: 0.9368
Epoch: 168, Training Loss: 0.4791, Test Loss: 0.9533
Epoch: 169, Training Loss: 0.4762, Test Loss: 0.9519
Epoch: 170, Training Loss: 0.4726, Test Loss: 0.9480
Epoch: 171, Training Loss: 0.4702, Test Loss: 0.9430
Epoch: 172, Training Loss: 0.4704, Test Loss: 0.9616
Epoch: 173, Training Loss: 0.4664, Test Loss: 0.9581
Epoch: 174, Training Loss: 0.4644, Test Loss: 0.9613
Epoch: 175, Training Loss: 0.4607, Test Loss: 0.9619
Epoch: 176, Training Loss: 0.4601, Test Loss: 0.9546
Epoch: 177, Training Loss: 0.4566, Test Loss: 0.9545
Epoch: 178, Training Loss: 0.4534, Test Loss: 0.9461
Epoch: 179, Training Loss: 0.4517, Test Loss: 0.9693
Epoch: 180, Training Loss: 0.4514, Test Loss: 0.9468
Epoch: 181, Training Loss: 0.4477, Test Loss: 0.9498
Epoch: 182, Training Loss: 0.4447, Test Loss: 0.9683
Epoch: 183, Training Loss: 0.4419, Test Loss: 0.9521
Epoch: 184, Training Loss: 0.4421, Test Loss: 0.9580
Epoch: 185, Training Loss: 0.4390, Test Loss: 0.9617
Epoch: 186, Training Loss: 0.4372, Test Loss: 0.9691
Epoch: 187, Training Loss: 0.4341, Test Loss: 0.9521
Epoch: 188, Training Loss: 0.4330, Test Loss: 0.9648
Epoch: 189, Training Loss: 0.4307, Test Loss: 0.9723
Epoch: 190, Training Loss: 0.4281, Test Loss: 0.9730
Epoch: 191, Training Loss: 0.4264, Test Loss: 0.9558
Epoch: 192, Training Loss: 0.4239, Test Loss: 0.9591
Epoch: 193, Training Loss: 0.4217, Test Loss: 0.9675

 76%|█████████████████████████████████████████████████████████████████████████▋                       | 190/250 [00:15<00:04, 12.12it/s, loss_test=0.973]
Epoch: 195, Training Loss: 0.4170, Test Loss: 0.9726
Epoch: 196, Training Loss: 0.4144, Test Loss: 0.9800
Epoch: 197, Training Loss: 0.4132, Test Loss: 0.9622
Epoch: 198, Training Loss: 0.4112, Test Loss: 0.9676
Epoch: 199, Training Loss: 0.4070, Test Loss: 0.9770
Epoch: 200, Training Loss: 0.4067, Test Loss: 0.9765
Epoch: 201, Training Loss: 0.4041, Test Loss: 0.9639
Epoch: 202, Training Loss: 0.4025, Test Loss: 0.9653
Epoch: 203, Training Loss: 0.4016, Test Loss: 0.9722
Epoch: 204, Training Loss: 0.3983, Test Loss: 0.9732
Epoch: 205, Training Loss: 0.3957, Test Loss: 0.9751
Epoch: 206, Training Loss: 0.3948, Test Loss: 0.9621
Epoch: 207, Training Loss: 0.3919, Test Loss: 0.9750
Epoch: 208, Training Loss: 0.3909, Test Loss: 0.9763
Epoch: 209, Training Loss: 0.3901, Test Loss: 0.9704
Epoch: 210, Training Loss: 0.3876, Test Loss: 0.9863
Epoch: 211, Training Loss: 0.3842, Test Loss: 0.9829
Epoch: 212, Training Loss: 0.3831, Test Loss: 0.9733
Epoch: 213, Training Loss: 0.3815, Test Loss: 0.9734
Epoch: 214, Training Loss: 0.3798, Test Loss: 0.9797
Epoch: 215, Training Loss: 0.3769, Test Loss: 0.9865
Epoch: 216, Training Loss: 0.3763, Test Loss: 0.9725
Epoch: 217, Training Loss: 0.3735, Test Loss: 0.9865


 95%|████████████████████████████████████████████████████████████████████████████████████████████▎    | 238/250 [00:19<00:00, 12.09it/s, loss_test=0.994]
Epoch: 219, Training Loss: 0.3712, Test Loss: 0.9936
Epoch: 220, Training Loss: 0.3669, Test Loss: 0.9770
Epoch: 221, Training Loss: 0.3663, Test Loss: 0.9807
Epoch: 222, Training Loss: 0.3645, Test Loss: 0.9852
Epoch: 223, Training Loss: 0.3628, Test Loss: 0.9731
Epoch: 224, Training Loss: 0.3615, Test Loss: 0.9785
Epoch: 225, Training Loss: 0.3597, Test Loss: 0.9714
Epoch: 226, Training Loss: 0.3564, Test Loss: 0.9866
Epoch: 227, Training Loss: 0.3565, Test Loss: 0.9729
Epoch: 228, Training Loss: 0.3556, Test Loss: 0.9906
Epoch: 229, Training Loss: 0.3524, Test Loss: 0.9864
Epoch: 230, Training Loss: 0.3502, Test Loss: 0.9899
Epoch: 231, Training Loss: 0.3499, Test Loss: 0.9837
Epoch: 232, Training Loss: 0.3464, Test Loss: 0.9931
Epoch: 233, Training Loss: 0.3441, Test Loss: 0.9835
Epoch: 234, Training Loss: 0.3433, Test Loss: 0.9835
Epoch: 235, Training Loss: 0.3424, Test Loss: 0.9792
Epoch: 236, Training Loss: 0.3397, Test Loss: 0.9938
Epoch: 237, Training Loss: 0.3392, Test Loss: 0.9833
Epoch: 238, Training Loss: 0.3374, Test Loss: 0.9937
Epoch: 239, Training Loss: 0.3353, Test Loss: 0.9851
Epoch: 240, Training Loss: 0.3344, Test Loss: 0.9897
Epoch: 241, Training Loss: 0.3326, Test Loss: 0.9979
Epoch: 242, Training Loss: 0.3302, Test Loss: 0.9958

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.01it/s, loss_test=0.995]
Epoch: 244, Training Loss: 0.3284, Test Loss: 1.0057
Epoch: 245, Training Loss: 0.3268, Test Loss: 0.9987
Epoch: 246, Training Loss: 0.3258, Test Loss: 1.0055
Epoch: 247, Training Loss: 0.3228, Test Loss: 0.9924
Epoch: 248, Training Loss: 0.3218, Test Loss: 0.9999
Epoch: 249, Training Loss: 0.3208, Test Loss: 0.9954
Model saved as model_3976284np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-125000-3976284np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9946273446083069, 0.9932576894760132, 0.9938254475593566, 0.9974515199661255, 0.9914081215858459, 0.9821869730949402, 0.9923606991767884, 0.9878948211669922, 0.9919742226600647, 0.9900538086891174, 0.9924839973449707, 0.9892570018768311, 0.9881511807441712, 0.9877617716789245, 0.9872064471244812, 0.9896319508552551, 0.9850905537605286, 0.9883007168769836, 0.9868479132652282, 0.9814118027687073, 0.9897945880889892, 0.9850634932518005, 0.9859995365142822, 0.9849737405776977, 0.9833804845809937, 0.9793293237686157, 0.9817957520484925, 0.9811316847801208, 0.9765009641647339, 0.9770129084587097, 0.9708608150482178, 0.9617679476737976, 0.9525124192237854, 0.9449115514755249, 0.9369697332382202, 0.9331324100494385, 0.9244136452674866, 0.9106748104095459, 0.9122275114059448, 0.9027193903923034, 0.9013126611709594, 0.8988954663276673, 0.8928413391113281, 0.8880048394203186, 0.8858596801757812, 0.8848238110542297, 0.8817047357559205, 0.8766879320144654, 0.8751520991325379, 0.870206093788147, 0.8684311866760254, 0.864774739742279, 0.8624008655548095, 0.8565725684165955, 0.8550895094871521, 0.8517056345939636, 0.8502800345420838, 0.850256335735321, 0.8456554889678956, 0.8408213496208191, 0.8436748266220093, 0.8407631754875183, 0.8318811178207397, 0.8333626747131347, 0.8308391094207763, 0.8271978020668029, 0.823999571800232, 0.8251716613769531, 0.8201326131820679, 0.8172063231468201, 0.8155598163604736, 0.8115352749824524, 0.8065083026885986, 0.8030662059783935, 0.8014860630035401, 0.7976835370063782, 0.7949475407600403, 0.7903894305229187, 0.7862906694412232, 0.7859546065330505, 0.7819685935974121, 0.7777660012245178, 0.7735911130905151, 0.770429265499115, 0.7659589409828186, 0.7610753059387207, 0.7606091976165772, 0.7549184679985046, 0.7519039750099182, 0.7468203544616699, 0.7441934108734131, 0.7389778017997741, 0.7340007305145264, 0.7327186584472656, 0.7291236877441406, 0.7275960445404053, 0.7192197799682617, 0.7162409782409668, 0.7149521827697753, 0.7082940101623535, 0.7042586326599121, 0.7007519006729126, 0.6947057723999024, 0.6950704336166382, 0.6871872901916504, 0.6859679341316223, 0.6820121884346009, 0.676800549030304, 0.6714309096336365, 0.6678608179092407, 0.6648141622543335, 0.6585598945617676, 0.6549670100212097, 0.6484462857246399, 0.6465949058532715, 0.6447283267974854, 0.6397353529930114, 0.6355156779289246, 0.6310921311378479, 0.6263648986816406, 0.6243817687034607, 0.620709502696991, 0.6155484199523926, 0.6130435228347778, 0.6100226283073426, 0.6086970448493958, 0.6021785497665405, 0.5985070943832398, 0.5957467913627624, 0.5942915558815003, 0.5892834067344666, 0.5867801547050476, 0.5838303685188293, 0.5792092561721802, 0.5764816403388977, 0.5728347897529602, 0.5696107983589173, 0.5642837643623352, 0.562774920463562, 0.558689546585083, 0.5564247250556946, 0.5541523337364197, 0.5509474039077759, 0.5469002008438111, 0.5418634533882141, 0.5417661190032959, 0.5372091412544251, 0.535362434387207, 0.5329123973846436, 0.5304031133651733, 0.5254447102546692, 0.5232043504714966, 0.5211843252182007, 0.5193873286247254, 0.5162501931190491, 0.5115624845027924, 0.5092214345932007, 0.5047308206558228, 0.5031020641326904, 0.5010417699813843, 0.4989445388317108, 0.4966241419315338, 0.49343753457069395, 0.4905198872089386, 0.48766974210739134, 0.4856902301311493, 0.4847408473491669, 0.4796615540981293, 0.4791012227535248, 0.4761665999889374, 0.47258892059326174, 0.47022612690925597, 0.47036467790603637, 0.4663869202136993, 0.46438937187194823, 0.4606786370277405, 0.46006627678871154, 0.4566193759441376, 0.453383070230484, 0.45167717933654783, 0.45142826437950134, 0.4476505994796753, 0.44465630650520327, 0.4419407665729523, 0.4420900285243988, 0.43896542191505433, 0.4371985733509064, 0.4341204285621643, 0.43300701379776, 0.43065778613090516, 0.42812886238098147, 0.4263852000236511, 0.4239218056201935, 0.4216822385787964, 0.4189043641090393, 0.4169641137123108, 0.41435704231262205, 0.4131718635559082, 0.4111660122871399, 0.4069672226905823, 0.40673787593841554, 0.4041469693183899, 0.4025238692760468, 0.4015612781047821, 0.3983428835868835, 0.3956947445869446, 0.39478386044502256, 0.3919400453567505, 0.3909099578857422, 0.39006826281547546, 0.3875763833522797, 0.3842146575450897, 0.38307790756225585, 0.3815127074718475, 0.3797725260257721, 0.3769069492816925, 0.37633055448532104, 0.37353654503822326, 0.3714354395866394, 0.3712403893470764, 0.36687432527542113, 0.36630186438560486, 0.3645260512828827, 0.362789922952652, 0.3614590227603912, 0.3597041368484497, 0.3564387023448944, 0.3564971089363098, 0.35557992458343507, 0.3524267554283142, 0.3502109706401825, 0.34990150332450864, 0.346383261680603, 0.34405393004417417, 0.34328373670578005, 0.3423888683319092, 0.3396797597408295, 0.33919408917427063, 0.33744136691093446, 0.3353149175643921, 0.33438299894332885, 0.3326061487197876, 0.33022332191467285, 0.32947168350219724, 0.32841343283653257, 0.3268463373184204, 0.32582672238349913, 0.32279070019721984, 0.32184463143348696, 0.32079386711120605], 'loss_test': [1.0829558372497559, 1.0674023628234863, 1.074479579925537, 1.0760012865066528, 1.0578899383544922, 1.0887939929962158, 1.083926796913147, 1.0643200874328613, 1.0813332796096802, 1.0830868482589722, 1.08466637134552, 1.0743114948272705, 1.1007757186889648, 1.0713509321212769, 1.084479808807373, 1.0714480876922607, 1.0770155191421509, 1.0633330345153809, 1.080352783203125, 1.069451093673706, 1.0886472463607788, 1.0961238145828247, 1.0912704467773438, 1.082451343536377, 1.0876981019973755, 1.0920956134796143, 1.0833940505981445, 1.0791552066802979, 1.0829819440841675, 1.075952172279358, 1.0617971420288086, 1.0687828063964844, 1.0605946779251099, 1.0615935325622559, 1.043842077255249, 1.0340986251831055, 1.0208923816680908, 1.028633952140808, 1.0151598453521729, 1.0043578147888184, 0.9894782304763794, 0.9971339106559753, 0.9895782470703125, 0.9893363118171692, 0.9765934944152832, 0.9666163325309753, 0.9605066180229187, 0.9658030271530151, 0.962578535079956, 0.9587331414222717, 0.961012601852417, 0.9697836637496948, 0.9580402970314026, 0.9570394158363342, 0.9706769585609436, 0.9522618651390076, 0.9541734457015991, 0.9548256993293762, 0.9670541882514954, 0.9326401948928833, 0.93287193775177, 0.9389321208000183, 0.9535923004150391, 0.9384577870368958, 0.941339373588562, 0.9512251615524292, 0.941432535648346, 0.9299905896186829, 0.9251577258110046, 0.9410958290100098, 0.9348500967025757, 0.9270894527435303, 0.9433689713478088, 0.917106568813324, 0.9217761158943176, 0.9376770257949829, 0.9370802640914917, 0.9278706908226013, 0.9176478385925293, 0.9416449069976807, 0.9285268187522888, 0.9207212924957275, 0.9266048073768616, 0.9152223467826843, 0.9075921177864075, 0.9169734716415405, 0.9198232293128967, 0.9162004590034485, 0.9113357663154602, 0.920983076095581, 0.9262479543685913, 0.9286321401596069, 0.8943213820457458, 0.9124608635902405, 0.9257926940917969, 0.8998772501945496, 0.9220594763755798, 0.9138464331626892, 0.9084369540214539, 0.9099586009979248, 0.903496503829956, 0.9103386998176575, 0.913616955280304, 0.9035939574241638, 0.9262514710426331, 0.9176013469696045, 0.9209634065628052, 0.9112664461135864, 0.9140401482582092, 0.9192219376564026, 0.9136269092559814, 0.9139371514320374, 0.9127649068832397, 0.9251537919044495, 0.9123982787132263, 0.9197889566421509, 0.9098392128944397, 0.926539957523346, 0.9092125296592712, 0.9160468578338623, 0.9171736836433411, 0.9251642823219299, 0.9219458699226379, 0.9266971945762634, 0.9235184788703918, 0.9151952266693115, 0.9275140762329102, 0.9150713086128235, 0.9295459389686584, 0.9327731728553772, 0.9218632578849792, 0.9279409050941467, 0.9220449328422546, 0.9254884719848633, 0.9221341609954834, 0.9307551383972168, 0.9217455983161926, 0.9207949638366699, 0.9312334060668945, 0.9209727048873901, 0.9316943883895874, 0.9273191690444946, 0.9324327707290649, 0.9389228224754333, 0.9331100583076477, 0.9276193380355835, 0.9367725849151611, 0.9379464983940125, 0.9400892853736877, 0.9314741492271423, 0.936240553855896, 0.9334735870361328, 0.9495481848716736, 0.9418898224830627, 0.9383596181869507, 0.9526847004890442, 0.9362389445304871, 0.9436500668525696, 0.946577787399292, 0.940861701965332, 0.940128743648529, 0.9397921562194824, 0.9368056058883667, 0.9364193677902222, 0.9478381872177124, 0.9410293698310852, 0.95844966173172, 0.93682861328125, 0.9533249139785767, 0.9519309401512146, 0.947982668876648, 0.9429972767829895, 0.9616309404373169, 0.9581208229064941, 0.9612992405891418, 0.9618501663208008, 0.9546496272087097, 0.9545021057128906, 0.9461280107498169, 0.9692615270614624, 0.9467764496803284, 0.9497987031936646, 0.9683092832565308, 0.9521004557609558, 0.9579553604125977, 0.961704432964325, 0.9691123366355896, 0.9520740509033203, 0.964820384979248, 0.9723090529441833, 0.9730241894721985, 0.9557832479476929, 0.9591222405433655, 0.967473030090332, 0.9603751301765442, 0.9725961089134216, 0.9800490140914917, 0.9622241854667664, 0.9676401019096375, 0.9770348072052002, 0.9765126705169678, 0.9638799428939819, 0.9653283357620239, 0.9722393751144409, 0.973177969455719, 0.9750638604164124, 0.9620665311813354, 0.9750232696533203, 0.976301372051239, 0.9703940153121948, 0.9862849116325378, 0.9829218983650208, 0.9732719659805298, 0.9734458327293396, 0.9796923398971558, 0.9864653944969177, 0.9724835753440857, 0.986506998538971, 0.9716585874557495, 0.9936298131942749, 0.9769589304924011, 0.9807171821594238, 0.9851635694503784, 0.973064124584198, 0.978482186794281, 0.971424400806427, 0.9865825176239014, 0.9729019403457642, 0.990587055683136, 0.986445963382721, 0.9899351596832275, 0.9837329387664795, 0.9930903911590576, 0.9835324287414551, 0.9834561347961426, 0.9792345762252808, 0.9938474893569946, 0.9832894206047058, 0.9937167763710022, 0.9851477742195129, 0.9896785616874695, 0.9978564381599426, 0.9957606792449951, 1.0024951696395874, 1.0056835412979126, 0.9987192749977112, 1.0054913759231567, 0.9923823475837708, 0.9999011754989624, 0.9953736662864685], 'identifier': '3976284np'}