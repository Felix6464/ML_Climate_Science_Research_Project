
  7%|███████                                                                                           | 18/250 [00:01<00:19, 12.10it/s, loss_test=1.084]
Epoch: 00, Training Loss: 0.9968, Test Loss: 1.0770
Epoch: 01, Training Loss: 0.9933, Test Loss: 1.0786
Epoch: 02, Training Loss: 0.9939, Test Loss: 1.0762
Epoch: 03, Training Loss: 0.9945, Test Loss: 1.0702
Epoch: 04, Training Loss: 0.9933, Test Loss: 1.0769
Epoch: 05, Training Loss: 0.9880, Test Loss: 1.0761
Epoch: 06, Training Loss: 0.9891, Test Loss: 1.0839
Epoch: 07, Training Loss: 0.9901, Test Loss: 1.0801
Epoch: 08, Training Loss: 0.9864, Test Loss: 1.0805
Epoch: 09, Training Loss: 0.9910, Test Loss: 1.0676
Epoch: 10, Training Loss: 0.9899, Test Loss: 1.0796
Epoch: 11, Training Loss: 0.9906, Test Loss: 1.0966
Epoch: 12, Training Loss: 0.9902, Test Loss: 1.0781
Epoch: 13, Training Loss: 0.9907, Test Loss: 1.0733
Epoch: 14, Training Loss: 0.9872, Test Loss: 1.0803
Epoch: 15, Training Loss: 0.9884, Test Loss: 1.0806
Epoch: 16, Training Loss: 0.9853, Test Loss: 1.0855
Epoch: 17, Training Loss: 0.9842, Test Loss: 1.0747

 17%|████████████████▍                                                                                 | 42/250 [00:03<00:17, 12.02it/s, loss_test=1.023]
Epoch: 19, Training Loss: 0.9787, Test Loss: 1.0800
Epoch: 20, Training Loss: 0.9875, Test Loss: 1.0806
Epoch: 21, Training Loss: 0.9793, Test Loss: 1.0855
Epoch: 22, Training Loss: 0.9843, Test Loss: 1.0647
Epoch: 23, Training Loss: 0.9813, Test Loss: 1.0867
Epoch: 24, Training Loss: 0.9844, Test Loss: 1.0627
Epoch: 25, Training Loss: 0.9788, Test Loss: 1.0957
Epoch: 26, Training Loss: 0.9826, Test Loss: 1.0744
Epoch: 27, Training Loss: 0.9811, Test Loss: 1.0795
Epoch: 28, Training Loss: 0.9782, Test Loss: 1.0809
Epoch: 29, Training Loss: 0.9862, Test Loss: 1.0673
Epoch: 30, Training Loss: 0.9800, Test Loss: 1.0864
Epoch: 31, Training Loss: 0.9756, Test Loss: 1.0924
Epoch: 32, Training Loss: 0.9771, Test Loss: 1.0872
Epoch: 33, Training Loss: 0.9697, Test Loss: 1.0657
Epoch: 34, Training Loss: 0.9684, Test Loss: 1.0759
Epoch: 35, Training Loss: 0.9666, Test Loss: 1.0430
Epoch: 36, Training Loss: 0.9591, Test Loss: 1.0696
Epoch: 37, Training Loss: 0.9461, Test Loss: 1.0734
Epoch: 38, Training Loss: 0.9403, Test Loss: 1.0596
Epoch: 39, Training Loss: 0.9334, Test Loss: 1.0419
Epoch: 40, Training Loss: 0.9233, Test Loss: 1.0287
Epoch: 41, Training Loss: 0.9177, Test Loss: 1.0395

 26%|█████████████████████████▊                                                                        | 66/250 [00:05<00:15, 11.88it/s, loss_test=0.928]
Epoch: 43, Training Loss: 0.9031, Test Loss: 1.0176
Epoch: 44, Training Loss: 0.8966, Test Loss: 1.0136
Epoch: 45, Training Loss: 0.8964, Test Loss: 1.0179
Epoch: 46, Training Loss: 0.8861, Test Loss: 1.0193
Epoch: 47, Training Loss: 0.8812, Test Loss: 1.0052
Epoch: 48, Training Loss: 0.8789, Test Loss: 1.0044
Epoch: 49, Training Loss: 0.8741, Test Loss: 0.9793
Epoch: 50, Training Loss: 0.8648, Test Loss: 1.0035
Epoch: 51, Training Loss: 0.8689, Test Loss: 0.9995
Epoch: 52, Training Loss: 0.8563, Test Loss: 0.9950
Epoch: 53, Training Loss: 0.8532, Test Loss: 0.9853
Epoch: 54, Training Loss: 0.8561, Test Loss: 0.9851
Epoch: 55, Training Loss: 0.8427, Test Loss: 0.9862
Epoch: 56, Training Loss: 0.8421, Test Loss: 0.9844
Epoch: 57, Training Loss: 0.8376, Test Loss: 0.9776
Epoch: 58, Training Loss: 0.8329, Test Loss: 0.9708
Epoch: 59, Training Loss: 0.8268, Test Loss: 0.9519
Epoch: 60, Training Loss: 0.8249, Test Loss: 0.9632
Epoch: 61, Training Loss: 0.8189, Test Loss: 0.9626
Epoch: 62, Training Loss: 0.8156, Test Loss: 0.9458
Epoch: 63, Training Loss: 0.8115, Test Loss: 0.9390
Epoch: 64, Training Loss: 0.8099, Test Loss: 0.9425
Epoch: 65, Training Loss: 0.8002, Test Loss: 0.9369

 37%|████████████████████████████████████                                                              | 92/250 [00:07<00:12, 12.20it/s, loss_test=0.900]
Epoch: 67, Training Loss: 0.7986, Test Loss: 0.9534
Epoch: 68, Training Loss: 0.7923, Test Loss: 0.9418
Epoch: 69, Training Loss: 0.7897, Test Loss: 0.9202
Epoch: 70, Training Loss: 0.7867, Test Loss: 0.9303
Epoch: 71, Training Loss: 0.7855, Test Loss: 0.9147
Epoch: 72, Training Loss: 0.7767, Test Loss: 0.9240
Epoch: 73, Training Loss: 0.7750, Test Loss: 0.9149
Epoch: 74, Training Loss: 0.7711, Test Loss: 0.9179
Epoch: 75, Training Loss: 0.7670, Test Loss: 0.9279
Epoch: 76, Training Loss: 0.7597, Test Loss: 0.9215
Epoch: 77, Training Loss: 0.7564, Test Loss: 0.9109
Epoch: 78, Training Loss: 0.7523, Test Loss: 0.9019
Epoch: 79, Training Loss: 0.7465, Test Loss: 0.9151
Epoch: 80, Training Loss: 0.7452, Test Loss: 0.9130
Epoch: 81, Training Loss: 0.7398, Test Loss: 0.9127
Epoch: 82, Training Loss: 0.7360, Test Loss: 0.9074
Epoch: 83, Training Loss: 0.7318, Test Loss: 0.9000
Epoch: 84, Training Loss: 0.7304, Test Loss: 0.9063
Epoch: 85, Training Loss: 0.7249, Test Loss: 0.9002
Epoch: 86, Training Loss: 0.7226, Test Loss: 0.9033
Epoch: 87, Training Loss: 0.7180, Test Loss: 0.8967
Epoch: 88, Training Loss: 0.7140, Test Loss: 0.8910
Epoch: 89, Training Loss: 0.7131, Test Loss: 0.9014
Epoch: 90, Training Loss: 0.7082, Test Loss: 0.8938

 46%|█████████████████████████████████████████████                                                    | 116/250 [00:09<00:11, 12.06it/s, loss_test=0.890]
Epoch: 92, Training Loss: 0.7001, Test Loss: 0.8896
Epoch: 93, Training Loss: 0.6952, Test Loss: 0.9004
Epoch: 94, Training Loss: 0.6918, Test Loss: 0.8896
Epoch: 95, Training Loss: 0.6893, Test Loss: 0.8878
Epoch: 96, Training Loss: 0.6857, Test Loss: 0.8944
Epoch: 97, Training Loss: 0.6805, Test Loss: 0.8821
Epoch: 98, Training Loss: 0.6742, Test Loss: 0.8855
Epoch: 99, Training Loss: 0.6740, Test Loss: 0.8708
Epoch: 100, Training Loss: 0.6740, Test Loss: 0.8760
Epoch: 101, Training Loss: 0.6692, Test Loss: 0.8855
Epoch: 102, Training Loss: 0.6636, Test Loss: 0.8804
Epoch: 103, Training Loss: 0.6604, Test Loss: 0.8876
Epoch: 104, Training Loss: 0.6571, Test Loss: 0.8886
Epoch: 105, Training Loss: 0.6535, Test Loss: 0.8836
Epoch: 106, Training Loss: 0.6505, Test Loss: 0.8876
Epoch: 107, Training Loss: 0.6479, Test Loss: 0.8747
Epoch: 108, Training Loss: 0.6445, Test Loss: 0.8793
Epoch: 109, Training Loss: 0.6395, Test Loss: 0.8978
Epoch: 110, Training Loss: 0.6363, Test Loss: 0.8888
Epoch: 111, Training Loss: 0.6320, Test Loss: 0.8774
Epoch: 112, Training Loss: 0.6284, Test Loss: 0.8754
Epoch: 113, Training Loss: 0.6254, Test Loss: 0.8849
Epoch: 114, Training Loss: 0.6237, Test Loss: 0.8950
Epoch: 115, Training Loss: 0.6215, Test Loss: 0.8903
Epoch: 116, Training Loss: 0.6177, Test Loss: 0.9114
Epoch: 117, Training Loss: 0.6133, Test Loss: 0.8951
Epoch: 118, Training Loss: 0.6079, Test Loss: 0.8991
Epoch: 119, Training Loss: 0.6053, Test Loss: 0.9006
Epoch: 120, Training Loss: 0.6043, Test Loss: 0.9081
Epoch: 121, Training Loss: 0.5993, Test Loss: 0.8989
Epoch: 122, Training Loss: 0.5991, Test Loss: 0.9053
Epoch: 123, Training Loss: 0.5940, Test Loss: 0.8993
Epoch: 124, Training Loss: 0.5941, Test Loss: 0.9009
Epoch: 125, Training Loss: 0.5926, Test Loss: 0.9066
Epoch: 126, Training Loss: 0.5863, Test Loss: 0.9064
Epoch: 127, Training Loss: 0.5831, Test Loss: 0.9027
Epoch: 128, Training Loss: 0.5811, Test Loss: 0.9040
Epoch: 129, Training Loss: 0.5773, Test Loss: 0.9165
Epoch: 130, Training Loss: 0.5723, Test Loss: 0.9085
Epoch: 131, Training Loss: 0.5693, Test Loss: 0.9105
Epoch: 132, Training Loss: 0.5664, Test Loss: 0.9087
Epoch: 133, Training Loss: 0.5650, Test Loss: 0.9169
Epoch: 134, Training Loss: 0.5630, Test Loss: 0.9215
Epoch: 135, Training Loss: 0.5585, Test Loss: 0.9206
Epoch: 136, Training Loss: 0.5583, Test Loss: 0.9153
Epoch: 137, Training Loss: 0.5542, Test Loss: 0.9173
Epoch: 138, Training Loss: 0.5525, Test Loss: 0.9178
Epoch: 139, Training Loss: 0.5512, Test Loss: 0.9202

 56%|██████████████████████████████████████████████████████▎                                          | 140/250 [00:11<00:09, 11.83it/s, loss_test=0.917]
Epoch: 141, Training Loss: 0.5438, Test Loss: 0.9085
Epoch: 142, Training Loss: 0.5401, Test Loss: 0.9207
Epoch: 143, Training Loss: 0.5360, Test Loss: 0.9209
Epoch: 144, Training Loss: 0.5340, Test Loss: 0.9136
Epoch: 145, Training Loss: 0.5318, Test Loss: 0.9273
Epoch: 146, Training Loss: 0.5261, Test Loss: 0.9190
Epoch: 147, Training Loss: 0.5263, Test Loss: 0.9167
Epoch: 148, Training Loss: 0.5240, Test Loss: 0.9186
Epoch: 149, Training Loss: 0.5211, Test Loss: 0.9261
Epoch: 150, Training Loss: 0.5179, Test Loss: 0.9269
Epoch: 151, Training Loss: 0.5158, Test Loss: 0.9206
Epoch: 152, Training Loss: 0.5133, Test Loss: 0.9171
Epoch: 153, Training Loss: 0.5129, Test Loss: 0.9322
Epoch: 154, Training Loss: 0.5087, Test Loss: 0.9289
Epoch: 155, Training Loss: 0.5044, Test Loss: 0.9362
Epoch: 156, Training Loss: 0.5013, Test Loss: 0.9362
Epoch: 157, Training Loss: 0.5009, Test Loss: 0.9350
Epoch: 158, Training Loss: 0.4995, Test Loss: 0.9238
Epoch: 159, Training Loss: 0.4949, Test Loss: 0.9370
Epoch: 160, Training Loss: 0.4930, Test Loss: 0.9362
Epoch: 161, Training Loss: 0.4918, Test Loss: 0.9393
Epoch: 162, Training Loss: 0.4884, Test Loss: 0.9264
Epoch: 163, Training Loss: 0.4859, Test Loss: 0.9369

 66%|███████████████████████████████████████████████████████████████▋                                 | 164/250 [00:13<00:07, 12.18it/s, loss_test=0.925]
Epoch: 165, Training Loss: 0.4806, Test Loss: 0.9285
Epoch: 166, Training Loss: 0.4773, Test Loss: 0.9392
Epoch: 167, Training Loss: 0.4767, Test Loss: 0.9239
Epoch: 168, Training Loss: 0.4731, Test Loss: 0.9276
Epoch: 169, Training Loss: 0.4701, Test Loss: 0.9374
Epoch: 170, Training Loss: 0.4701, Test Loss: 0.9478
Epoch: 171, Training Loss: 0.4678, Test Loss: 0.9282
Epoch: 172, Training Loss: 0.4647, Test Loss: 0.9386
Epoch: 173, Training Loss: 0.4616, Test Loss: 0.9330
Epoch: 174, Training Loss: 0.4586, Test Loss: 0.9382
Epoch: 175, Training Loss: 0.4579, Test Loss: 0.9462
Epoch: 176, Training Loss: 0.4549, Test Loss: 0.9282
Epoch: 177, Training Loss: 0.4518, Test Loss: 0.9423
Epoch: 178, Training Loss: 0.4527, Test Loss: 0.9604
Epoch: 179, Training Loss: 0.4475, Test Loss: 0.9386
Epoch: 180, Training Loss: 0.4442, Test Loss: 0.9432
Epoch: 181, Training Loss: 0.4415, Test Loss: 0.9550
Epoch: 182, Training Loss: 0.4405, Test Loss: 0.9424
Epoch: 183, Training Loss: 0.4396, Test Loss: 0.9423
Epoch: 184, Training Loss: 0.4371, Test Loss: 0.9555
Epoch: 185, Training Loss: 0.4350, Test Loss: 0.9493
Epoch: 186, Training Loss: 0.4334, Test Loss: 0.9546
Epoch: 187, Training Loss: 0.4309, Test Loss: 0.9544
Epoch: 188, Training Loss: 0.4288, Test Loss: 0.9380


 86%|███████████████████████████████████████████████████████████████████████████████████              | 214/250 [00:17<00:03, 11.83it/s, loss_test=0.977]
Epoch: 190, Training Loss: 0.4226, Test Loss: 0.9516
Epoch: 191, Training Loss: 0.4201, Test Loss: 0.9532
Epoch: 192, Training Loss: 0.4179, Test Loss: 0.9437
Epoch: 193, Training Loss: 0.4165, Test Loss: 0.9460
Epoch: 194, Training Loss: 0.4168, Test Loss: 0.9423
Epoch: 195, Training Loss: 0.4129, Test Loss: 0.9483
Epoch: 196, Training Loss: 0.4102, Test Loss: 0.9513
Epoch: 197, Training Loss: 0.4081, Test Loss: 0.9692
Epoch: 198, Training Loss: 0.4067, Test Loss: 0.9530
Epoch: 199, Training Loss: 0.4037, Test Loss: 0.9554
Epoch: 200, Training Loss: 0.4015, Test Loss: 0.9524
Epoch: 201, Training Loss: 0.3988, Test Loss: 0.9533
Epoch: 202, Training Loss: 0.3979, Test Loss: 0.9543
Epoch: 203, Training Loss: 0.3948, Test Loss: 0.9623
Epoch: 204, Training Loss: 0.3948, Test Loss: 0.9524
Epoch: 205, Training Loss: 0.3913, Test Loss: 0.9605
Epoch: 206, Training Loss: 0.3892, Test Loss: 0.9577
Epoch: 207, Training Loss: 0.3887, Test Loss: 0.9656
Epoch: 208, Training Loss: 0.3843, Test Loss: 0.9568
Epoch: 209, Training Loss: 0.3825, Test Loss: 0.9623
Epoch: 210, Training Loss: 0.3800, Test Loss: 0.9535
Epoch: 211, Training Loss: 0.3800, Test Loss: 0.9654
Epoch: 212, Training Loss: 0.3765, Test Loss: 0.9697
Epoch: 213, Training Loss: 0.3752, Test Loss: 0.9772
Epoch: 214, Training Loss: 0.3735, Test Loss: 0.9746
Epoch: 215, Training Loss: 0.3722, Test Loss: 0.9563
Epoch: 216, Training Loss: 0.3707, Test Loss: 0.9730
Epoch: 217, Training Loss: 0.3689, Test Loss: 0.9521
Epoch: 218, Training Loss: 0.3674, Test Loss: 0.9740
Epoch: 219, Training Loss: 0.3636, Test Loss: 0.9650
Epoch: 220, Training Loss: 0.3638, Test Loss: 0.9719
Epoch: 221, Training Loss: 0.3616, Test Loss: 0.9689
Epoch: 222, Training Loss: 0.3576, Test Loss: 0.9522
Epoch: 223, Training Loss: 0.3589, Test Loss: 0.9604
Epoch: 224, Training Loss: 0.3557, Test Loss: 0.9713
Epoch: 225, Training Loss: 0.3541, Test Loss: 0.9790
Epoch: 226, Training Loss: 0.3523, Test Loss: 0.9745
Epoch: 227, Training Loss: 0.3503, Test Loss: 0.9825
Epoch: 228, Training Loss: 0.3488, Test Loss: 0.9691
Epoch: 229, Training Loss: 0.3484, Test Loss: 0.9682
Epoch: 230, Training Loss: 0.3468, Test Loss: 0.9784
Epoch: 231, Training Loss: 0.3423, Test Loss: 0.9847
Epoch: 232, Training Loss: 0.3429, Test Loss: 0.9630
Epoch: 233, Training Loss: 0.3407, Test Loss: 0.9843
Epoch: 234, Training Loss: 0.3394, Test Loss: 0.9716
Epoch: 235, Training Loss: 0.3373, Test Loss: 0.9700


100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 11.99it/s, loss_test=0.976]
Epoch: 237, Training Loss: 0.3339, Test Loss: 0.9795
Epoch: 238, Training Loss: 0.3324, Test Loss: 0.9735
Epoch: 239, Training Loss: 0.3305, Test Loss: 0.9897
Epoch: 240, Training Loss: 0.3294, Test Loss: 0.9723
Epoch: 241, Training Loss: 0.3279, Test Loss: 0.9807
Epoch: 242, Training Loss: 0.3259, Test Loss: 0.9847
Epoch: 243, Training Loss: 0.3247, Test Loss: 0.9805
Epoch: 244, Training Loss: 0.3225, Test Loss: 0.9733
Epoch: 245, Training Loss: 0.3205, Test Loss: 0.9862
Epoch: 246, Training Loss: 0.3200, Test Loss: 0.9921
Epoch: 247, Training Loss: 0.3178, Test Loss: 0.9990
Epoch: 248, Training Loss: 0.3180, Test Loss: 0.9766
Epoch: 249, Training Loss: 0.3161, Test Loss: 0.9763
Model saved as model_6113829np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-126000-6113829np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9968311548233032, 0.9932987332344055, 0.99388188123703, 0.9944535374641419, 0.9933161020278931, 0.9880218505859375, 0.989116621017456, 0.9901234030723571, 0.9863953232765198, 0.9909511804580688, 0.9898616552352906, 0.990550410747528, 0.9901648044586182, 0.9907333016395569, 0.9871722459793091, 0.9884498834609985, 0.9852599263191223, 0.9841667413711548, 0.9842717409133911, 0.9787392735481262, 0.9874528288841248, 0.9792684555053711, 0.9842885494232178, 0.9812542796134949, 0.9844431638717651, 0.978849470615387, 0.9826266169548035, 0.9810820698738099, 0.9781558156013489, 0.9861643433570861, 0.9799905300140381, 0.9756390929222107, 0.9771442413330078, 0.9697404980659485, 0.9684005975723267, 0.9665856122970581, 0.9591040134429931, 0.9461220741271973, 0.9403100490570069, 0.9333539724349975, 0.9233299136161804, 0.9177028656005859, 0.9128620386123657, 0.9031405687332154, 0.8966340541839599, 0.8963940262794494, 0.8860652685165405, 0.8812389731407165, 0.8789461970329284, 0.8740704417228699, 0.8647510170936584, 0.8689360022544861, 0.8562663197517395, 0.8532402873039245, 0.8560566544532776, 0.8426599383354187, 0.8421358346939087, 0.8376408457756043, 0.8329293489456177, 0.8268054246902465, 0.8248990893363952, 0.8189141631126404, 0.8155777812004089, 0.8114738821983337, 0.8098519325256348, 0.8002379298210144, 0.8033272981643677, 0.7985711216926574, 0.7922927618026734, 0.7896655559539795, 0.7866600751876831, 0.7854750871658325, 0.7766634702682496, 0.7750415086746216, 0.7711109519004822, 0.7669929504394531, 0.7597327828407288, 0.7563752889633178, 0.7523455142974853, 0.7465441346168518, 0.7451597332954407, 0.7398434042930603, 0.7360249042510987, 0.7317874550819397, 0.7303697228431701, 0.7248668313026428, 0.722560703754425, 0.7180405855178833, 0.7140133857727051, 0.7130994439125061, 0.7081551313400268, 0.7035258531570434, 0.700083065032959, 0.6952043294906616, 0.6917787075042725, 0.6893252730369568, 0.6856918692588806, 0.6805455088615417, 0.6741814494132996, 0.6739792466163635, 0.6739926338195801, 0.669171929359436, 0.6635512590408326, 0.6603554606437683, 0.6571159482002258, 0.6534917712211609, 0.6504908084869385, 0.6478707194328308, 0.6445330142974853, 0.639459240436554, 0.6362772107124328, 0.6319960355758667, 0.6284194707870483, 0.6254008412361145, 0.6237337827682495, 0.621527123451233, 0.6177131533622742, 0.6133148193359375, 0.6079277157783508, 0.6053340554237365, 0.6043440818786621, 0.5993467926979065, 0.5991346955299377, 0.5939880251884461, 0.594054663181305, 0.5926426410675049, 0.5862972855567932, 0.5830723881721497, 0.5811114549636841, 0.5772897720336914, 0.572254228591919, 0.5693414092063904, 0.5664395689964294, 0.5650339245796203, 0.5629720449447632, 0.5585037469863892, 0.558306074142456, 0.5541714668273926, 0.5524549603462219, 0.5512312054634094, 0.5457996726036072, 0.543795132637024, 0.5400972843170166, 0.5360361933708191, 0.5340093612670899, 0.5318198680877686, 0.5260905861854553, 0.5263288974761963, 0.5239946782588959, 0.521129310131073, 0.5178968071937561, 0.5157851338386535, 0.5133309602737427, 0.5129250645637512, 0.5087031364440918, 0.5044111669063568, 0.5012736856937409, 0.5009445071220398, 0.4995110690593719, 0.49494646191596986, 0.49302868247032167, 0.49178858399391173, 0.48839612007141114, 0.4859088182449341, 0.48278815150260923, 0.480603951215744, 0.47725685238838195, 0.4766538739204407, 0.4731045126914978, 0.4700958371162415, 0.4700945496559143, 0.46775665283203127, 0.4646967887878418, 0.46156570315361023, 0.45857704877853395, 0.4578932344913483, 0.4548988163471222, 0.45182823538780215, 0.45273821949958803, 0.4474665462970734, 0.44419407844543457, 0.4415353000164032, 0.44051513671875, 0.439580899477005, 0.4370871424674988, 0.43500571250915526, 0.4334408938884735, 0.43091591000556945, 0.4287658631801605, 0.42461988925933836, 0.422647500038147, 0.42014533281326294, 0.41787284016609194, 0.41648035049438475, 0.41678454279899596, 0.41287411451339723, 0.4101882874965668, 0.4081157922744751, 0.4067454278469086, 0.4036894619464874, 0.40149431228637694, 0.39876399636268617, 0.39786574244499207, 0.39484349489212034, 0.3947655439376831, 0.391296261548996, 0.3891940891742706, 0.38872688412666323, 0.38434276580810545, 0.3824927031993866, 0.3799725413322449, 0.3799989640712738, 0.3764570415019989, 0.37521464228630064, 0.37351990938186647, 0.3722171366214752, 0.37065439224243163, 0.36885095238685606, 0.3673624753952026, 0.3636078774929047, 0.3637843132019043, 0.3615707218647003, 0.35763946175575256, 0.3589107573032379, 0.35570389628410337, 0.3541231513023376, 0.35225251913070676, 0.3503003418445587, 0.3487756550312042, 0.34843568205833436, 0.3468353867530823, 0.3423410177230835, 0.3428827941417694, 0.340735536813736, 0.3393850803375244, 0.3373139798641205, 0.33667572736740115, 0.33394336700439453, 0.33243786096572875, 0.33052304983139036, 0.3293509244918823, 0.3279101848602295, 0.3259051501750946, 0.32466658353805544, 0.3224862217903137, 0.3205267548561096, 0.3199628949165344, 0.31776472330093386, 0.31803479194641116, 0.31606037020683286], 'loss_test': [1.0769848823547363, 1.0785510540008545, 1.0761600732803345, 1.0702227354049683, 1.0768699645996094, 1.0760518312454224, 1.0838772058486938, 1.080100417137146, 1.0805082321166992, 1.0676147937774658, 1.079637050628662, 1.0966217517852783, 1.0780783891677856, 1.0733329057693481, 1.0802758932113647, 1.0805782079696655, 1.0855178833007812, 1.0746877193450928, 1.0836924314498901, 1.0800338983535767, 1.0805712938308716, 1.085484266281128, 1.0647069215774536, 1.0866960287094116, 1.0626581907272339, 1.0957258939743042, 1.0744125843048096, 1.0795481204986572, 1.0809342861175537, 1.0672721862792969, 1.0864112377166748, 1.0924237966537476, 1.0872021913528442, 1.0657025575637817, 1.0759042501449585, 1.042988657951355, 1.069568395614624, 1.073405385017395, 1.0595893859863281, 1.0419474840164185, 1.0286791324615479, 1.0394574403762817, 1.0225647687911987, 1.017596960067749, 1.0136317014694214, 1.0179200172424316, 1.0193147659301758, 1.0052337646484375, 1.0044318437576294, 0.9792590737342834, 1.003490924835205, 0.9995174407958984, 0.9949951171875, 0.9852895140647888, 0.9851199984550476, 0.9861564040184021, 0.9843792915344238, 0.9776023030281067, 0.9707669019699097, 0.951930582523346, 0.9632222652435303, 0.9626156687736511, 0.9457575678825378, 0.9390133619308472, 0.9425287842750549, 0.9368959665298462, 0.9276729226112366, 0.9533925652503967, 0.9418216347694397, 0.9202439188957214, 0.9302770495414734, 0.9147289991378784, 0.9240483641624451, 0.9149265289306641, 0.9179279208183289, 0.9278927445411682, 0.9214594960212708, 0.9109156727790833, 0.9018934965133667, 0.9150713086128235, 0.9129529595375061, 0.9126608967781067, 0.9074048399925232, 0.899994432926178, 0.9063483476638794, 0.9002140760421753, 0.9032669067382812, 0.8966635465621948, 0.8909532427787781, 0.9014430046081543, 0.8937781453132629, 0.9000261425971985, 0.8896158933639526, 0.9003987908363342, 0.8896400928497314, 0.8877918124198914, 0.8943513631820679, 0.8820521235466003, 0.8855087757110596, 0.8708325028419495, 0.8760195374488831, 0.8855218887329102, 0.8803533315658569, 0.8876376748085022, 0.8886128664016724, 0.8836050629615784, 0.8875719904899597, 0.8747443556785583, 0.8792966604232788, 0.8977810740470886, 0.888778805732727, 0.8774409294128418, 0.8754028677940369, 0.8849067091941833, 0.8949552774429321, 0.8903436064720154, 0.9114035964012146, 0.8950799703598022, 0.899094820022583, 0.900631308555603, 0.9080978631973267, 0.8988513946533203, 0.9052850008010864, 0.8992719054222107, 0.9009100198745728, 0.9066428542137146, 0.9064392447471619, 0.902714192867279, 0.9039945006370544, 0.9164538979530334, 0.9084743857383728, 0.9105352163314819, 0.9086506366729736, 0.916860044002533, 0.9214664101600647, 0.9206287264823914, 0.9152981638908386, 0.9172745943069458, 0.9177566766738892, 0.9201635122299194, 0.9171876311302185, 0.9084650874137878, 0.9207391142845154, 0.9209080338478088, 0.9135812520980835, 0.9273480176925659, 0.9189781546592712, 0.9166765213012695, 0.918552815914154, 0.9261457920074463, 0.9269497394561768, 0.9206002354621887, 0.9171150326728821, 0.9322473406791687, 0.9289241433143616, 0.9362109899520874, 0.9361635446548462, 0.935022234916687, 0.9237610697746277, 0.9369612336158752, 0.9361838698387146, 0.939256489276886, 0.926440954208374, 0.9368917942047119, 0.9252960681915283, 0.928533136844635, 0.9392159581184387, 0.9239261150360107, 0.9276291728019714, 0.9373603463172913, 0.9477894306182861, 0.9281874299049377, 0.938613772392273, 0.9330095052719116, 0.9381802082061768, 0.9462164640426636, 0.9282376170158386, 0.9423040747642517, 0.960435688495636, 0.9385610818862915, 0.9432353973388672, 0.9550408720970154, 0.9424359798431396, 0.9422601461410522, 0.9554643034934998, 0.949315071105957, 0.9546190500259399, 0.9544470906257629, 0.9380165934562683, 0.9590807557106018, 0.951569139957428, 0.9532289505004883, 0.9437388181686401, 0.9459673166275024, 0.942322313785553, 0.9482820630073547, 0.9513397216796875, 0.969180166721344, 0.9529639482498169, 0.9554035067558289, 0.9523678421974182, 0.9533179402351379, 0.9542562365531921, 0.9622780680656433, 0.9524341225624084, 0.96047043800354, 0.9576714038848877, 0.9656050205230713, 0.9567926526069641, 0.9622814655303955, 0.9534566402435303, 0.9653864502906799, 0.9697403907775879, 0.9771974682807922, 0.9746265411376953, 0.9562680125236511, 0.9730160236358643, 0.9521324634552002, 0.9740056395530701, 0.9650075435638428, 0.9719470739364624, 0.9689285755157471, 0.9522338509559631, 0.9604355096817017, 0.9712554216384888, 0.9790040850639343, 0.9744929075241089, 0.9824993014335632, 0.9690512418746948, 0.9682015776634216, 0.9784388542175293, 0.9847226142883301, 0.9630295038223267, 0.9842935800552368, 0.9715939164161682, 0.9700412750244141, 0.9722449779510498, 0.9794608354568481, 0.9734895825386047, 0.9896525144577026, 0.9723281860351562, 0.9806658625602722, 0.9847415685653687, 0.9805442094802856, 0.9733010530471802, 0.986224889755249, 0.992051899433136, 0.9990453124046326, 0.9765951633453369, 0.9763271808624268], 'identifier': '6113829np'}