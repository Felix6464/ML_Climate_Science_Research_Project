
  8%|████████▏                                                                                         | 21/250 [00:01<00:19, 11.45it/s, loss_test=1.082]
Epoch: 00, Training Loss: 0.9914, Test Loss: 1.0892
Epoch: 01, Training Loss: 0.9934, Test Loss: 1.0754
Epoch: 02, Training Loss: 0.9919, Test Loss: 1.0786
Epoch: 03, Training Loss: 0.9909, Test Loss: 1.0616
Epoch: 04, Training Loss: 0.9924, Test Loss: 1.0785
Epoch: 05, Training Loss: 0.9897, Test Loss: 1.0916
Epoch: 06, Training Loss: 0.9947, Test Loss: 1.0747
Epoch: 07, Training Loss: 0.9933, Test Loss: 1.0772
Epoch: 08, Training Loss: 0.9917, Test Loss: 1.0725
Epoch: 09, Training Loss: 0.9970, Test Loss: 1.0782
Epoch: 10, Training Loss: 0.9896, Test Loss: 1.0842
Epoch: 11, Training Loss: 0.9867, Test Loss: 1.0723
Epoch: 12, Training Loss: 0.9848, Test Loss: 1.0716
Epoch: 13, Training Loss: 0.9868, Test Loss: 1.0720
Epoch: 14, Training Loss: 0.9825, Test Loss: 1.0791
Epoch: 15, Training Loss: 0.9880, Test Loss: 1.0791
Epoch: 16, Training Loss: 0.9864, Test Loss: 1.0840
Epoch: 17, Training Loss: 0.9876, Test Loss: 1.0791
Epoch: 18, Training Loss: 0.9867, Test Loss: 1.0809
Epoch: 19, Training Loss: 0.9860, Test Loss: 1.0857

 18%|█████████████████▋                                                                                | 45/250 [00:03<00:16, 12.41it/s, loss_test=0.997]
Epoch: 21, Training Loss: 0.9795, Test Loss: 1.0739
Epoch: 22, Training Loss: 0.9791, Test Loss: 1.0805
Epoch: 23, Training Loss: 0.9878, Test Loss: 1.0750
Epoch: 24, Training Loss: 0.9815, Test Loss: 1.0854
Epoch: 25, Training Loss: 0.9820, Test Loss: 1.0825
Epoch: 26, Training Loss: 0.9811, Test Loss: 1.0786
Epoch: 27, Training Loss: 0.9802, Test Loss: 1.0614
Epoch: 28, Training Loss: 0.9844, Test Loss: 1.0644
Epoch: 29, Training Loss: 0.9726, Test Loss: 1.0868
Epoch: 30, Training Loss: 0.9675, Test Loss: 1.0806
Epoch: 31, Training Loss: 0.9624, Test Loss: 1.0819
Epoch: 32, Training Loss: 0.9579, Test Loss: 1.0839
Epoch: 33, Training Loss: 0.9486, Test Loss: 1.0777
Epoch: 34, Training Loss: 0.9422, Test Loss: 1.0452
Epoch: 35, Training Loss: 0.9391, Test Loss: 1.0552
Epoch: 36, Training Loss: 0.9334, Test Loss: 1.0451
Epoch: 37, Training Loss: 0.9301, Test Loss: 1.0323
Epoch: 38, Training Loss: 0.9188, Test Loss: 1.0416
Epoch: 39, Training Loss: 0.9159, Test Loss: 1.0266
Epoch: 40, Training Loss: 0.9119, Test Loss: 1.0324
Epoch: 41, Training Loss: 0.9067, Test Loss: 0.9986
Epoch: 42, Training Loss: 0.9019, Test Loss: 1.0063
Epoch: 43, Training Loss: 0.8963, Test Loss: 0.9970

 27%|██████████████████████████▎                                                                       | 67/250 [00:05<00:15, 11.75it/s, loss_test=0.927]
Epoch: 45, Training Loss: 0.8877, Test Loss: 1.0026
Epoch: 46, Training Loss: 0.8859, Test Loss: 1.0011
Epoch: 47, Training Loss: 0.8822, Test Loss: 1.0007
Epoch: 48, Training Loss: 0.8783, Test Loss: 0.9980
Epoch: 49, Training Loss: 0.8733, Test Loss: 0.9747
Epoch: 50, Training Loss: 0.8695, Test Loss: 0.9797
Epoch: 51, Training Loss: 0.8683, Test Loss: 0.9728
Epoch: 52, Training Loss: 0.8618, Test Loss: 0.9804
Epoch: 53, Training Loss: 0.8593, Test Loss: 0.9761
Epoch: 54, Training Loss: 0.8512, Test Loss: 0.9767
Epoch: 55, Training Loss: 0.8503, Test Loss: 0.9728
Epoch: 56, Training Loss: 0.8495, Test Loss: 0.9749
Epoch: 57, Training Loss: 0.8424, Test Loss: 0.9600
Epoch: 58, Training Loss: 0.8404, Test Loss: 0.9589
Epoch: 59, Training Loss: 0.8354, Test Loss: 0.9603
Epoch: 60, Training Loss: 0.8313, Test Loss: 0.9697
Epoch: 61, Training Loss: 0.8305, Test Loss: 0.9491
Epoch: 62, Training Loss: 0.8216, Test Loss: 0.9441
Epoch: 63, Training Loss: 0.8180, Test Loss: 0.9519
Epoch: 64, Training Loss: 0.8124, Test Loss: 0.9392
Epoch: 65, Training Loss: 0.8069, Test Loss: 0.9492

 36%|███████████████████████████████████▋                                                              | 91/250 [00:07<00:13, 12.08it/s, loss_test=0.909]
Epoch: 67, Training Loss: 0.8025, Test Loss: 0.9273
Epoch: 68, Training Loss: 0.7989, Test Loss: 0.9398
Epoch: 69, Training Loss: 0.7964, Test Loss: 0.9256
Epoch: 70, Training Loss: 0.7916, Test Loss: 0.9269
Epoch: 71, Training Loss: 0.7853, Test Loss: 0.9185
Epoch: 72, Training Loss: 0.7842, Test Loss: 0.9107
Epoch: 73, Training Loss: 0.7789, Test Loss: 0.9115
Epoch: 74, Training Loss: 0.7771, Test Loss: 0.9335
Epoch: 75, Training Loss: 0.7691, Test Loss: 0.9148
Epoch: 76, Training Loss: 0.7693, Test Loss: 0.9109
Epoch: 77, Training Loss: 0.7624, Test Loss: 0.9134
Epoch: 78, Training Loss: 0.7575, Test Loss: 0.9046
Epoch: 79, Training Loss: 0.7580, Test Loss: 0.9041
Epoch: 80, Training Loss: 0.7502, Test Loss: 0.8937
Epoch: 81, Training Loss: 0.7486, Test Loss: 0.8988
Epoch: 82, Training Loss: 0.7444, Test Loss: 0.9161
Epoch: 83, Training Loss: 0.7407, Test Loss: 0.9171
Epoch: 84, Training Loss: 0.7375, Test Loss: 0.9128
Epoch: 85, Training Loss: 0.7340, Test Loss: 0.9061
Epoch: 86, Training Loss: 0.7285, Test Loss: 0.9000
Epoch: 87, Training Loss: 0.7277, Test Loss: 0.9015
Epoch: 88, Training Loss: 0.7232, Test Loss: 0.9030
Epoch: 89, Training Loss: 0.7193, Test Loss: 0.8927
Epoch: 90, Training Loss: 0.7131, Test Loss: 0.9031

 47%|█████████████████████████████████████████████▍                                                   | 117/250 [00:09<00:11, 11.90it/s, loss_test=0.923]
Epoch: 92, Training Loss: 0.7065, Test Loss: 0.9055
Epoch: 93, Training Loss: 0.7021, Test Loss: 0.9025
Epoch: 94, Training Loss: 0.6990, Test Loss: 0.9018
Epoch: 95, Training Loss: 0.6940, Test Loss: 0.9031
Epoch: 96, Training Loss: 0.6912, Test Loss: 0.8978
Epoch: 97, Training Loss: 0.6884, Test Loss: 0.9066
Epoch: 98, Training Loss: 0.6846, Test Loss: 0.9073
Epoch: 99, Training Loss: 0.6783, Test Loss: 0.9040
Epoch: 100, Training Loss: 0.6770, Test Loss: 0.9081
Epoch: 101, Training Loss: 0.6741, Test Loss: 0.9081
Epoch: 102, Training Loss: 0.6699, Test Loss: 0.9056
Epoch: 103, Training Loss: 0.6672, Test Loss: 0.9133
Epoch: 104, Training Loss: 0.6617, Test Loss: 0.9091
Epoch: 105, Training Loss: 0.6611, Test Loss: 0.9063
Epoch: 106, Training Loss: 0.6556, Test Loss: 0.9210
Epoch: 107, Training Loss: 0.6542, Test Loss: 0.9053
Epoch: 108, Training Loss: 0.6491, Test Loss: 0.9137
Epoch: 109, Training Loss: 0.6450, Test Loss: 0.9090
Epoch: 110, Training Loss: 0.6460, Test Loss: 0.8992
Epoch: 111, Training Loss: 0.6409, Test Loss: 0.9060
Epoch: 112, Training Loss: 0.6366, Test Loss: 0.9057
Epoch: 113, Training Loss: 0.6335, Test Loss: 0.9202
Epoch: 114, Training Loss: 0.6324, Test Loss: 0.9167

 56%|██████████████████████████████████████████████████████▋                                          | 141/250 [00:11<00:08, 12.14it/s, loss_test=0.933]
Epoch: 116, Training Loss: 0.6249, Test Loss: 0.9232
Epoch: 117, Training Loss: 0.6232, Test Loss: 0.9103
Epoch: 118, Training Loss: 0.6187, Test Loss: 0.9107
Epoch: 119, Training Loss: 0.6169, Test Loss: 0.9092
Epoch: 120, Training Loss: 0.6144, Test Loss: 0.9235
Epoch: 121, Training Loss: 0.6093, Test Loss: 0.9183
Epoch: 122, Training Loss: 0.6064, Test Loss: 0.9138
Epoch: 123, Training Loss: 0.6049, Test Loss: 0.9136
Epoch: 124, Training Loss: 0.6002, Test Loss: 0.9199
Epoch: 125, Training Loss: 0.5956, Test Loss: 0.9301
Epoch: 126, Training Loss: 0.5961, Test Loss: 0.9308
Epoch: 127, Training Loss: 0.5930, Test Loss: 0.9259
Epoch: 128, Training Loss: 0.5891, Test Loss: 0.9280
Epoch: 129, Training Loss: 0.5865, Test Loss: 0.9215
Epoch: 130, Training Loss: 0.5813, Test Loss: 0.9220
Epoch: 131, Training Loss: 0.5806, Test Loss: 0.9218
Epoch: 132, Training Loss: 0.5749, Test Loss: 0.9351
Epoch: 133, Training Loss: 0.5722, Test Loss: 0.9177
Epoch: 134, Training Loss: 0.5667, Test Loss: 0.9328
Epoch: 135, Training Loss: 0.5678, Test Loss: 0.9402
Epoch: 136, Training Loss: 0.5612, Test Loss: 0.9379
Epoch: 137, Training Loss: 0.5583, Test Loss: 0.9377
Epoch: 138, Training Loss: 0.5562, Test Loss: 0.9309
Epoch: 139, Training Loss: 0.5532, Test Loss: 0.9248

 66%|████████████████████████████████████████████████████████████████                                 | 165/250 [00:13<00:07, 11.93it/s, loss_test=0.943]
Epoch: 141, Training Loss: 0.5470, Test Loss: 0.9328
Epoch: 142, Training Loss: 0.5442, Test Loss: 0.9482
Epoch: 143, Training Loss: 0.5386, Test Loss: 0.9452
Epoch: 144, Training Loss: 0.5389, Test Loss: 0.9506
Epoch: 145, Training Loss: 0.5353, Test Loss: 0.9267
Epoch: 146, Training Loss: 0.5317, Test Loss: 0.9355
Epoch: 147, Training Loss: 0.5285, Test Loss: 0.9291
Epoch: 148, Training Loss: 0.5238, Test Loss: 0.9286
Epoch: 149, Training Loss: 0.5219, Test Loss: 0.9448
Epoch: 150, Training Loss: 0.5217, Test Loss: 0.9512
Epoch: 151, Training Loss: 0.5170, Test Loss: 0.9533
Epoch: 152, Training Loss: 0.5137, Test Loss: 0.9395
Epoch: 153, Training Loss: 0.5121, Test Loss: 0.9430
Epoch: 154, Training Loss: 0.5097, Test Loss: 0.9442
Epoch: 155, Training Loss: 0.5072, Test Loss: 0.9507
Epoch: 156, Training Loss: 0.5023, Test Loss: 0.9455
Epoch: 157, Training Loss: 0.5010, Test Loss: 0.9454
Epoch: 158, Training Loss: 0.4981, Test Loss: 0.9434
Epoch: 159, Training Loss: 0.4951, Test Loss: 0.9452
Epoch: 160, Training Loss: 0.4938, Test Loss: 0.9467
Epoch: 161, Training Loss: 0.4904, Test Loss: 0.9537
Epoch: 162, Training Loss: 0.4880, Test Loss: 0.9429
Epoch: 163, Training Loss: 0.4837, Test Loss: 0.9493

 76%|█████████████████████████████████████████████████████████████████████████▎                       | 189/250 [00:15<00:05, 12.17it/s, loss_test=0.972]
Epoch: 165, Training Loss: 0.4812, Test Loss: 0.9435
Epoch: 166, Training Loss: 0.4783, Test Loss: 0.9560
Epoch: 167, Training Loss: 0.4745, Test Loss: 0.9474
Epoch: 168, Training Loss: 0.4742, Test Loss: 0.9536
Epoch: 169, Training Loss: 0.4679, Test Loss: 0.9474
Epoch: 170, Training Loss: 0.4683, Test Loss: 0.9585
Epoch: 171, Training Loss: 0.4647, Test Loss: 0.9662
Epoch: 172, Training Loss: 0.4639, Test Loss: 0.9609
Epoch: 173, Training Loss: 0.4583, Test Loss: 0.9551
Epoch: 174, Training Loss: 0.4566, Test Loss: 0.9604
Epoch: 175, Training Loss: 0.4553, Test Loss: 0.9586
Epoch: 176, Training Loss: 0.4525, Test Loss: 0.9438
Epoch: 177, Training Loss: 0.4501, Test Loss: 0.9595
Epoch: 178, Training Loss: 0.4490, Test Loss: 0.9567
Epoch: 179, Training Loss: 0.4462, Test Loss: 0.9642
Epoch: 180, Training Loss: 0.4449, Test Loss: 0.9643
Epoch: 181, Training Loss: 0.4404, Test Loss: 0.9435
Epoch: 182, Training Loss: 0.4383, Test Loss: 0.9696
Epoch: 183, Training Loss: 0.4384, Test Loss: 0.9681
Epoch: 184, Training Loss: 0.4334, Test Loss: 0.9442
Epoch: 185, Training Loss: 0.4296, Test Loss: 0.9612
Epoch: 186, Training Loss: 0.4291, Test Loss: 0.9560
Epoch: 187, Training Loss: 0.4288, Test Loss: 0.9586

 86%|███████████████████████████████████████████████████████████████████████████████████▍             | 215/250 [00:17<00:02, 12.41it/s, loss_test=0.991]
Epoch: 189, Training Loss: 0.4244, Test Loss: 0.9721
Epoch: 190, Training Loss: 0.4216, Test Loss: 0.9655
Epoch: 191, Training Loss: 0.4182, Test Loss: 0.9657
Epoch: 192, Training Loss: 0.4178, Test Loss: 0.9757
Epoch: 193, Training Loss: 0.4144, Test Loss: 0.9689
Epoch: 194, Training Loss: 0.4125, Test Loss: 0.9683
Epoch: 195, Training Loss: 0.4116, Test Loss: 0.9818
Epoch: 196, Training Loss: 0.4083, Test Loss: 0.9517
Epoch: 197, Training Loss: 0.4080, Test Loss: 0.9775
Epoch: 198, Training Loss: 0.4040, Test Loss: 0.9679
Epoch: 199, Training Loss: 0.4021, Test Loss: 0.9744
Epoch: 200, Training Loss: 0.4007, Test Loss: 0.9802
Epoch: 201, Training Loss: 0.3995, Test Loss: 0.9766
Epoch: 202, Training Loss: 0.3972, Test Loss: 0.9815
Epoch: 203, Training Loss: 0.3952, Test Loss: 0.9777
Epoch: 204, Training Loss: 0.3914, Test Loss: 0.9721
Epoch: 205, Training Loss: 0.3917, Test Loss: 0.9852
Epoch: 206, Training Loss: 0.3887, Test Loss: 0.9997
Epoch: 207, Training Loss: 0.3874, Test Loss: 0.9769
Epoch: 208, Training Loss: 0.3851, Test Loss: 0.9895
Epoch: 209, Training Loss: 0.3832, Test Loss: 0.9890
Epoch: 210, Training Loss: 0.3820, Test Loss: 0.9742
Epoch: 211, Training Loss: 0.3797, Test Loss: 0.9779
Epoch: 212, Training Loss: 0.3767, Test Loss: 0.9659

 96%|████████████████████████████████████████████████████████████████████████████████████████████▋    | 239/250 [00:19<00:00, 12.64it/s, loss_test=0.997]
Epoch: 214, Training Loss: 0.3722, Test Loss: 0.9914
Epoch: 215, Training Loss: 0.3719, Test Loss: 0.9814
Epoch: 216, Training Loss: 0.3694, Test Loss: 0.9956
Epoch: 217, Training Loss: 0.3680, Test Loss: 0.9924
Epoch: 218, Training Loss: 0.3668, Test Loss: 0.9770
Epoch: 219, Training Loss: 0.3644, Test Loss: 0.9847
Epoch: 220, Training Loss: 0.3629, Test Loss: 0.9935
Epoch: 221, Training Loss: 0.3612, Test Loss: 0.9748
Epoch: 222, Training Loss: 0.3586, Test Loss: 1.0037
Epoch: 223, Training Loss: 0.3572, Test Loss: 0.9847
Epoch: 224, Training Loss: 0.3565, Test Loss: 0.9850
Epoch: 225, Training Loss: 0.3557, Test Loss: 1.0012
Epoch: 226, Training Loss: 0.3521, Test Loss: 0.9811
Epoch: 227, Training Loss: 0.3520, Test Loss: 0.9915
Epoch: 228, Training Loss: 0.3494, Test Loss: 0.9829
Epoch: 229, Training Loss: 0.3477, Test Loss: 0.9959
Epoch: 230, Training Loss: 0.3447, Test Loss: 1.0058
Epoch: 231, Training Loss: 0.3437, Test Loss: 0.9832
Epoch: 232, Training Loss: 0.3429, Test Loss: 0.9862
Epoch: 233, Training Loss: 0.3407, Test Loss: 0.9855
Epoch: 234, Training Loss: 0.3381, Test Loss: 1.0129
Epoch: 235, Training Loss: 0.3385, Test Loss: 1.0012
Epoch: 236, Training Loss: 0.3365, Test Loss: 1.0154
Epoch: 237, Training Loss: 0.3341, Test Loss: 0.9978

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.07it/s, loss_test=1.011]
Epoch: 239, Training Loss: 0.3310, Test Loss: 0.9972
Epoch: 240, Training Loss: 0.3301, Test Loss: 1.0075
Epoch: 241, Training Loss: 0.3280, Test Loss: 1.0187
Epoch: 242, Training Loss: 0.3274, Test Loss: 1.0076
Epoch: 243, Training Loss: 0.3257, Test Loss: 1.0139
Epoch: 244, Training Loss: 0.3230, Test Loss: 1.0301
Epoch: 245, Training Loss: 0.3233, Test Loss: 1.0051
Epoch: 246, Training Loss: 0.3206, Test Loss: 1.0065
Epoch: 247, Training Loss: 0.3181, Test Loss: 0.9995
Epoch: 248, Training Loss: 0.3179, Test Loss: 0.9907
Epoch: 249, Training Loss: 0.3154, Test Loss: 1.0107
Model saved as model_7297090np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-12200000-7297090np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9913561820983887, 0.9933610677719116, 0.9919476866722107, 0.9909416556358337, 0.9923808574676514, 0.9896896839141845, 0.9946621298789978, 0.9932897567749024, 0.991674268245697, 0.9970290303230286, 0.9896174073219299, 0.9866610765457153, 0.9847805857658386, 0.9868123054504394, 0.9824532985687255, 0.9879616737365723, 0.9864449977874756, 0.9876279592514038, 0.9867308020591736, 0.985998022556305, 0.9871981739997864, 0.9795241236686707, 0.9791317939758301, 0.9878074407577515, 0.9815297603607178, 0.9820397615432739, 0.9811220765113831, 0.9801747798919678, 0.9843563318252564, 0.9726154327392578, 0.9674715638160706, 0.9623978495597839, 0.9579311847686768, 0.9486266255378724, 0.942171037197113, 0.9390852808952331, 0.9334312915802002, 0.9300848960876464, 0.9187713742256165, 0.915947413444519, 0.911882221698761, 0.9066518425941468, 0.9018930792808533, 0.8962647438049316, 0.8952280044555664, 0.8876625657081604, 0.8859236478805542, 0.8821739912033081, 0.8782813787460327, 0.8732765316963196, 0.869507622718811, 0.8682726263999939, 0.8618454098701477, 0.8592780232429504, 0.8511813998222351, 0.8503000974655152, 0.8494553446769715, 0.8424190759658814, 0.8403644323348999, 0.8354312539100647, 0.8312994360923767, 0.8304717540740967, 0.8215760469436646, 0.8179977655410766, 0.812430489063263, 0.8069115877151489, 0.8058927416801452, 0.8025483965873719, 0.7988740086555481, 0.7964128732681275, 0.7915927529335022, 0.7853490233421325, 0.784177589416504, 0.7789293885231018, 0.7770637392997741, 0.7691132068634033, 0.7693255543708801, 0.7624465942382812, 0.757464075088501, 0.7579569101333619, 0.7501896858215332, 0.7485617518424987, 0.7444485425949097, 0.7407415866851806, 0.7374557733535767, 0.7339558601379395, 0.7284921050071717, 0.7277152061462402, 0.7232011437416077, 0.7192881464958191, 0.7131450414657593, 0.7104762673377991, 0.706506872177124, 0.7020716667175293, 0.6989869832992553, 0.6939804911613464, 0.6911751866340637, 0.6884286522865295, 0.6845809340476989, 0.6782663583755493, 0.6770101428031922, 0.6740857839584351, 0.6699116230010986, 0.6672319054603577, 0.661692452430725, 0.6610725164413452, 0.6556052327156067, 0.654211676120758, 0.6491125226020813, 0.6450310468673706, 0.6460345029830933, 0.6409158110618591, 0.6365662097930909, 0.6335373163223267, 0.6324102997779846, 0.6302051305770874, 0.6248693227767944, 0.6232322096824646, 0.6186565160751343, 0.616928243637085, 0.6144219875335694, 0.6093302845954895, 0.6064411997795105, 0.6049481987953186, 0.6002259969711303, 0.5956454157829285, 0.5960814952850342, 0.5930191159248352, 0.5890802145004272, 0.586503529548645, 0.5812983632087707, 0.5806078910827637, 0.5748945355415345, 0.5721751689910889, 0.5667460560798645, 0.567794680595398, 0.5612054228782654, 0.5583198547363282, 0.5561922430992127, 0.5531754851341247, 0.5513214707374573, 0.5470341324806214, 0.5441900849342346, 0.5386053800582886, 0.5388914823532105, 0.5353127479553222, 0.5317276120185852, 0.5285432815551758, 0.5237569212913513, 0.5218549251556397, 0.5217216849327088, 0.5170152425765991, 0.5136962532997131, 0.5120736241340638, 0.5097361624240875, 0.5072141766548157, 0.5022876143455506, 0.5009932398796082, 0.498136967420578, 0.49512484669685364, 0.4937624096870422, 0.4904475033283234, 0.4880241692066193, 0.48371376991271975, 0.4837217092514038, 0.4811983108520508, 0.4783226251602173, 0.4745015144348145, 0.4741684079170227, 0.46787593364715574, 0.4683088779449463, 0.4646669328212738, 0.46386678218841554, 0.45829073786735536, 0.456617546081543, 0.45533748269081115, 0.4525199294090271, 0.45006691813468935, 0.44899106621742246, 0.44622676968574526, 0.4448788225650787, 0.4404448390007019, 0.4383289933204651, 0.4383709192276001, 0.43338067531585694, 0.42960801124572756, 0.4290563464164734, 0.42880255579948423, 0.42468189001083373, 0.42436925768852235, 0.4215696632862091, 0.4182158291339874, 0.417843896150589, 0.4143616795539856, 0.41249881982803344, 0.411562180519104, 0.40827116966247556, 0.4079567790031433, 0.40401928424835204, 0.40213578939437866, 0.40074750781059265, 0.39952494502067565, 0.39719581604003906, 0.3951637089252472, 0.3913700461387634, 0.3916565477848053, 0.38869253993034364, 0.3874213039875031, 0.38508865237236023, 0.38323902487754824, 0.3820070862770081, 0.37974616289138796, 0.37673680782318114, 0.37556018233299254, 0.37215153574943544, 0.37191004157066343, 0.3694376528263092, 0.3680333137512207, 0.36684409379959104, 0.36435201168060305, 0.362870591878891, 0.3611807107925415, 0.3585638999938965, 0.3571892619132996, 0.3565415501594543, 0.3556891977787018, 0.35212127566337587, 0.3519852340221405, 0.3494125843048096, 0.3477008640766144, 0.34474546909332277, 0.343665623664856, 0.34286047220230104, 0.3407291352748871, 0.3381101548671722, 0.3385319173336029, 0.33646937012672423, 0.3341281294822693, 0.3333303213119507, 0.3309522092342377, 0.33007981181144713, 0.3279725730419159, 0.32741779685020445, 0.3256550669670105, 0.3229678630828857, 0.32333452701568605, 0.3206296622753143, 0.3181217908859253, 0.31790804862976074, 0.31543023586273194], 'loss_test': [1.0892317295074463, 1.0753891468048096, 1.0786150693893433, 1.0615562200546265, 1.0784761905670166, 1.0915851593017578, 1.074652075767517, 1.077211856842041, 1.0725491046905518, 1.0781843662261963, 1.0841777324676514, 1.072298288345337, 1.0716443061828613, 1.072019100189209, 1.0791258811950684, 1.0790624618530273, 1.0839959383010864, 1.079123854637146, 1.080875039100647, 1.0857269763946533, 1.0820095539093018, 1.073944091796875, 1.0804680585861206, 1.0749694108963013, 1.0853736400604248, 1.08249831199646, 1.078626036643982, 1.0614374876022339, 1.064414620399475, 1.0868245363235474, 1.0806080102920532, 1.0819073915481567, 1.0838871002197266, 1.0777305364608765, 1.0452157258987427, 1.0552420616149902, 1.0451467037200928, 1.0323246717453003, 1.0415641069412231, 1.0266176462173462, 1.0324293375015259, 0.998600959777832, 1.006333589553833, 0.9970369338989258, 0.997431218624115, 1.0026416778564453, 1.0010994672775269, 1.000705361366272, 0.9980427622795105, 0.9747257232666016, 0.979668140411377, 0.9727931022644043, 0.9804169535636902, 0.9761027097702026, 0.9766788482666016, 0.9728414416313171, 0.9748871922492981, 0.9599575996398926, 0.9588658809661865, 0.9602731466293335, 0.969666063785553, 0.9490808248519897, 0.9441449046134949, 0.9519004225730896, 0.9391886591911316, 0.9492098093032837, 0.9197413325309753, 0.9272896647453308, 0.9397808909416199, 0.9256093502044678, 0.9268519282341003, 0.9184852838516235, 0.9107109904289246, 0.9114668369293213, 0.9334959387779236, 0.9147866368293762, 0.910932183265686, 0.9133616089820862, 0.9045652151107788, 0.9041212797164917, 0.8937074542045593, 0.898804247379303, 0.9160782098770142, 0.9171057939529419, 0.9128113985061646, 0.9060656428337097, 0.8999932408332825, 0.9015353322029114, 0.9029577970504761, 0.8927192091941833, 0.903083324432373, 0.908710241317749, 0.9055217504501343, 0.9025141596794128, 0.9018132090568542, 0.9031263589859009, 0.8977617025375366, 0.9065917134284973, 0.9073463082313538, 0.9039559364318848, 0.908064603805542, 0.9080530405044556, 0.9055694937705994, 0.9133292436599731, 0.9090805053710938, 0.9063117504119873, 0.9210174083709717, 0.9053284525871277, 0.9137430191040039, 0.9089794754981995, 0.899161696434021, 0.9060335755348206, 0.9056971669197083, 0.9201721549034119, 0.9166610240936279, 0.9142378568649292, 0.9232394099235535, 0.9102585911750793, 0.9106603860855103, 0.9091631174087524, 0.9235497713088989, 0.9182659983634949, 0.9138036966323853, 0.9136295914649963, 0.9198539853096008, 0.930109441280365, 0.9308394193649292, 0.9259379506111145, 0.9280240535736084, 0.9215213656425476, 0.9220405220985413, 0.9217536449432373, 0.9350696206092834, 0.9177083373069763, 0.9328137040138245, 0.9402358531951904, 0.9378840327262878, 0.9377292394638062, 0.9308925271034241, 0.9248260855674744, 0.9329032301902771, 0.9328476190567017, 0.9481688141822815, 0.9452303647994995, 0.9505652785301208, 0.9267103672027588, 0.9354770183563232, 0.9290611743927002, 0.9285539388656616, 0.9447810053825378, 0.951235830783844, 0.9533060789108276, 0.9394811987876892, 0.9429613351821899, 0.9441840648651123, 0.9506632089614868, 0.9454991221427917, 0.9454395174980164, 0.9434420466423035, 0.9451612830162048, 0.9467154741287231, 0.9536856412887573, 0.9428789019584656, 0.9492892026901245, 0.9479821920394897, 0.9434598684310913, 0.9560351967811584, 0.9473887085914612, 0.9536234736442566, 0.9474436640739441, 0.9584610462188721, 0.9662434458732605, 0.9609187245368958, 0.9551035761833191, 0.9604475498199463, 0.9585608839988708, 0.9438117146492004, 0.9594627022743225, 0.9566945433616638, 0.9641636610031128, 0.9643063545227051, 0.9434938430786133, 0.9696263670921326, 0.9681167602539062, 0.9442172050476074, 0.9611728191375732, 0.9560486078262329, 0.9585742950439453, 0.9709513783454895, 0.9721486568450928, 0.9654683470726013, 0.9657241106033325, 0.975720226764679, 0.9688565135002136, 0.9682690501213074, 0.9818080067634583, 0.9516763091087341, 0.9775338172912598, 0.9679368734359741, 0.974438488483429, 0.9801754951477051, 0.9765523672103882, 0.9815186858177185, 0.977683424949646, 0.9720771312713623, 0.9852054119110107, 0.9997428059577942, 0.9768571853637695, 0.9894542694091797, 0.9889875054359436, 0.9741727709770203, 0.9778754711151123, 0.9659256339073181, 0.9799533486366272, 0.9914312958717346, 0.9813661575317383, 0.9956324696540833, 0.992438793182373, 0.9769553542137146, 0.9846894145011902, 0.9935103058815002, 0.9748250842094421, 1.0036588907241821, 0.9846706390380859, 0.9850367307662964, 1.0011627674102783, 0.9811071753501892, 0.9914564490318298, 0.982940673828125, 0.9958736300468445, 1.0058449506759644, 0.9832122921943665, 0.9862419962882996, 0.9855135083198547, 1.0128918886184692, 1.0012032985687256, 1.0154364109039307, 0.9978219866752625, 1.001137614250183, 0.9972449541091919, 1.00750732421875, 1.01872718334198, 1.0075985193252563, 1.013891577720642, 1.03014075756073, 1.0051137208938599, 1.0065339803695679, 0.9994881749153137, 0.9906939268112183, 1.010679006576538], 'identifier': '7297090np'}