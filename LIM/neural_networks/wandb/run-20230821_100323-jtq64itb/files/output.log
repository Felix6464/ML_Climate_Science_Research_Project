
 20%|███████████████████▌                                                                              | 20/100 [00:01<00:06, 11.69it/s, loss_test=1.090]
Epoch: 00, Training Loss: 0.9921, Test Loss: 1.0814
Epoch: 01, Training Loss: 0.9887, Test Loss: 1.0662
Epoch: 02, Training Loss: 0.9918, Test Loss: 1.0867
Epoch: 03, Training Loss: 0.9870, Test Loss: 1.0710
Epoch: 04, Training Loss: 0.9884, Test Loss: 1.0686
Epoch: 05, Training Loss: 0.9851, Test Loss: 1.0814
Epoch: 06, Training Loss: 0.9924, Test Loss: 1.0884
Epoch: 07, Training Loss: 0.9914, Test Loss: 1.0855
Epoch: 08, Training Loss: 0.9859, Test Loss: 1.0742
Epoch: 09, Training Loss: 0.9883, Test Loss: 1.0831
Epoch: 10, Training Loss: 0.9884, Test Loss: 1.0858
Epoch: 11, Training Loss: 0.9875, Test Loss: 1.0898
Epoch: 12, Training Loss: 0.9824, Test Loss: 1.0725
Epoch: 13, Training Loss: 0.9873, Test Loss: 1.0831
Epoch: 14, Training Loss: 0.9900, Test Loss: 1.0648
Epoch: 15, Training Loss: 0.9869, Test Loss: 1.0877
Epoch: 16, Training Loss: 0.9872, Test Loss: 1.0891
Epoch: 17, Training Loss: 0.9817, Test Loss: 1.0795
Epoch: 18, Training Loss: 0.9861, Test Loss: 1.0888
Epoch: 19, Training Loss: 0.9868, Test Loss: 1.0657

 44%|███████████████████████████████████████████                                                       | 44/100 [00:03<00:04, 12.49it/s, loss_test=0.985]
Epoch: 21, Training Loss: 0.9834, Test Loss: 1.0667
Epoch: 22, Training Loss: 0.9871, Test Loss: 1.0721
Epoch: 23, Training Loss: 0.9812, Test Loss: 1.0776
Epoch: 24, Training Loss: 0.9803, Test Loss: 1.0836
Epoch: 25, Training Loss: 0.9814, Test Loss: 1.0946
Epoch: 26, Training Loss: 0.9790, Test Loss: 1.0908
Epoch: 27, Training Loss: 0.9761, Test Loss: 1.0720
Epoch: 28, Training Loss: 0.9765, Test Loss: 1.0541
Epoch: 29, Training Loss: 0.9696, Test Loss: 1.0576
Epoch: 30, Training Loss: 0.9609, Test Loss: 1.0627
Epoch: 31, Training Loss: 0.9511, Test Loss: 1.0548
Epoch: 32, Training Loss: 0.9466, Test Loss: 1.0346
Epoch: 33, Training Loss: 0.9392, Test Loss: 1.0474
Epoch: 34, Training Loss: 0.9343, Test Loss: 1.0216
Epoch: 35, Training Loss: 0.9263, Test Loss: 1.0122
Epoch: 36, Training Loss: 0.9223, Test Loss: 1.0301
Epoch: 37, Training Loss: 0.9191, Test Loss: 1.0153
Epoch: 38, Training Loss: 0.9123, Test Loss: 0.9942
Epoch: 39, Training Loss: 0.9075, Test Loss: 1.0055
Epoch: 40, Training Loss: 0.9044, Test Loss: 0.9902
Epoch: 41, Training Loss: 0.9003, Test Loss: 0.9935
Epoch: 42, Training Loss: 0.8935, Test Loss: 0.9859
Epoch: 43, Training Loss: 0.8951, Test Loss: 0.9848

 68%|██████████████████████████████████████████████████████████████████▋                               | 68/100 [00:05<00:02, 11.44it/s, loss_test=0.920]
Epoch: 45, Training Loss: 0.8859, Test Loss: 0.9716
Epoch: 46, Training Loss: 0.8781, Test Loss: 0.9720
Epoch: 47, Training Loss: 0.8764, Test Loss: 0.9671
Epoch: 48, Training Loss: 0.8768, Test Loss: 0.9823
Epoch: 49, Training Loss: 0.8715, Test Loss: 0.9629
Epoch: 50, Training Loss: 0.8699, Test Loss: 0.9669
Epoch: 51, Training Loss: 0.8634, Test Loss: 0.9591
Epoch: 52, Training Loss: 0.8607, Test Loss: 0.9452
Epoch: 53, Training Loss: 0.8565, Test Loss: 0.9540
Epoch: 54, Training Loss: 0.8489, Test Loss: 0.9424
Epoch: 55, Training Loss: 0.8468, Test Loss: 0.9441
Epoch: 56, Training Loss: 0.8465, Test Loss: 0.9406
Epoch: 57, Training Loss: 0.8398, Test Loss: 0.9369
Epoch: 58, Training Loss: 0.8376, Test Loss: 0.9261
Epoch: 59, Training Loss: 0.8335, Test Loss: 0.9402
Epoch: 60, Training Loss: 0.8287, Test Loss: 0.9310
Epoch: 61, Training Loss: 0.8253, Test Loss: 0.9329
Epoch: 62, Training Loss: 0.8222, Test Loss: 0.9212
Epoch: 63, Training Loss: 0.8167, Test Loss: 0.9169
Epoch: 64, Training Loss: 0.8115, Test Loss: 0.9144
Epoch: 65, Training Loss: 0.8133, Test Loss: 0.9065
Epoch: 66, Training Loss: 0.8082, Test Loss: 0.9187
Epoch: 67, Training Loss: 0.8017, Test Loss: 0.9076

 94%|████████████████████████████████████████████████████████████████████████████████████████████      | 94/100 [00:07<00:00, 12.09it/s, loss_test=0.876]
Epoch: 69, Training Loss: 0.7912, Test Loss: 0.9099
Epoch: 70, Training Loss: 0.7883, Test Loss: 0.9043
Epoch: 71, Training Loss: 0.7857, Test Loss: 0.9180
Epoch: 72, Training Loss: 0.7848, Test Loss: 0.9027
Epoch: 73, Training Loss: 0.7797, Test Loss: 0.9187
Epoch: 74, Training Loss: 0.7754, Test Loss: 0.9118
Epoch: 75, Training Loss: 0.7685, Test Loss: 0.9072
Epoch: 76, Training Loss: 0.7686, Test Loss: 0.9062
Epoch: 77, Training Loss: 0.7656, Test Loss: 0.8972
Epoch: 78, Training Loss: 0.7590, Test Loss: 0.8989
Epoch: 79, Training Loss: 0.7562, Test Loss: 0.9076
Epoch: 80, Training Loss: 0.7525, Test Loss: 0.8970
Epoch: 81, Training Loss: 0.7499, Test Loss: 0.9019
Epoch: 82, Training Loss: 0.7459, Test Loss: 0.8924
Epoch: 83, Training Loss: 0.7413, Test Loss: 0.9035
Epoch: 84, Training Loss: 0.7346, Test Loss: 0.9018
Epoch: 85, Training Loss: 0.7296, Test Loss: 0.8976
Epoch: 86, Training Loss: 0.7278, Test Loss: 0.8841
Epoch: 87, Training Loss: 0.7222, Test Loss: 0.8892
Epoch: 88, Training Loss: 0.7176, Test Loss: 0.9053
Epoch: 89, Training Loss: 0.7158, Test Loss: 0.8895
Epoch: 90, Training Loss: 0.7120, Test Loss: 0.8907
Epoch: 91, Training Loss: 0.7069, Test Loss: 0.8963
Epoch: 92, Training Loss: 0.7047, Test Loss: 0.8954

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.12it/s, loss_test=0.902]
Epoch: 94, Training Loss: 0.6938, Test Loss: 0.8858
Epoch: 95, Training Loss: 0.6902, Test Loss: 0.9008
Epoch: 96, Training Loss: 0.6856, Test Loss: 0.8956
Epoch: 97, Training Loss: 0.6842, Test Loss: 0.8985
Epoch: 98, Training Loss: 0.6817, Test Loss: 0.8990
Epoch: 99, Training Loss: 0.6782, Test Loss: 0.9020
Model saved as model_3951248np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-122000-3951248np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 100, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9921347856521606, 0.9887219190597534, 0.9918017506599426, 0.9869571328163147, 0.9883861660957336, 0.9850750923156738, 0.9923725008964539, 0.9914399862289429, 0.9859187841415405, 0.9883364558219909, 0.988396155834198, 0.9874708414077759, 0.9824248790740967, 0.9873220682144165, 0.990014660358429, 0.9869028329849243, 0.9871781468391418, 0.9817153692245484, 0.9861249446868896, 0.9867668747901917, 0.9867998480796814, 0.9833778381347656, 0.9871073842048645, 0.9811977624893189, 0.9802953958511352, 0.9814376950263977, 0.9790473937988281, 0.9761339664459229, 0.9765296578407288, 0.9696410775184632, 0.9608569383621216, 0.9511493444442749, 0.9466226935386658, 0.9391968250274658, 0.9342783093452454, 0.9262677907943726, 0.9222866773605347, 0.9190920114517211, 0.912339961528778, 0.9075205087661743, 0.9044494986534118, 0.9003116726875305, 0.8935475945472717, 0.8950789451599122, 0.8894272565841674, 0.8859200716018677, 0.8780975699424743, 0.8763864278793335, 0.876813781261444, 0.871458625793457, 0.8699105381965637, 0.8634071111679077, 0.8606770634651184, 0.8565353870391845, 0.8489207625389099, 0.8467672109603882, 0.8464565634727478, 0.839807140827179, 0.8376343607902527, 0.8334930896759033, 0.828736674785614, 0.8253212809562683, 0.8221775889396667, 0.8167281150817871, 0.8114680647850037, 0.8133111596107483, 0.8082244753837585, 0.8017285585403442, 0.8007897734642029, 0.7912440299987793, 0.7882619142532349, 0.785724675655365, 0.7847891926765442, 0.7797266721725464, 0.7753806948661804, 0.7685076594352722, 0.768576693534851, 0.7655590772628784, 0.7590439796447754, 0.756199836730957, 0.7524507999420166, 0.7498855471611023, 0.7459134697914124, 0.7413187265396118, 0.7345895528793335, 0.7296334505081177, 0.7278401255607605, 0.7221786260604859, 0.7175583124160767, 0.7157841444015502, 0.7119501948356628, 0.7069361686706543, 0.704745602607727, 0.6990679502487183, 0.6938401937484742, 0.6901595115661621, 0.685646653175354, 0.6842366576194763, 0.6816840052604676, 0.6782032370567321], 'loss_test': [1.0813919305801392, 1.0662223100662231, 1.0866546630859375, 1.0709826946258545, 1.0686230659484863, 1.0813952684402466, 1.0883843898773193, 1.0855377912521362, 1.074156403541565, 1.0830546617507935, 1.0858310461044312, 1.0898340940475464, 1.0724939107894897, 1.0830618143081665, 1.0647886991500854, 1.087683081626892, 1.0890742540359497, 1.0794668197631836, 1.088785171508789, 1.065745234489441, 1.0899524688720703, 1.0667266845703125, 1.0720998048782349, 1.0776407718658447, 1.0836372375488281, 1.0946455001831055, 1.0908360481262207, 1.071955680847168, 1.054090976715088, 1.0576400756835938, 1.06265127658844, 1.0548170804977417, 1.0346156358718872, 1.0474169254302979, 1.0216262340545654, 1.0121800899505615, 1.0300689935684204, 1.015328288078308, 0.9942180514335632, 1.0054548978805542, 0.9901759028434753, 0.9935340285301208, 0.9858906269073486, 0.9847708344459534, 0.9852525591850281, 0.9715800881385803, 0.972007691860199, 0.967059850692749, 0.9823330640792847, 0.9628823399543762, 0.9669253826141357, 0.9591037631034851, 0.9451901316642761, 0.9539905190467834, 0.9424150586128235, 0.9440774321556091, 0.9406273365020752, 0.9368966221809387, 0.9261291027069092, 0.9402111768722534, 0.930976390838623, 0.9328507781028748, 0.9211860299110413, 0.9168640375137329, 0.9143809080123901, 0.9064517021179199, 0.9186781644821167, 0.9076451659202576, 0.9199697375297546, 0.9099338054656982, 0.9042864441871643, 0.9179800748825073, 0.9027062654495239, 0.9186928272247314, 0.9118017554283142, 0.9071779251098633, 0.9062106609344482, 0.8972068428993225, 0.8988956212997437, 0.9075525999069214, 0.8969548940658569, 0.9019463658332825, 0.8923768401145935, 0.9035071730613708, 0.9017807245254517, 0.8975871205329895, 0.8841110467910767, 0.8891633749008179, 0.9053236246109009, 0.8895291686058044, 0.8907240629196167, 0.89633709192276, 0.8953676223754883, 0.8755669593811035, 0.8858341574668884, 0.9008033871650696, 0.8955687284469604, 0.8985249400138855, 0.8989530205726624, 0.901951014995575], 'identifier': '3951248np'}