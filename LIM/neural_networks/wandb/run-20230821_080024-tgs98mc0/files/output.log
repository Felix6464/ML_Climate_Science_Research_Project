
  8%|███████▍                                                                                          | 19/250 [00:01<00:19, 11.92it/s, loss_test=1.060]
Epoch: 00, Training Loss: 0.9953, Test Loss: 1.0785
Epoch: 01, Training Loss: 0.9926, Test Loss: 1.0701
Epoch: 02, Training Loss: 0.9915, Test Loss: 1.0906
Epoch: 03, Training Loss: 0.9917, Test Loss: 1.0751
Epoch: 04, Training Loss: 0.9925, Test Loss: 1.0762
Epoch: 05, Training Loss: 0.9891, Test Loss: 1.0688
Epoch: 06, Training Loss: 0.9910, Test Loss: 1.0995
Epoch: 07, Training Loss: 0.9903, Test Loss: 1.0760
Epoch: 08, Training Loss: 0.9922, Test Loss: 1.0822
Epoch: 09, Training Loss: 0.9938, Test Loss: 1.0721
Epoch: 10, Training Loss: 0.9848, Test Loss: 1.0851
Epoch: 11, Training Loss: 0.9889, Test Loss: 1.0707
Epoch: 12, Training Loss: 0.9819, Test Loss: 1.0878
Epoch: 13, Training Loss: 0.9882, Test Loss: 1.0637
Epoch: 14, Training Loss: 0.9913, Test Loss: 1.0756
Epoch: 15, Training Loss: 0.9860, Test Loss: 1.0735
Epoch: 16, Training Loss: 0.9890, Test Loss: 1.0790
Epoch: 17, Training Loss: 0.9894, Test Loss: 1.0970
Epoch: 18, Training Loss: 0.9861, Test Loss: 1.0809

 17%|████████████████▊                                                                                 | 43/250 [00:03<00:18, 11.24it/s, loss_test=0.997]
Epoch: 20, Training Loss: 0.9864, Test Loss: 1.0632
Epoch: 21, Training Loss: 0.9824, Test Loss: 1.0713
Epoch: 22, Training Loss: 0.9873, Test Loss: 1.0787
Epoch: 23, Training Loss: 0.9812, Test Loss: 1.0763
Epoch: 24, Training Loss: 0.9764, Test Loss: 1.0674
Epoch: 25, Training Loss: 0.9797, Test Loss: 1.0808
Epoch: 26, Training Loss: 0.9832, Test Loss: 1.0845
Epoch: 27, Training Loss: 0.9774, Test Loss: 1.0781
Epoch: 28, Training Loss: 0.9743, Test Loss: 1.0815
Epoch: 29, Training Loss: 0.9715, Test Loss: 1.0763
Epoch: 30, Training Loss: 0.9666, Test Loss: 1.0566
Epoch: 31, Training Loss: 0.9650, Test Loss: 1.0583
Epoch: 32, Training Loss: 0.9591, Test Loss: 1.0558
Epoch: 33, Training Loss: 0.9470, Test Loss: 1.0516
Epoch: 34, Training Loss: 0.9444, Test Loss: 1.0512
Epoch: 35, Training Loss: 0.9422, Test Loss: 1.0458
Epoch: 36, Training Loss: 0.9340, Test Loss: 1.0323
Epoch: 37, Training Loss: 0.9290, Test Loss: 1.0190
Epoch: 38, Training Loss: 0.9241, Test Loss: 1.0041
Epoch: 39, Training Loss: 0.9158, Test Loss: 1.0150
Epoch: 40, Training Loss: 0.9120, Test Loss: 1.0144
Epoch: 41, Training Loss: 0.9071, Test Loss: 1.0086
Epoch: 42, Training Loss: 0.9031, Test Loss: 1.0066

 28%|███████████████████████████                                                                       | 69/250 [00:05<00:15, 11.90it/s, loss_test=0.903]
Epoch: 44, Training Loss: 0.8922, Test Loss: 0.9960
Epoch: 45, Training Loss: 0.8876, Test Loss: 0.9826
Epoch: 46, Training Loss: 0.8839, Test Loss: 0.9843
Epoch: 47, Training Loss: 0.8823, Test Loss: 0.9726
Epoch: 48, Training Loss: 0.8757, Test Loss: 0.9548
Epoch: 49, Training Loss: 0.8732, Test Loss: 0.9730
Epoch: 50, Training Loss: 0.8676, Test Loss: 0.9510
Epoch: 51, Training Loss: 0.8661, Test Loss: 0.9609
Epoch: 52, Training Loss: 0.8610, Test Loss: 0.9396
Epoch: 53, Training Loss: 0.8554, Test Loss: 0.9567
Epoch: 54, Training Loss: 0.8507, Test Loss: 0.9618
Epoch: 55, Training Loss: 0.8484, Test Loss: 0.9483
Epoch: 56, Training Loss: 0.8447, Test Loss: 0.9506
Epoch: 57, Training Loss: 0.8382, Test Loss: 0.9466
Epoch: 58, Training Loss: 0.8342, Test Loss: 0.9181
Epoch: 59, Training Loss: 0.8318, Test Loss: 0.9296
Epoch: 60, Training Loss: 0.8270, Test Loss: 0.9287
Epoch: 61, Training Loss: 0.8237, Test Loss: 0.9241
Epoch: 62, Training Loss: 0.8220, Test Loss: 0.9204
Epoch: 63, Training Loss: 0.8182, Test Loss: 0.9311
Epoch: 64, Training Loss: 0.8138, Test Loss: 0.9255
Epoch: 65, Training Loss: 0.8113, Test Loss: 0.9103
Epoch: 66, Training Loss: 0.8048, Test Loss: 0.9127

 37%|████████████████████████████████████▍                                                             | 93/250 [00:07<00:13, 11.76it/s, loss_test=0.892]
Epoch: 68, Training Loss: 0.7999, Test Loss: 0.9026
Epoch: 69, Training Loss: 0.7948, Test Loss: 0.9028
Epoch: 70, Training Loss: 0.7858, Test Loss: 0.8892
Epoch: 71, Training Loss: 0.7862, Test Loss: 0.8989
Epoch: 72, Training Loss: 0.7839, Test Loss: 0.9161
Epoch: 73, Training Loss: 0.7786, Test Loss: 0.9016
Epoch: 74, Training Loss: 0.7751, Test Loss: 0.8877
Epoch: 75, Training Loss: 0.7686, Test Loss: 0.8965
Epoch: 76, Training Loss: 0.7645, Test Loss: 0.8880
Epoch: 77, Training Loss: 0.7615, Test Loss: 0.8838
Epoch: 78, Training Loss: 0.7566, Test Loss: 0.9002
Epoch: 79, Training Loss: 0.7526, Test Loss: 0.8953
Epoch: 80, Training Loss: 0.7479, Test Loss: 0.8845
Epoch: 81, Training Loss: 0.7419, Test Loss: 0.8924
Epoch: 82, Training Loss: 0.7437, Test Loss: 0.8797
Epoch: 83, Training Loss: 0.7355, Test Loss: 0.8794
Epoch: 84, Training Loss: 0.7318, Test Loss: 0.8840
Epoch: 85, Training Loss: 0.7242, Test Loss: 0.8802
Epoch: 86, Training Loss: 0.7241, Test Loss: 0.8774
Epoch: 87, Training Loss: 0.7196, Test Loss: 0.8883
Epoch: 88, Training Loss: 0.7173, Test Loss: 0.8843
Epoch: 89, Training Loss: 0.7110, Test Loss: 0.8817
Epoch: 90, Training Loss: 0.7084, Test Loss: 0.8855
Epoch: 91, Training Loss: 0.7020, Test Loss: 0.8716

 48%|██████████████████████████████████████████████▏                                                  | 119/250 [00:09<00:10, 12.53it/s, loss_test=0.898]
Epoch: 93, Training Loss: 0.6978, Test Loss: 0.8919
Epoch: 94, Training Loss: 0.6932, Test Loss: 0.8838
Epoch: 95, Training Loss: 0.6873, Test Loss: 0.8784
Epoch: 96, Training Loss: 0.6874, Test Loss: 0.8780
Epoch: 97, Training Loss: 0.6794, Test Loss: 0.8822
Epoch: 98, Training Loss: 0.6792, Test Loss: 0.8714
Epoch: 99, Training Loss: 0.6725, Test Loss: 0.8845
Epoch: 100, Training Loss: 0.6698, Test Loss: 0.8935
Epoch: 101, Training Loss: 0.6671, Test Loss: 0.8782
Epoch: 102, Training Loss: 0.6635, Test Loss: 0.8811
Epoch: 103, Training Loss: 0.6612, Test Loss: 0.8795
Epoch: 104, Training Loss: 0.6574, Test Loss: 0.8633
Epoch: 105, Training Loss: 0.6536, Test Loss: 0.8856
Epoch: 106, Training Loss: 0.6528, Test Loss: 0.8765
Epoch: 107, Training Loss: 0.6464, Test Loss: 0.8873
Epoch: 108, Training Loss: 0.6446, Test Loss: 0.8845
Epoch: 109, Training Loss: 0.6406, Test Loss: 0.8842
Epoch: 110, Training Loss: 0.6352, Test Loss: 0.8833
Epoch: 111, Training Loss: 0.6318, Test Loss: 0.8885
Epoch: 112, Training Loss: 0.6295, Test Loss: 0.8857
Epoch: 113, Training Loss: 0.6289, Test Loss: 0.8797
Epoch: 114, Training Loss: 0.6257, Test Loss: 0.8879
Epoch: 115, Training Loss: 0.6178, Test Loss: 0.8846
Epoch: 116, Training Loss: 0.6197, Test Loss: 0.8878

 57%|███████████████████████████████████████████████████████▍                                         | 143/250 [00:11<00:08, 11.93it/s, loss_test=0.915]
Epoch: 118, Training Loss: 0.6115, Test Loss: 0.8983
Epoch: 119, Training Loss: 0.6097, Test Loss: 0.8927
Epoch: 120, Training Loss: 0.6034, Test Loss: 0.8943
Epoch: 121, Training Loss: 0.6010, Test Loss: 0.8870
Epoch: 122, Training Loss: 0.5980, Test Loss: 0.8869
Epoch: 123, Training Loss: 0.5962, Test Loss: 0.8948
Epoch: 124, Training Loss: 0.5920, Test Loss: 0.8913
Epoch: 125, Training Loss: 0.5893, Test Loss: 0.8928
Epoch: 126, Training Loss: 0.5879, Test Loss: 0.8909
Epoch: 127, Training Loss: 0.5856, Test Loss: 0.9111
Epoch: 128, Training Loss: 0.5811, Test Loss: 0.8997
Epoch: 129, Training Loss: 0.5795, Test Loss: 0.9031
Epoch: 130, Training Loss: 0.5755, Test Loss: 0.8963
Epoch: 131, Training Loss: 0.5694, Test Loss: 0.8934
Epoch: 132, Training Loss: 0.5673, Test Loss: 0.9059
Epoch: 133, Training Loss: 0.5659, Test Loss: 0.8950
Epoch: 134, Training Loss: 0.5606, Test Loss: 0.9108
Epoch: 135, Training Loss: 0.5578, Test Loss: 0.8990
Epoch: 136, Training Loss: 0.5570, Test Loss: 0.9081
Epoch: 137, Training Loss: 0.5550, Test Loss: 0.9246
Epoch: 138, Training Loss: 0.5496, Test Loss: 0.8966
Epoch: 139, Training Loss: 0.5463, Test Loss: 0.9217
Epoch: 140, Training Loss: 0.5450, Test Loss: 0.9071
Epoch: 141, Training Loss: 0.5411, Test Loss: 0.9023
Epoch: 142, Training Loss: 0.5388, Test Loss: 0.9145
Epoch: 143, Training Loss: 0.5348, Test Loss: 0.9137
Epoch: 144, Training Loss: 0.5314, Test Loss: 0.9319
Epoch: 145, Training Loss: 0.5309, Test Loss: 0.9086
Epoch: 146, Training Loss: 0.5255, Test Loss: 0.9188
Epoch: 147, Training Loss: 0.5226, Test Loss: 0.9173
Epoch: 148, Training Loss: 0.5216, Test Loss: 0.9394
Epoch: 149, Training Loss: 0.5166, Test Loss: 0.9135
Epoch: 150, Training Loss: 0.5146, Test Loss: 0.9109
Epoch: 151, Training Loss: 0.5112, Test Loss: 0.9202
Epoch: 152, Training Loss: 0.5087, Test Loss: 0.9223
Epoch: 153, Training Loss: 0.5067, Test Loss: 0.9241
Epoch: 154, Training Loss: 0.5022, Test Loss: 0.9481
Epoch: 155, Training Loss: 0.4994, Test Loss: 0.9349
Epoch: 156, Training Loss: 0.4968, Test Loss: 0.9382
Epoch: 157, Training Loss: 0.4955, Test Loss: 0.9315
Epoch: 158, Training Loss: 0.4916, Test Loss: 0.9339
Epoch: 159, Training Loss: 0.4877, Test Loss: 0.9337
Epoch: 160, Training Loss: 0.4868, Test Loss: 0.9341
Epoch: 161, Training Loss: 0.4828, Test Loss: 0.9321
Epoch: 162, Training Loss: 0.4793, Test Loss: 0.9497
Epoch: 163, Training Loss: 0.4782, Test Loss: 0.9426
Epoch: 164, Training Loss: 0.4750, Test Loss: 0.9421
Epoch: 165, Training Loss: 0.4710, Test Loss: 0.9405

 67%|████████████████████████████████████████████████████████████████▊                                | 167/250 [00:13<00:06, 12.38it/s, loss_test=0.935]
Epoch: 167, Training Loss: 0.4673, Test Loss: 0.9393
Epoch: 168, Training Loss: 0.4630, Test Loss: 0.9354
Epoch: 169, Training Loss: 0.4611, Test Loss: 0.9375
Epoch: 170, Training Loss: 0.4606, Test Loss: 0.9292
Epoch: 171, Training Loss: 0.4561, Test Loss: 0.9356
Epoch: 172, Training Loss: 0.4556, Test Loss: 0.9439
Epoch: 173, Training Loss: 0.4512, Test Loss: 0.9425
Epoch: 174, Training Loss: 0.4492, Test Loss: 0.9423
Epoch: 175, Training Loss: 0.4455, Test Loss: 0.9500
Epoch: 176, Training Loss: 0.4426, Test Loss: 0.9421
Epoch: 177, Training Loss: 0.4416, Test Loss: 0.9364
Epoch: 178, Training Loss: 0.4385, Test Loss: 0.9373
Epoch: 179, Training Loss: 0.4380, Test Loss: 0.9460
Epoch: 180, Training Loss: 0.4352, Test Loss: 0.9416
Epoch: 181, Training Loss: 0.4311, Test Loss: 0.9394
Epoch: 182, Training Loss: 0.4291, Test Loss: 0.9378
Epoch: 183, Training Loss: 0.4268, Test Loss: 0.9533
Epoch: 184, Training Loss: 0.4250, Test Loss: 0.9486
Epoch: 185, Training Loss: 0.4216, Test Loss: 0.9491
Epoch: 186, Training Loss: 0.4187, Test Loss: 0.9427
Epoch: 187, Training Loss: 0.4189, Test Loss: 0.9534
Epoch: 188, Training Loss: 0.4154, Test Loss: 0.9399
Epoch: 189, Training Loss: 0.4137, Test Loss: 0.9426
Epoch: 190, Training Loss: 0.4117, Test Loss: 0.9450

 77%|██████████████████████████████████████████████████████████████████████████▉                      | 193/250 [00:15<00:04, 12.44it/s, loss_test=0.925]
Epoch: 192, Training Loss: 0.4052, Test Loss: 0.9546
Epoch: 193, Training Loss: 0.4036, Test Loss: 0.9251
Epoch: 194, Training Loss: 0.4025, Test Loss: 0.9464
Epoch: 195, Training Loss: 0.4006, Test Loss: 0.9488
Epoch: 196, Training Loss: 0.3974, Test Loss: 0.9483
Epoch: 197, Training Loss: 0.3956, Test Loss: 0.9652
Epoch: 198, Training Loss: 0.3938, Test Loss: 0.9456
Epoch: 199, Training Loss: 0.3919, Test Loss: 0.9553
Epoch: 200, Training Loss: 0.3908, Test Loss: 0.9528
Epoch: 201, Training Loss: 0.3879, Test Loss: 0.9473
Epoch: 202, Training Loss: 0.3849, Test Loss: 0.9525
Epoch: 203, Training Loss: 0.3821, Test Loss: 0.9415
Epoch: 204, Training Loss: 0.3804, Test Loss: 0.9570
Epoch: 205, Training Loss: 0.3775, Test Loss: 0.9630
Epoch: 206, Training Loss: 0.3766, Test Loss: 0.9590
Epoch: 207, Training Loss: 0.3748, Test Loss: 0.9490
Epoch: 208, Training Loss: 0.3724, Test Loss: 0.9581
Epoch: 209, Training Loss: 0.3709, Test Loss: 0.9529
Epoch: 210, Training Loss: 0.3703, Test Loss: 0.9597
Epoch: 211, Training Loss: 0.3681, Test Loss: 0.9609
Epoch: 212, Training Loss: 0.3653, Test Loss: 0.9602
Epoch: 213, Training Loss: 0.3644, Test Loss: 0.9750
Epoch: 214, Training Loss: 0.3608, Test Loss: 0.9608

 87%|████████████████████████████████████████████████████████████████████████████████████▏            | 217/250 [00:17<00:02, 12.71it/s, loss_test=0.953]
Epoch: 216, Training Loss: 0.3577, Test Loss: 0.9588
Epoch: 217, Training Loss: 0.3557, Test Loss: 0.9533
Epoch: 218, Training Loss: 0.3546, Test Loss: 0.9675
Epoch: 219, Training Loss: 0.3528, Test Loss: 0.9600
Epoch: 220, Training Loss: 0.3500, Test Loss: 0.9591
Epoch: 221, Training Loss: 0.3490, Test Loss: 0.9528
Epoch: 222, Training Loss: 0.3474, Test Loss: 0.9704
Epoch: 223, Training Loss: 0.3441, Test Loss: 0.9699
Epoch: 224, Training Loss: 0.3436, Test Loss: 0.9712
Epoch: 225, Training Loss: 0.3414, Test Loss: 0.9784
Epoch: 226, Training Loss: 0.3397, Test Loss: 0.9742
Epoch: 227, Training Loss: 0.3380, Test Loss: 0.9836
Epoch: 228, Training Loss: 0.3359, Test Loss: 0.9573
Epoch: 229, Training Loss: 0.3351, Test Loss: 0.9634
Epoch: 230, Training Loss: 0.3321, Test Loss: 0.9726
Epoch: 231, Training Loss: 0.3310, Test Loss: 0.9750
Epoch: 232, Training Loss: 0.3298, Test Loss: 0.9684
Epoch: 233, Training Loss: 0.3290, Test Loss: 0.9730
Epoch: 234, Training Loss: 0.3251, Test Loss: 0.9861
Epoch: 235, Training Loss: 0.3246, Test Loss: 0.9857
Epoch: 236, Training Loss: 0.3233, Test Loss: 0.9673
Epoch: 237, Training Loss: 0.3217, Test Loss: 0.9821
Epoch: 238, Training Loss: 0.3199, Test Loss: 0.9921
Epoch: 239, Training Loss: 0.3183, Test Loss: 0.9888


100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.21it/s, loss_test=0.985]
Epoch: 241, Training Loss: 0.3143, Test Loss: 0.9747
Epoch: 242, Training Loss: 0.3129, Test Loss: 0.9907
Epoch: 243, Training Loss: 0.3124, Test Loss: 0.9824
Epoch: 244, Training Loss: 0.3103, Test Loss: 0.9937
Epoch: 245, Training Loss: 0.3083, Test Loss: 1.0041
Epoch: 246, Training Loss: 0.3072, Test Loss: 0.9842
Epoch: 247, Training Loss: 0.3056, Test Loss: 0.9870
Epoch: 248, Training Loss: 0.3048, Test Loss: 0.9942
Epoch: 249, Training Loss: 0.3027, Test Loss: 0.9846
Model saved as model_1651740np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-12140000-1651740np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9952810764312744, 0.9925689697265625, 0.9915416598320007, 0.9917291760444641, 0.992457365989685, 0.9890703678131103, 0.991046941280365, 0.9903096199035645, 0.9922351241111755, 0.9937859296798706, 0.9848195075988769, 0.9888726472854614, 0.9818980097770691, 0.9882111549377441, 0.9913015484809875, 0.9859879851341248, 0.9889503836631774, 0.989392077922821, 0.9860854148864746, 0.984980046749115, 0.9863518476486206, 0.9823671340942383, 0.9873090982437134, 0.9812275886535644, 0.9764346480369568, 0.9797136425971985, 0.9831621885299683, 0.9773545861244202, 0.9742542743682862, 0.9714565753936768, 0.9665639877319336, 0.9650452494621277, 0.9590933680534363, 0.9469680070877076, 0.9444082856178284, 0.9422145366668702, 0.9339776754379272, 0.9289619088172912, 0.9240946292877197, 0.9158323645591736, 0.9120285391807557, 0.9071473479270935, 0.903083860874176, 0.8951506972312927, 0.8921935677528381, 0.8876034140586853, 0.8838802814483643, 0.8822787046432495, 0.8757132411003112, 0.873197615146637, 0.8675711512565613, 0.8661447405815125, 0.8609737873077392, 0.855433976650238, 0.850703763961792, 0.8484140038490295, 0.844703209400177, 0.8382386803627014, 0.8341654777526856, 0.8317670941352844, 0.8270101428031922, 0.8237389206886292, 0.8220476865768432, 0.8182379961013794, 0.8137938618659973, 0.8113292336463929, 0.8048406600952148, 0.800984013080597, 0.7999276876449585, 0.7948318243026733, 0.78582284450531, 0.7861685037612915, 0.7839370489120483, 0.7786244750022888, 0.7751360774040222, 0.7686411738395691, 0.7644700527191162, 0.761500334739685, 0.7566236257553101, 0.7526398301124573, 0.747900915145874, 0.7418765425682068, 0.7436834216117859, 0.7354992866516114, 0.7317649483680725, 0.7241586089134217, 0.7241397738456726, 0.7196167826652526, 0.7173461794853211, 0.7109731554985046, 0.708445954322815, 0.7019888639450074, 0.7005173206329346, 0.697809886932373, 0.693215835094452, 0.6872920870780945, 0.6874184012413025, 0.6793526411056519, 0.6792119979858399, 0.6725213289260864, 0.6698049902915955, 0.667112934589386, 0.6635350584983826, 0.6611584186553955, 0.6574469208717346, 0.6536126852035522, 0.652753722667694, 0.6463939309120178, 0.6446055054664612, 0.6406054377555848, 0.6351882457733155, 0.6318287134170533, 0.6295351386070251, 0.628889799118042, 0.6256794452667236, 0.6177595019340515, 0.6197200536727905, 0.6148074269294739, 0.6115427732467651, 0.6096805930137634, 0.6033777832984925, 0.6009570717811584, 0.5979729771614075, 0.5962455749511719, 0.591955590248108, 0.5893232941627502, 0.5879189729690552, 0.5856250047683715, 0.5811394333839417, 0.5794861197471619, 0.5755013346672058, 0.5694012761116027, 0.567341148853302, 0.5659114480018616, 0.5605939269065857, 0.5578322291374207, 0.5570483565330505, 0.5549614071846009, 0.5495901226997375, 0.5462730765342713, 0.5449820041656495, 0.5410988330841064, 0.5388452410697937, 0.5348473906517028, 0.5314325809478759, 0.5308551907539367, 0.5254798531532288, 0.5226381301879883, 0.5215879917144776, 0.5165652334690094, 0.5146125555038452, 0.5112437963485718, 0.5086749017238616, 0.5067126154899597, 0.5022228598594666, 0.49936272501945494, 0.49684028029441835, 0.49551604986190795, 0.4916122317314148, 0.48769272565841676, 0.4867847144603729, 0.48277307748794557, 0.4793480336666107, 0.4782150685787201, 0.4750293791294098, 0.4710063934326172, 0.4678652584552765, 0.46729440689086915, 0.46295519471168517, 0.4611382603645325, 0.4606141746044159, 0.45609371066093446, 0.4556253671646118, 0.4512319445610046, 0.44916794300079343, 0.4454915881156921, 0.442553186416626, 0.44159531593322754, 0.43850547075271606, 0.43795586824417115, 0.4351946473121643, 0.43114523887634276, 0.4291216850280762, 0.4268481731414795, 0.4249549627304077, 0.42164161801338196, 0.41866321563720704, 0.41892825365066527, 0.4154428422451019, 0.41371303200721743, 0.4116628110408783, 0.40964809656143186, 0.40518402457237246, 0.40356519222259524, 0.40246527791023257, 0.40058120489120486, 0.3974285781383514, 0.39561200737953184, 0.39379085302352906, 0.3918514370918274, 0.3908291161060333, 0.3879019498825073, 0.3849138796329498, 0.38208593130111695, 0.38039135932922363, 0.3775344789028168, 0.37656991481781005, 0.3748022437095642, 0.3724413633346558, 0.3709393501281738, 0.3702724039554596, 0.3681121587753296, 0.3652503788471222, 0.3643524646759033, 0.36076881885528567, 0.3588368892669678, 0.35769752264022825, 0.35569830536842345, 0.35460707545280457, 0.35278228521347044, 0.3500095307826996, 0.34895984530448915, 0.3473974049091339, 0.34405806064605715, 0.3435999870300293, 0.341438090801239, 0.3397341132164001, 0.3380050599575043, 0.33592172265052794, 0.33511879444122317, 0.3321152091026306, 0.33100929856300354, 0.3298061966896057, 0.3289883553981781, 0.32512491941452026, 0.32458257079124453, 0.32334532737731936, 0.3217089235782623, 0.3199496328830719, 0.31834904551506044, 0.3167294800281525, 0.3143238604068756, 0.31291099190711974, 0.31241871118545533, 0.31028521060943604, 0.30834206342697146, 0.30720112323760984, 0.3056382715702057, 0.30478317737579347, 0.3027182400226593], 'loss_test': [1.0785479545593262, 1.070055365562439, 1.090636134147644, 1.0751383304595947, 1.0761642456054688, 1.0687991380691528, 1.0994775295257568, 1.0760451555252075, 1.0821897983551025, 1.0720607042312622, 1.0850824117660522, 1.0706948041915894, 1.0878233909606934, 1.0637383460998535, 1.0755802392959595, 1.0734633207321167, 1.0790166854858398, 1.0970094203948975, 1.0808885097503662, 1.0601842403411865, 1.06317138671875, 1.071289300918579, 1.0786914825439453, 1.076251745223999, 1.0674428939819336, 1.0808333158493042, 1.0844550132751465, 1.0781350135803223, 1.0815293788909912, 1.0762696266174316, 1.056602954864502, 1.0582962036132812, 1.0557856559753418, 1.051571011543274, 1.051151156425476, 1.0457693338394165, 1.0322599411010742, 1.0190293788909912, 1.0041464567184448, 1.0150386095046997, 1.0144355297088623, 1.0085628032684326, 1.0065513849258423, 0.9969009757041931, 0.9959683418273926, 0.9825953245162964, 0.9842588901519775, 0.9726015329360962, 0.9548028707504272, 0.973047137260437, 0.9510098099708557, 0.9608880877494812, 0.9395684599876404, 0.9567388296127319, 0.9618455767631531, 0.9482704401016235, 0.9505568146705627, 0.9466075897216797, 0.9181252717971802, 0.9296060800552368, 0.928695023059845, 0.9240937232971191, 0.9204440712928772, 0.9311323761940002, 0.9255435466766357, 0.9102548360824585, 0.9126549959182739, 0.9126781821250916, 0.9025877714157104, 0.9028315544128418, 0.8891659379005432, 0.8988649249076843, 0.9160792231559753, 0.9016082882881165, 0.8876651525497437, 0.896531879901886, 0.888034999370575, 0.8838167786598206, 0.9002373814582825, 0.8953242301940918, 0.8844919204711914, 0.8923816680908203, 0.8797334432601929, 0.8793850541114807, 0.8840333223342896, 0.8801851868629456, 0.8773542046546936, 0.8883482217788696, 0.884304940700531, 0.8817330598831177, 0.8854751586914062, 0.8715654611587524, 0.8888400197029114, 0.891929030418396, 0.8838490843772888, 0.8783682584762573, 0.8779639601707458, 0.882183849811554, 0.8714261651039124, 0.8845346570014954, 0.8935374021530151, 0.878163754940033, 0.8811183571815491, 0.8794916272163391, 0.8632760047912598, 0.8855951428413391, 0.8764892816543579, 0.8873403072357178, 0.8844570517539978, 0.8841513991355896, 0.8832960724830627, 0.8885300159454346, 0.8857084512710571, 0.8796713352203369, 0.887913703918457, 0.8846200108528137, 0.8877699375152588, 0.8960534334182739, 0.898327648639679, 0.8927246332168579, 0.8943115472793579, 0.8870020508766174, 0.8868719339370728, 0.8948444128036499, 0.8912925124168396, 0.8927665948867798, 0.8908540606498718, 0.911146879196167, 0.8997157216072083, 0.9031218886375427, 0.89633709192276, 0.8933595418930054, 0.9059121012687683, 0.8950383067131042, 0.9108163118362427, 0.8990126252174377, 0.9081059098243713, 0.9245535135269165, 0.8966240882873535, 0.9217179417610168, 0.9071243405342102, 0.9022544026374817, 0.9145113229751587, 0.9136736989021301, 0.9318616390228271, 0.9085907340049744, 0.9188404083251953, 0.9172918200492859, 0.9394356608390808, 0.9134804606437683, 0.9109259843826294, 0.9201964139938354, 0.9223105907440186, 0.9241123199462891, 0.9480687975883484, 0.9349135756492615, 0.9382144808769226, 0.9315314888954163, 0.9338799715042114, 0.9336943030357361, 0.9341481924057007, 0.9320790767669678, 0.9497341513633728, 0.942601203918457, 0.9421405792236328, 0.9404963254928589, 0.9293195605278015, 0.939272403717041, 0.9354165196418762, 0.9375041723251343, 0.9291517734527588, 0.935582160949707, 0.9438902139663696, 0.9424802660942078, 0.9422906637191772, 0.9500058889389038, 0.9420971274375916, 0.936373770236969, 0.9373124241828918, 0.9460306763648987, 0.9416059851646423, 0.9393877983093262, 0.9378284215927124, 0.95326828956604, 0.9485763907432556, 0.9491307139396667, 0.9426827430725098, 0.9533769488334656, 0.9399440288543701, 0.942557692527771, 0.9449688792228699, 0.9497100114822388, 0.9545940160751343, 0.9251208305358887, 0.9464070796966553, 0.9487521648406982, 0.9483132362365723, 0.9652486443519592, 0.9455727934837341, 0.9552562236785889, 0.9527556896209717, 0.9473480582237244, 0.9524654746055603, 0.9415073394775391, 0.9569788575172424, 0.9630388021469116, 0.9590190052986145, 0.948981761932373, 0.9581024646759033, 0.952869713306427, 0.9597488641738892, 0.9609468579292297, 0.9602360725402832, 0.9749681353569031, 0.9608076810836792, 0.9694831371307373, 0.9588437676429749, 0.9533401727676392, 0.9675175547599792, 0.9600494503974915, 0.9591035842895508, 0.9528343677520752, 0.9703900814056396, 0.969892144203186, 0.9711903929710388, 0.9783508777618408, 0.9742119908332825, 0.9835853576660156, 0.9573338031768799, 0.9633505940437317, 0.9725708365440369, 0.974959135055542, 0.968442976474762, 0.9729914665222168, 0.986079752445221, 0.985720694065094, 0.9672611951828003, 0.9821308851242065, 0.9921063184738159, 0.9887610077857971, 0.9862691760063171, 0.9746950268745422, 0.9906796216964722, 0.982429027557373, 0.9937316179275513, 1.0041016340255737, 0.9841868281364441, 0.9870232939720154, 0.9941902160644531, 0.9845574498176575], 'identifier': '1651740np'}