
  8%|███████▊                                                                                          | 20/250 [00:01<00:19, 12.04it/s, loss_test=1.077]
Epoch: 00, Training Loss: 0.9891, Test Loss: 1.0981
Epoch: 01, Training Loss: 0.9902, Test Loss: 1.0721
Epoch: 02, Training Loss: 0.9872, Test Loss: 1.0884
Epoch: 03, Training Loss: 0.9875, Test Loss: 1.0762
Epoch: 04, Training Loss: 0.9923, Test Loss: 1.0880
Epoch: 05, Training Loss: 0.9943, Test Loss: 1.0862
Epoch: 06, Training Loss: 0.9878, Test Loss: 1.0854
Epoch: 07, Training Loss: 0.9869, Test Loss: 1.0814
Epoch: 08, Training Loss: 0.9881, Test Loss: 1.0960
Epoch: 09, Training Loss: 0.9895, Test Loss: 1.0718
Epoch: 10, Training Loss: 0.9887, Test Loss: 1.0844
Epoch: 11, Training Loss: 0.9881, Test Loss: 1.0966
Epoch: 12, Training Loss: 0.9849, Test Loss: 1.0830
Epoch: 13, Training Loss: 0.9867, Test Loss: 1.0876
Epoch: 14, Training Loss: 0.9839, Test Loss: 1.0833
Epoch: 15, Training Loss: 0.9896, Test Loss: 1.0585
Epoch: 16, Training Loss: 0.9843, Test Loss: 1.0917
Epoch: 17, Training Loss: 0.9836, Test Loss: 1.0794
Epoch: 18, Training Loss: 0.9855, Test Loss: 1.0928
Epoch: 19, Training Loss: 0.9833, Test Loss: 1.0885

 18%|█████████████████▏                                                                                | 44/250 [00:03<00:16, 12.38it/s, loss_test=1.010]
Epoch: 21, Training Loss: 0.9786, Test Loss: 1.0880
Epoch: 22, Training Loss: 0.9808, Test Loss: 1.0844
Epoch: 23, Training Loss: 0.9830, Test Loss: 1.0849
Epoch: 24, Training Loss: 0.9846, Test Loss: 1.0807
Epoch: 25, Training Loss: 0.9844, Test Loss: 1.0805
Epoch: 26, Training Loss: 0.9792, Test Loss: 1.0792
Epoch: 27, Training Loss: 0.9755, Test Loss: 1.0785
Epoch: 28, Training Loss: 0.9803, Test Loss: 1.1031
Epoch: 29, Training Loss: 0.9779, Test Loss: 1.0788
Epoch: 30, Training Loss: 0.9749, Test Loss: 1.0741
Epoch: 31, Training Loss: 0.9633, Test Loss: 1.0699
Epoch: 32, Training Loss: 0.9615, Test Loss: 1.0765
Epoch: 33, Training Loss: 0.9565, Test Loss: 1.0795
Epoch: 34, Training Loss: 0.9583, Test Loss: 1.0605
Epoch: 35, Training Loss: 0.9467, Test Loss: 1.0303
Epoch: 36, Training Loss: 0.9445, Test Loss: 1.0602
Epoch: 37, Training Loss: 0.9337, Test Loss: 1.0332
Epoch: 38, Training Loss: 0.9326, Test Loss: 1.0305
Epoch: 39, Training Loss: 0.9233, Test Loss: 1.0438
Epoch: 40, Training Loss: 0.9176, Test Loss: 1.0235
Epoch: 41, Training Loss: 0.9088, Test Loss: 1.0342
Epoch: 42, Training Loss: 0.9066, Test Loss: 1.0239
Epoch: 43, Training Loss: 0.8978, Test Loss: 1.0094

 28%|███████████████████████████▍                                                                      | 70/250 [00:05<00:14, 12.60it/s, loss_test=0.919]
Epoch: 45, Training Loss: 0.8857, Test Loss: 1.0096
Epoch: 46, Training Loss: 0.8812, Test Loss: 0.9929
Epoch: 47, Training Loss: 0.8791, Test Loss: 0.9838
Epoch: 48, Training Loss: 0.8721, Test Loss: 0.9908
Epoch: 49, Training Loss: 0.8689, Test Loss: 0.9667
Epoch: 50, Training Loss: 0.8602, Test Loss: 0.9754
Epoch: 51, Training Loss: 0.8585, Test Loss: 0.9616
Epoch: 52, Training Loss: 0.8547, Test Loss: 0.9572
Epoch: 53, Training Loss: 0.8524, Test Loss: 0.9531
Epoch: 54, Training Loss: 0.8461, Test Loss: 0.9741
Epoch: 55, Training Loss: 0.8401, Test Loss: 0.9529
Epoch: 56, Training Loss: 0.8390, Test Loss: 0.9451
Epoch: 57, Training Loss: 0.8345, Test Loss: 0.9525
Epoch: 58, Training Loss: 0.8295, Test Loss: 0.9320
Epoch: 59, Training Loss: 0.8255, Test Loss: 0.9459
Epoch: 60, Training Loss: 0.8244, Test Loss: 0.9195
Epoch: 61, Training Loss: 0.8132, Test Loss: 0.9323
Epoch: 62, Training Loss: 0.8153, Test Loss: 0.9386
Epoch: 63, Training Loss: 0.8129, Test Loss: 0.9108
Epoch: 64, Training Loss: 0.8075, Test Loss: 0.9300
Epoch: 65, Training Loss: 0.8051, Test Loss: 0.9416
Epoch: 66, Training Loss: 0.8027, Test Loss: 0.9094
Epoch: 67, Training Loss: 0.7949, Test Loss: 0.9450
Epoch: 68, Training Loss: 0.7897, Test Loss: 0.9262
Epoch: 69, Training Loss: 0.7889, Test Loss: 0.9146

 38%|█████████████████████████████████████▋                                                            | 96/250 [00:07<00:12, 12.31it/s, loss_test=0.892]
Epoch: 71, Training Loss: 0.7818, Test Loss: 0.9157
Epoch: 72, Training Loss: 0.7794, Test Loss: 0.9249
Epoch: 73, Training Loss: 0.7751, Test Loss: 0.9092
Epoch: 74, Training Loss: 0.7708, Test Loss: 0.9159
Epoch: 75, Training Loss: 0.7682, Test Loss: 0.9067
Epoch: 76, Training Loss: 0.7616, Test Loss: 0.9223
Epoch: 77, Training Loss: 0.7575, Test Loss: 0.9189
Epoch: 78, Training Loss: 0.7553, Test Loss: 0.8959
Epoch: 79, Training Loss: 0.7542, Test Loss: 0.9162
Epoch: 80, Training Loss: 0.7521, Test Loss: 0.9108
Epoch: 81, Training Loss: 0.7499, Test Loss: 0.9155
Epoch: 82, Training Loss: 0.7422, Test Loss: 0.9160
Epoch: 83, Training Loss: 0.7396, Test Loss: 0.8927
Epoch: 84, Training Loss: 0.7374, Test Loss: 0.9236
Epoch: 85, Training Loss: 0.7347, Test Loss: 0.9095
Epoch: 86, Training Loss: 0.7297, Test Loss: 0.9006
Epoch: 87, Training Loss: 0.7289, Test Loss: 0.8996
Epoch: 88, Training Loss: 0.7259, Test Loss: 0.8867
Epoch: 89, Training Loss: 0.7245, Test Loss: 0.9093
Epoch: 90, Training Loss: 0.7193, Test Loss: 0.8969
Epoch: 91, Training Loss: 0.7163, Test Loss: 0.9070
Epoch: 92, Training Loss: 0.7123, Test Loss: 0.9007
Epoch: 93, Training Loss: 0.7106, Test Loss: 0.8913
Epoch: 94, Training Loss: 0.7082, Test Loss: 0.8841

 48%|██████████████████████████████████████████████▌                                                  | 120/250 [00:09<00:10, 12.24it/s, loss_test=0.884]
Epoch: 96, Training Loss: 0.7003, Test Loss: 0.8893
Epoch: 97, Training Loss: 0.7027, Test Loss: 0.8914
Epoch: 98, Training Loss: 0.6966, Test Loss: 0.8924
Epoch: 99, Training Loss: 0.6898, Test Loss: 0.8860
Epoch: 100, Training Loss: 0.6911, Test Loss: 0.8945
Epoch: 101, Training Loss: 0.6878, Test Loss: 0.8971
Epoch: 102, Training Loss: 0.6830, Test Loss: 0.8848
Epoch: 103, Training Loss: 0.6808, Test Loss: 0.8892
Epoch: 104, Training Loss: 0.6764, Test Loss: 0.8694
Epoch: 105, Training Loss: 0.6751, Test Loss: 0.8780
Epoch: 106, Training Loss: 0.6696, Test Loss: 0.8920
Epoch: 107, Training Loss: 0.6680, Test Loss: 0.8931
Epoch: 108, Training Loss: 0.6630, Test Loss: 0.8997
Epoch: 109, Training Loss: 0.6600, Test Loss: 0.8850
Epoch: 110, Training Loss: 0.6585, Test Loss: 0.8809
Epoch: 111, Training Loss: 0.6555, Test Loss: 0.8967
Epoch: 112, Training Loss: 0.6526, Test Loss: 0.8777
Epoch: 113, Training Loss: 0.6461, Test Loss: 0.8807
Epoch: 114, Training Loss: 0.6451, Test Loss: 0.8968
Epoch: 115, Training Loss: 0.6407, Test Loss: 0.8915
Epoch: 116, Training Loss: 0.6377, Test Loss: 0.8827
Epoch: 117, Training Loss: 0.6347, Test Loss: 0.8889
Epoch: 118, Training Loss: 0.6335, Test Loss: 0.8847

 58%|███████████████████████████████████████████████████████▊                                         | 144/250 [00:11<00:08, 12.28it/s, loss_test=0.897]
Epoch: 120, Training Loss: 0.6240, Test Loss: 0.8837
Epoch: 121, Training Loss: 0.6230, Test Loss: 0.8966
Epoch: 122, Training Loss: 0.6168, Test Loss: 0.8835
Epoch: 123, Training Loss: 0.6154, Test Loss: 0.8858
Epoch: 124, Training Loss: 0.6110, Test Loss: 0.8867
Epoch: 125, Training Loss: 0.6068, Test Loss: 0.8861
Epoch: 126, Training Loss: 0.6059, Test Loss: 0.8883
Epoch: 127, Training Loss: 0.6013, Test Loss: 0.8778
Epoch: 128, Training Loss: 0.5998, Test Loss: 0.8872
Epoch: 129, Training Loss: 0.5955, Test Loss: 0.8907
Epoch: 130, Training Loss: 0.5924, Test Loss: 0.8914
Epoch: 131, Training Loss: 0.5907, Test Loss: 0.8908
Epoch: 132, Training Loss: 0.5857, Test Loss: 0.9014
Epoch: 133, Training Loss: 0.5836, Test Loss: 0.8993
Epoch: 134, Training Loss: 0.5816, Test Loss: 0.8939
Epoch: 135, Training Loss: 0.5775, Test Loss: 0.8929
Epoch: 136, Training Loss: 0.5713, Test Loss: 0.8869
Epoch: 137, Training Loss: 0.5671, Test Loss: 0.8968
Epoch: 138, Training Loss: 0.5656, Test Loss: 0.8846
Epoch: 139, Training Loss: 0.5645, Test Loss: 0.8839
Epoch: 140, Training Loss: 0.5588, Test Loss: 0.8930
Epoch: 141, Training Loss: 0.5589, Test Loss: 0.8914
Epoch: 142, Training Loss: 0.5561, Test Loss: 0.8941

 68%|█████████████████████████████████████████████████████████████████▉                               | 170/250 [00:13<00:06, 12.43it/s, loss_test=0.907]
Epoch: 144, Training Loss: 0.5481, Test Loss: 0.8971
Epoch: 145, Training Loss: 0.5456, Test Loss: 0.8937
Epoch: 146, Training Loss: 0.5419, Test Loss: 0.8921
Epoch: 147, Training Loss: 0.5413, Test Loss: 0.8938
Epoch: 148, Training Loss: 0.5358, Test Loss: 0.9023
Epoch: 149, Training Loss: 0.5316, Test Loss: 0.8954
Epoch: 150, Training Loss: 0.5305, Test Loss: 0.8932
Epoch: 151, Training Loss: 0.5270, Test Loss: 0.8889
Epoch: 152, Training Loss: 0.5259, Test Loss: 0.8945
Epoch: 153, Training Loss: 0.5221, Test Loss: 0.8962
Epoch: 154, Training Loss: 0.5183, Test Loss: 0.8890
Epoch: 155, Training Loss: 0.5161, Test Loss: 0.8872
Epoch: 156, Training Loss: 0.5138, Test Loss: 0.8945
Epoch: 157, Training Loss: 0.5102, Test Loss: 0.8903
Epoch: 158, Training Loss: 0.5057, Test Loss: 0.9078
Epoch: 159, Training Loss: 0.5042, Test Loss: 0.8948
Epoch: 160, Training Loss: 0.5016, Test Loss: 0.8943
Epoch: 161, Training Loss: 0.5003, Test Loss: 0.9124
Epoch: 162, Training Loss: 0.4965, Test Loss: 0.9127
Epoch: 163, Training Loss: 0.4946, Test Loss: 0.9136
Epoch: 164, Training Loss: 0.4927, Test Loss: 0.8994
Epoch: 165, Training Loss: 0.4887, Test Loss: 0.9149
Epoch: 166, Training Loss: 0.4859, Test Loss: 0.9087
Epoch: 167, Training Loss: 0.4827, Test Loss: 0.9029

 78%|███████████████████████████████████████████████████████████████████████████▎                     | 194/250 [00:15<00:04, 12.51it/s, loss_test=0.931]
Epoch: 169, Training Loss: 0.4797, Test Loss: 0.9074
Epoch: 170, Training Loss: 0.4760, Test Loss: 0.9050
Epoch: 171, Training Loss: 0.4730, Test Loss: 0.9072
Epoch: 172, Training Loss: 0.4703, Test Loss: 0.9123
Epoch: 173, Training Loss: 0.4666, Test Loss: 0.9111
Epoch: 174, Training Loss: 0.4657, Test Loss: 0.9141
Epoch: 175, Training Loss: 0.4651, Test Loss: 0.9022
Epoch: 176, Training Loss: 0.4611, Test Loss: 0.9137
Epoch: 177, Training Loss: 0.4579, Test Loss: 0.9123
Epoch: 178, Training Loss: 0.4561, Test Loss: 0.9255
Epoch: 179, Training Loss: 0.4549, Test Loss: 0.9244
Epoch: 180, Training Loss: 0.4512, Test Loss: 0.9142
Epoch: 181, Training Loss: 0.4481, Test Loss: 0.9205
Epoch: 182, Training Loss: 0.4462, Test Loss: 0.9173
Epoch: 183, Training Loss: 0.4446, Test Loss: 0.9215
Epoch: 184, Training Loss: 0.4402, Test Loss: 0.9160
Epoch: 185, Training Loss: 0.4376, Test Loss: 0.9247
Epoch: 186, Training Loss: 0.4352, Test Loss: 0.9281
Epoch: 187, Training Loss: 0.4345, Test Loss: 0.9359
Epoch: 188, Training Loss: 0.4320, Test Loss: 0.9263
Epoch: 189, Training Loss: 0.4302, Test Loss: 0.9305
Epoch: 190, Training Loss: 0.4270, Test Loss: 0.9391
Epoch: 191, Training Loss: 0.4239, Test Loss: 0.9216
Epoch: 192, Training Loss: 0.4234, Test Loss: 0.9226

 88%|█████████████████████████████████████████████████████████████████████████████████████▎           | 220/250 [00:17<00:02, 12.31it/s, loss_test=0.961]
Epoch: 194, Training Loss: 0.4185, Test Loss: 0.9307
Epoch: 195, Training Loss: 0.4155, Test Loss: 0.9337
Epoch: 196, Training Loss: 0.4143, Test Loss: 0.9448
Epoch: 197, Training Loss: 0.4121, Test Loss: 0.9368
Epoch: 198, Training Loss: 0.4078, Test Loss: 0.9299
Epoch: 199, Training Loss: 0.4097, Test Loss: 0.9390
Epoch: 200, Training Loss: 0.4063, Test Loss: 0.9370
Epoch: 201, Training Loss: 0.4023, Test Loss: 0.9412
Epoch: 202, Training Loss: 0.3991, Test Loss: 0.9448
Epoch: 203, Training Loss: 0.3979, Test Loss: 0.9423
Epoch: 204, Training Loss: 0.3972, Test Loss: 0.9459
Epoch: 205, Training Loss: 0.3937, Test Loss: 0.9422
Epoch: 206, Training Loss: 0.3925, Test Loss: 0.9470
Epoch: 207, Training Loss: 0.3909, Test Loss: 0.9468
Epoch: 208, Training Loss: 0.3872, Test Loss: 0.9360
Epoch: 209, Training Loss: 0.3872, Test Loss: 0.9635
Epoch: 210, Training Loss: 0.3844, Test Loss: 0.9499
Epoch: 211, Training Loss: 0.3817, Test Loss: 0.9737
Epoch: 212, Training Loss: 0.3803, Test Loss: 0.9428
Epoch: 213, Training Loss: 0.3775, Test Loss: 0.9477
Epoch: 214, Training Loss: 0.3769, Test Loss: 0.9494
Epoch: 215, Training Loss: 0.3745, Test Loss: 0.9563
Epoch: 216, Training Loss: 0.3724, Test Loss: 0.9624
Epoch: 217, Training Loss: 0.3708, Test Loss: 0.9478

 98%|██████████████████████████████████████████████████████████████████████████████████████████████▋  | 244/250 [00:19<00:00, 12.44it/s, loss_test=0.970]
Epoch: 219, Training Loss: 0.3665, Test Loss: 0.9612
Epoch: 220, Training Loss: 0.3658, Test Loss: 0.9729
Epoch: 221, Training Loss: 0.3636, Test Loss: 0.9545
Epoch: 222, Training Loss: 0.3611, Test Loss: 0.9718
Epoch: 223, Training Loss: 0.3601, Test Loss: 0.9476
Epoch: 224, Training Loss: 0.3582, Test Loss: 0.9690
Epoch: 225, Training Loss: 0.3569, Test Loss: 0.9639
Epoch: 226, Training Loss: 0.3535, Test Loss: 0.9652
Epoch: 227, Training Loss: 0.3514, Test Loss: 0.9658
Epoch: 228, Training Loss: 0.3521, Test Loss: 0.9734
Epoch: 229, Training Loss: 0.3488, Test Loss: 0.9823
Epoch: 230, Training Loss: 0.3473, Test Loss: 0.9632
Epoch: 231, Training Loss: 0.3451, Test Loss: 0.9700
Epoch: 232, Training Loss: 0.3437, Test Loss: 0.9811
Epoch: 233, Training Loss: 0.3419, Test Loss: 0.9860
Epoch: 234, Training Loss: 0.3398, Test Loss: 0.9832
Epoch: 235, Training Loss: 0.3384, Test Loss: 0.9656
Epoch: 236, Training Loss: 0.3373, Test Loss: 0.9861
Epoch: 237, Training Loss: 0.3354, Test Loss: 0.9788
Epoch: 238, Training Loss: 0.3343, Test Loss: 0.9793
Epoch: 239, Training Loss: 0.3313, Test Loss: 0.9806
Epoch: 240, Training Loss: 0.3302, Test Loss: 0.9831
Epoch: 241, Training Loss: 0.3289, Test Loss: 0.9669

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.30it/s, loss_test=0.972]
Epoch: 243, Training Loss: 0.3256, Test Loss: 0.9696
Epoch: 244, Training Loss: 0.3243, Test Loss: 0.9599
Epoch: 245, Training Loss: 0.3225, Test Loss: 0.9576
Epoch: 246, Training Loss: 0.3218, Test Loss: 0.9773
Epoch: 247, Training Loss: 0.3195, Test Loss: 0.9859
Epoch: 248, Training Loss: 0.3175, Test Loss: 0.9799
Epoch: 249, Training Loss: 0.3167, Test Loss: 0.9723
Model saved as model_280662np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-12190000-280662np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9891085028648376, 0.99016934633255, 0.9871613264083863, 0.9874693274497985, 0.9922651410102844, 0.9942737221717834, 0.9878019690513611, 0.9868836879730225, 0.9881176233291626, 0.9894869327545166, 0.9886533498764039, 0.9880980968475341, 0.9848761439323426, 0.986655330657959, 0.9838540554046631, 0.9895759701728821, 0.9843446135520935, 0.9836325287818909, 0.985462737083435, 0.9832606315612793, 0.9869067668914795, 0.9785813450813293, 0.98077392578125, 0.9830174446105957, 0.9845926880836486, 0.9844422578811646, 0.979159700870514, 0.9754753112792969, 0.9803325772285462, 0.977851128578186, 0.9748698949813843, 0.9632713079452515, 0.9614926934242248, 0.9564765453338623, 0.9582914352416992, 0.9467179775238037, 0.944498872756958, 0.9336857318878173, 0.9326427102088928, 0.9232878088951111, 0.9176384329795837, 0.9087674498558045, 0.9066007971763611, 0.8977761030197143, 0.891788899898529, 0.8856705307960511, 0.8812435626983642, 0.8791004657745362, 0.8721284985542297, 0.8688862681388855, 0.8602363109588623, 0.8584795236587525, 0.8547280550003051, 0.8524021029472351, 0.8460890531539917, 0.8401091814041137, 0.8390321850776672, 0.8344906568527222, 0.8295211911201477, 0.8254698991775513, 0.824419128894806, 0.8132197856903076, 0.8152912735939026, 0.8128894448280335, 0.8074930548667908, 0.8051432967185974, 0.8026825785636902, 0.7948760271072388, 0.7897414088249206, 0.7888666152954101, 0.7865430116653442, 0.7817592620849609, 0.7794276237487793, 0.7751487731933594, 0.7707713603973388, 0.7682317495346069, 0.7616464734077454, 0.7574904322624206, 0.7552790760993957, 0.754153823852539, 0.7520891308784485, 0.7499136924743652, 0.7421993255615235, 0.7395724892616272, 0.7374266266822815, 0.7347210288047791, 0.7296875238418579, 0.7288971900939941, 0.7258560538291932, 0.7244702339172363, 0.7193148493766784, 0.7162874817848206, 0.7123217225074768, 0.710626196861267, 0.7082265973091125, 0.7067688226699829, 0.7002994775772095, 0.702655303478241, 0.6965966820716858, 0.6898377537727356, 0.691089141368866, 0.6878079891204834, 0.6829718470573425, 0.6808070540428162, 0.676438283920288, 0.6751405119895935, 0.6695509672164917, 0.6679872274398804, 0.6630156397819519, 0.6599772453308106, 0.6584623217582702, 0.6554581880569458, 0.6526231288909912, 0.6461496949195862, 0.6450922608375549, 0.6407090067863465, 0.6377390623092651, 0.6347317695617676, 0.6335036039352417, 0.626962149143219, 0.6239918231964111, 0.6229578852653503, 0.6167859196662903, 0.61543949842453, 0.6110143780708313, 0.6068222880363464, 0.6058764457702637, 0.6012521743774414, 0.5997907042503356, 0.5955288887023926, 0.5923642635345459, 0.5906720399856568, 0.5857322454452515, 0.5835869669914245, 0.5816321015357971, 0.5774966716766358, 0.5713321924209595, 0.5670997738838196, 0.5656012892723083, 0.564466679096222, 0.558832323551178, 0.5588811755180358, 0.5561174273490905, 0.5504006028175354, 0.5481252312660218, 0.5455929160118103, 0.5418622374534607, 0.5412784218788147, 0.5357830047607421, 0.5315595149993897, 0.5305080652236939, 0.5269843935966492, 0.5258637547492981, 0.5221379041671753, 0.5182868480682373, 0.5161227583885193, 0.5137969613075256, 0.5102288484573364, 0.5057307004928588, 0.5041882514953613, 0.5016305863857269, 0.5003189027309418, 0.4965012311935425, 0.4945511817932129, 0.49267604351043703, 0.4886720359325409, 0.48592973947525026, 0.4826680362224579, 0.4808768093585968, 0.47971524596214293, 0.4759806334972382, 0.47301709055900576, 0.470320463180542, 0.4666014134883881, 0.46569671630859377, 0.4651367962360382, 0.4610909640789032, 0.4578859984874725, 0.456132185459137, 0.4548761427402496, 0.4512097895145416, 0.4480712115764618, 0.4462124228477478, 0.44463288187980654, 0.4401500642299652, 0.4375716090202332, 0.43520995378494265, 0.4344703912734985, 0.43198593258857726, 0.4302250802516937, 0.427048122882843, 0.4238723874092102, 0.42341409921646117, 0.4200408399105072, 0.41851028203964236, 0.41554139256477357, 0.4143192172050476, 0.41209774613380434, 0.40783745646476743, 0.409693169593811, 0.40634878277778624, 0.4022986590862274, 0.3991257071495056, 0.39786568880081175, 0.3972008049488068, 0.393735808134079, 0.39245004057884214, 0.3908738434314728, 0.3872184157371521, 0.3871710538864136, 0.38439937829971316, 0.38170803189277647, 0.380348265171051, 0.37749727368354796, 0.3769137978553772, 0.37448731660842893, 0.37238889932632446, 0.3707521498203278, 0.36941290497779844, 0.36653061509132384, 0.36576218008995054, 0.36360697746276854, 0.36113777160644533, 0.3600560188293457, 0.35824334621429443, 0.356868302822113, 0.3535068452358246, 0.35135514140129087, 0.35207525491714475, 0.3488361120223999, 0.3472741723060608, 0.34510484933853147, 0.3437202513217926, 0.34187724590301516, 0.33975231647491455, 0.3384183406829834, 0.3373264789581299, 0.3354351222515106, 0.33433958888053894, 0.33130837678909303, 0.3302223801612854, 0.32888423204421996, 0.3280819296836853, 0.32559707164764407, 0.32434300184249876, 0.3224764585494995, 0.32183789610862734, 0.3194960534572601, 0.3175242841243744, 0.3166632831096649], 'loss_test': [1.0980640649795532, 1.0721025466918945, 1.0883976221084595, 1.0761827230453491, 1.0879815816879272, 1.0861796140670776, 1.0854487419128418, 1.081386685371399, 1.0960197448730469, 1.0717564821243286, 1.0843675136566162, 1.0965557098388672, 1.0829870700836182, 1.0875645875930786, 1.0833094120025635, 1.0584704875946045, 1.0916523933410645, 1.079357385635376, 1.092826247215271, 1.0885056257247925, 1.0765339136123657, 1.0879515409469604, 1.084423303604126, 1.0849459171295166, 1.0807433128356934, 1.0805009603500366, 1.0792220830917358, 1.078514814376831, 1.1030707359313965, 1.0788339376449585, 1.0741186141967773, 1.0698944330215454, 1.0764782428741455, 1.0794578790664673, 1.0604528188705444, 1.0303248167037964, 1.0601987838745117, 1.0331618785858154, 1.030498743057251, 1.0437859296798706, 1.0234575271606445, 1.034201979637146, 1.0239168405532837, 1.0093721151351929, 1.010921835899353, 1.0095616579055786, 0.9928771257400513, 0.9838126301765442, 0.9908007383346558, 0.9666950106620789, 0.9754085540771484, 0.9615833163261414, 0.9572469592094421, 0.9530817866325378, 0.9740926623344421, 0.9529010653495789, 0.9450857043266296, 0.9524826407432556, 0.9320216178894043, 0.9459390044212341, 0.9195078611373901, 0.9322786331176758, 0.9385966062545776, 0.9108493328094482, 0.9299960136413574, 0.9415910840034485, 0.9093741774559021, 0.9450215697288513, 0.9261807203292847, 0.9146348834037781, 0.9191045165061951, 0.9156746864318848, 0.9248984456062317, 0.90919429063797, 0.9159036874771118, 0.9066888093948364, 0.9223429560661316, 0.9189227819442749, 0.8958826661109924, 0.9161604642868042, 0.9108026623725891, 0.915532648563385, 0.9160006642341614, 0.8927193880081177, 0.9235700964927673, 0.909490168094635, 0.900566041469574, 0.8996376991271973, 0.8866711258888245, 0.9093151092529297, 0.8968884348869324, 0.9069861173629761, 0.9006959199905396, 0.8912667632102966, 0.8840625286102295, 0.8915475010871887, 0.889345645904541, 0.8913828730583191, 0.892418622970581, 0.886023759841919, 0.8945356607437134, 0.8970606923103333, 0.8847890496253967, 0.8891966342926025, 0.869378924369812, 0.8779900074005127, 0.8920283913612366, 0.8930872678756714, 0.8997416496276855, 0.8849923014640808, 0.8809424638748169, 0.896664023399353, 0.8777072429656982, 0.8806668519973755, 0.8967877626419067, 0.8914852738380432, 0.8826941847801208, 0.8889173865318298, 0.8847123980522156, 0.8892652988433838, 0.8836894035339355, 0.8965608477592468, 0.8834940195083618, 0.8858417868614197, 0.886694610118866, 0.8860681653022766, 0.88825523853302, 0.8778069019317627, 0.8871970772743225, 0.890657901763916, 0.8914410471916199, 0.8908340930938721, 0.9013745188713074, 0.8992938995361328, 0.8938586115837097, 0.892898678779602, 0.8868727087974548, 0.8968406915664673, 0.8845608234405518, 0.8838586211204529, 0.8930373787879944, 0.891355574131012, 0.8941028118133545, 0.8849770426750183, 0.8970669507980347, 0.8937408924102783, 0.8921196460723877, 0.8938429355621338, 0.9022530913352966, 0.8954381346702576, 0.8932249546051025, 0.8888623118400574, 0.8945289254188538, 0.8961970210075378, 0.8890414834022522, 0.8872039914131165, 0.8945456743240356, 0.8903356194496155, 0.9078117609024048, 0.8948139548301697, 0.8942696452140808, 0.9124129414558411, 0.912703812122345, 0.9135525226593018, 0.8994138240814209, 0.9149224162101746, 0.9086795449256897, 0.9029188752174377, 0.9093590974807739, 0.9073767066001892, 0.9049621820449829, 0.9071628451347351, 0.9123440384864807, 0.9110731482505798, 0.9141184687614441, 0.902172863483429, 0.9136950969696045, 0.91225266456604, 0.9254586100578308, 0.924360990524292, 0.9141913056373596, 0.9204961657524109, 0.9173126220703125, 0.9214640259742737, 0.9159915447235107, 0.9247291684150696, 0.9281463623046875, 0.9359380602836609, 0.9262590408325195, 0.9304708242416382, 0.9391485452651978, 0.9215884804725647, 0.9225749969482422, 0.9157422780990601, 0.9307149648666382, 0.933678388595581, 0.9447701573371887, 0.9367926120758057, 0.9298850297927856, 0.9390217661857605, 0.9369617104530334, 0.94123375415802, 0.9448277354240417, 0.9423109292984009, 0.9459139108657837, 0.9421936273574829, 0.9469979405403137, 0.9468071460723877, 0.9359897375106812, 0.9634796977043152, 0.94986492395401, 0.9737070202827454, 0.9428126215934753, 0.9476783275604248, 0.9494221210479736, 0.9563236236572266, 0.9623677730560303, 0.9478343725204468, 0.9505506753921509, 0.9611771106719971, 0.9729402661323547, 0.9544711112976074, 0.9718402028083801, 0.9476171135902405, 0.969027578830719, 0.9638573527336121, 0.9652409553527832, 0.9658402800559998, 0.9734205603599548, 0.9822725057601929, 0.9631729125976562, 0.9699744582176208, 0.9811421036720276, 0.9859819412231445, 0.9832366108894348, 0.9656229615211487, 0.9861361980438232, 0.9787843823432922, 0.9792714715003967, 0.9805861115455627, 0.9830702543258667, 0.9669256806373596, 0.9809319376945496, 0.9695984125137329, 0.9599403738975525, 0.9575766324996948, 0.9773255586624146, 0.9859051704406738, 0.9798796772956848, 0.9722689390182495], 'identifier': '280662np'}