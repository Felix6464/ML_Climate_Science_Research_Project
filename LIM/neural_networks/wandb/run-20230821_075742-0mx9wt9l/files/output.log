
  8%|███████▊                                                                                          | 20/250 [00:01<00:18, 12.21it/s, loss_test=1.076]
Epoch: 00, Training Loss: 0.9957, Test Loss: 1.0889
Epoch: 01, Training Loss: 0.9930, Test Loss: 1.0841
Epoch: 02, Training Loss: 0.9883, Test Loss: 1.0883
Epoch: 03, Training Loss: 0.9925, Test Loss: 1.0936
Epoch: 04, Training Loss: 0.9909, Test Loss: 1.0867
Epoch: 05, Training Loss: 0.9910, Test Loss: 1.0845
Epoch: 06, Training Loss: 0.9899, Test Loss: 1.0720
Epoch: 07, Training Loss: 0.9871, Test Loss: 1.0801
Epoch: 08, Training Loss: 0.9887, Test Loss: 1.0847
Epoch: 09, Training Loss: 0.9878, Test Loss: 1.0843
Epoch: 10, Training Loss: 0.9868, Test Loss: 1.0879
Epoch: 11, Training Loss: 0.9870, Test Loss: 1.0830
Epoch: 12, Training Loss: 0.9853, Test Loss: 1.0719
Epoch: 13, Training Loss: 0.9818, Test Loss: 1.0705
Epoch: 14, Training Loss: 0.9836, Test Loss: 1.0740
Epoch: 15, Training Loss: 0.9863, Test Loss: 1.0878
Epoch: 16, Training Loss: 0.9853, Test Loss: 1.0885
Epoch: 17, Training Loss: 0.9817, Test Loss: 1.0883
Epoch: 18, Training Loss: 0.9879, Test Loss: 1.1015
Epoch: 19, Training Loss: 0.9815, Test Loss: 1.0904

 18%|█████████████████▏                                                                                | 44/250 [00:03<00:17, 11.89it/s, loss_test=0.984]
Epoch: 21, Training Loss: 0.9799, Test Loss: 1.0989
Epoch: 22, Training Loss: 0.9835, Test Loss: 1.0852
Epoch: 23, Training Loss: 0.9812, Test Loss: 1.0757
Epoch: 24, Training Loss: 0.9807, Test Loss: 1.0697
Epoch: 25, Training Loss: 0.9741, Test Loss: 1.0797
Epoch: 26, Training Loss: 0.9828, Test Loss: 1.0873
Epoch: 27, Training Loss: 0.9757, Test Loss: 1.0692
Epoch: 28, Training Loss: 0.9731, Test Loss: 1.0776
Epoch: 29, Training Loss: 0.9724, Test Loss: 1.0719
Epoch: 30, Training Loss: 0.9631, Test Loss: 1.0698
Epoch: 31, Training Loss: 0.9573, Test Loss: 1.0392
Epoch: 32, Training Loss: 0.9485, Test Loss: 1.0515
Epoch: 33, Training Loss: 0.9367, Test Loss: 1.0449
Epoch: 34, Training Loss: 0.9326, Test Loss: 1.0277
Epoch: 35, Training Loss: 0.9245, Test Loss: 1.0376
Epoch: 36, Training Loss: 0.9185, Test Loss: 1.0126
Epoch: 37, Training Loss: 0.9112, Test Loss: 1.0032
Epoch: 38, Training Loss: 0.9075, Test Loss: 1.0126
Epoch: 39, Training Loss: 0.9057, Test Loss: 0.9966
Epoch: 40, Training Loss: 0.9000, Test Loss: 1.0120
Epoch: 41, Training Loss: 0.8952, Test Loss: 0.9818
Epoch: 42, Training Loss: 0.8960, Test Loss: 1.0070
Epoch: 43, Training Loss: 0.8905, Test Loss: 0.9804

 27%|██████████████████████████▋                                                                       | 68/250 [00:05<00:14, 12.54it/s, loss_test=0.928]
Epoch: 45, Training Loss: 0.8844, Test Loss: 0.9802
Epoch: 46, Training Loss: 0.8776, Test Loss: 0.9835
Epoch: 47, Training Loss: 0.8803, Test Loss: 0.9823
Epoch: 48, Training Loss: 0.8722, Test Loss: 0.9714
Epoch: 49, Training Loss: 0.8700, Test Loss: 0.9729
Epoch: 50, Training Loss: 0.8636, Test Loss: 0.9744
Epoch: 51, Training Loss: 0.8642, Test Loss: 0.9598
Epoch: 52, Training Loss: 0.8582, Test Loss: 0.9482
Epoch: 53, Training Loss: 0.8515, Test Loss: 0.9546
Epoch: 54, Training Loss: 0.8507, Test Loss: 0.9467
Epoch: 55, Training Loss: 0.8469, Test Loss: 0.9566
Epoch: 56, Training Loss: 0.8454, Test Loss: 0.9534
Epoch: 57, Training Loss: 0.8404, Test Loss: 0.9524
Epoch: 58, Training Loss: 0.8364, Test Loss: 0.9609
Epoch: 59, Training Loss: 0.8319, Test Loss: 0.9468
Epoch: 60, Training Loss: 0.8270, Test Loss: 0.9470
Epoch: 61, Training Loss: 0.8230, Test Loss: 0.9248
Epoch: 62, Training Loss: 0.8189, Test Loss: 0.9436
Epoch: 63, Training Loss: 0.8129, Test Loss: 0.9387
Epoch: 64, Training Loss: 0.8093, Test Loss: 0.9313
Epoch: 65, Training Loss: 0.8065, Test Loss: 0.9264
Epoch: 66, Training Loss: 0.8048, Test Loss: 0.9314
Epoch: 67, Training Loss: 0.7989, Test Loss: 0.9282

 38%|████████████████████████████████████▊                                                             | 94/250 [00:07<00:12, 12.06it/s, loss_test=0.890]
Epoch: 69, Training Loss: 0.7900, Test Loss: 0.9276
Epoch: 70, Training Loss: 0.7851, Test Loss: 0.9312
Epoch: 71, Training Loss: 0.7813, Test Loss: 0.9254
Epoch: 72, Training Loss: 0.7805, Test Loss: 0.9087
Epoch: 73, Training Loss: 0.7766, Test Loss: 0.9123
Epoch: 74, Training Loss: 0.7707, Test Loss: 0.9278
Epoch: 75, Training Loss: 0.7655, Test Loss: 0.9143
Epoch: 76, Training Loss: 0.7609, Test Loss: 0.9126
Epoch: 77, Training Loss: 0.7576, Test Loss: 0.9195
Epoch: 78, Training Loss: 0.7552, Test Loss: 0.9104
Epoch: 79, Training Loss: 0.7495, Test Loss: 0.9071
Epoch: 80, Training Loss: 0.7478, Test Loss: 0.9046
Epoch: 81, Training Loss: 0.7455, Test Loss: 0.9031
Epoch: 82, Training Loss: 0.7418, Test Loss: 0.8948
Epoch: 83, Training Loss: 0.7373, Test Loss: 0.9102
Epoch: 84, Training Loss: 0.7335, Test Loss: 0.9178
Epoch: 85, Training Loss: 0.7296, Test Loss: 0.9225
Epoch: 86, Training Loss: 0.7247, Test Loss: 0.9138
Epoch: 87, Training Loss: 0.7235, Test Loss: 0.9165
Epoch: 88, Training Loss: 0.7230, Test Loss: 0.9215
Epoch: 89, Training Loss: 0.7147, Test Loss: 0.9083
Epoch: 90, Training Loss: 0.7131, Test Loss: 0.8936
Epoch: 91, Training Loss: 0.7067, Test Loss: 0.9095

 47%|█████████████████████████████████████████████▊                                                   | 118/250 [00:09<00:11, 11.63it/s, loss_test=0.913]
Epoch: 93, Training Loss: 0.7017, Test Loss: 0.8904
Epoch: 94, Training Loss: 0.7038, Test Loss: 0.9074
Epoch: 95, Training Loss: 0.6965, Test Loss: 0.8950
Epoch: 96, Training Loss: 0.6940, Test Loss: 0.8945
Epoch: 97, Training Loss: 0.6908, Test Loss: 0.9026
Epoch: 98, Training Loss: 0.6865, Test Loss: 0.9034
Epoch: 99, Training Loss: 0.6818, Test Loss: 0.9022
Epoch: 100, Training Loss: 0.6820, Test Loss: 0.9128
Epoch: 101, Training Loss: 0.6778, Test Loss: 0.9063
Epoch: 102, Training Loss: 0.6748, Test Loss: 0.9033
Epoch: 103, Training Loss: 0.6726, Test Loss: 0.9022
Epoch: 104, Training Loss: 0.6700, Test Loss: 0.8947
Epoch: 105, Training Loss: 0.6638, Test Loss: 0.9062
Epoch: 106, Training Loss: 0.6603, Test Loss: 0.9056
Epoch: 107, Training Loss: 0.6590, Test Loss: 0.9023
Epoch: 108, Training Loss: 0.6572, Test Loss: 0.9034
Epoch: 109, Training Loss: 0.6537, Test Loss: 0.9130
Epoch: 110, Training Loss: 0.6487, Test Loss: 0.8957
Epoch: 111, Training Loss: 0.6482, Test Loss: 0.9023
Epoch: 112, Training Loss: 0.6434, Test Loss: 0.8999
Epoch: 113, Training Loss: 0.6393, Test Loss: 0.9141
Epoch: 114, Training Loss: 0.6387, Test Loss: 0.9146
Epoch: 115, Training Loss: 0.6357, Test Loss: 0.9083

 57%|███████████████████████████████████████████████████████                                          | 142/250 [00:11<00:08, 12.13it/s, loss_test=0.923]
Epoch: 117, Training Loss: 0.6245, Test Loss: 0.9130
Epoch: 118, Training Loss: 0.6236, Test Loss: 0.8926
Epoch: 119, Training Loss: 0.6243, Test Loss: 0.8969
Epoch: 120, Training Loss: 0.6189, Test Loss: 0.9184
Epoch: 121, Training Loss: 0.6157, Test Loss: 0.8951
Epoch: 122, Training Loss: 0.6109, Test Loss: 0.8998
Epoch: 123, Training Loss: 0.6080, Test Loss: 0.9005
Epoch: 124, Training Loss: 0.6029, Test Loss: 0.9053
Epoch: 125, Training Loss: 0.6042, Test Loss: 0.9083
Epoch: 126, Training Loss: 0.6008, Test Loss: 0.9140
Epoch: 127, Training Loss: 0.5949, Test Loss: 0.9012
Epoch: 128, Training Loss: 0.5914, Test Loss: 0.8999
Epoch: 129, Training Loss: 0.5909, Test Loss: 0.9035
Epoch: 130, Training Loss: 0.5860, Test Loss: 0.8978
Epoch: 131, Training Loss: 0.5848, Test Loss: 0.9091
Epoch: 132, Training Loss: 0.5811, Test Loss: 0.9058
Epoch: 133, Training Loss: 0.5782, Test Loss: 0.9057
Epoch: 134, Training Loss: 0.5757, Test Loss: 0.9106
Epoch: 135, Training Loss: 0.5698, Test Loss: 0.9044
Epoch: 136, Training Loss: 0.5658, Test Loss: 0.9138
Epoch: 137, Training Loss: 0.5646, Test Loss: 0.9163
Epoch: 138, Training Loss: 0.5626, Test Loss: 0.9065
Epoch: 139, Training Loss: 0.5616, Test Loss: 0.9009
Epoch: 140, Training Loss: 0.5556, Test Loss: 0.9072

 66%|████████████████████████████████████████████████████████████████▍                                | 166/250 [00:13<00:06, 12.00it/s, loss_test=0.947]
Epoch: 142, Training Loss: 0.5498, Test Loss: 0.9233
Epoch: 143, Training Loss: 0.5455, Test Loss: 0.9104
Epoch: 144, Training Loss: 0.5438, Test Loss: 0.9252
Epoch: 145, Training Loss: 0.5424, Test Loss: 0.9133
Epoch: 146, Training Loss: 0.5363, Test Loss: 0.9217
Epoch: 147, Training Loss: 0.5344, Test Loss: 0.9136
Epoch: 148, Training Loss: 0.5318, Test Loss: 0.9274
Epoch: 149, Training Loss: 0.5304, Test Loss: 0.9137
Epoch: 150, Training Loss: 0.5263, Test Loss: 0.9153
Epoch: 151, Training Loss: 0.5224, Test Loss: 0.9352
Epoch: 152, Training Loss: 0.5172, Test Loss: 0.9313
Epoch: 153, Training Loss: 0.5169, Test Loss: 0.9330
Epoch: 154, Training Loss: 0.5120, Test Loss: 0.9138
Epoch: 155, Training Loss: 0.5095, Test Loss: 0.9406
Epoch: 156, Training Loss: 0.5059, Test Loss: 0.9304
Epoch: 157, Training Loss: 0.5056, Test Loss: 0.9265
Epoch: 158, Training Loss: 0.5034, Test Loss: 0.9358
Epoch: 159, Training Loss: 0.4991, Test Loss: 0.9393
Epoch: 160, Training Loss: 0.4966, Test Loss: 0.9420
Epoch: 161, Training Loss: 0.4918, Test Loss: 0.9171
Epoch: 162, Training Loss: 0.4904, Test Loss: 0.9245
Epoch: 163, Training Loss: 0.4873, Test Loss: 0.9291
Epoch: 164, Training Loss: 0.4863, Test Loss: 0.9397

 77%|██████████████████████████████████████████████████████████████████████████▍                      | 192/250 [00:15<00:04, 12.37it/s, loss_test=0.963]
Epoch: 166, Training Loss: 0.4800, Test Loss: 0.9466
Epoch: 167, Training Loss: 0.4768, Test Loss: 0.9355
Epoch: 168, Training Loss: 0.4754, Test Loss: 0.9177
Epoch: 169, Training Loss: 0.4705, Test Loss: 0.9247
Epoch: 170, Training Loss: 0.4689, Test Loss: 0.9368
Epoch: 171, Training Loss: 0.4658, Test Loss: 0.9437
Epoch: 172, Training Loss: 0.4621, Test Loss: 0.9430
Epoch: 173, Training Loss: 0.4627, Test Loss: 0.9409
Epoch: 174, Training Loss: 0.4595, Test Loss: 0.9458
Epoch: 175, Training Loss: 0.4563, Test Loss: 0.9522
Epoch: 176, Training Loss: 0.4538, Test Loss: 0.9637
Epoch: 177, Training Loss: 0.4499, Test Loss: 0.9430
Epoch: 178, Training Loss: 0.4503, Test Loss: 0.9351
Epoch: 179, Training Loss: 0.4468, Test Loss: 0.9458
Epoch: 180, Training Loss: 0.4435, Test Loss: 0.9524
Epoch: 181, Training Loss: 0.4430, Test Loss: 0.9455
Epoch: 182, Training Loss: 0.4374, Test Loss: 0.9483
Epoch: 183, Training Loss: 0.4351, Test Loss: 0.9462
Epoch: 184, Training Loss: 0.4353, Test Loss: 0.9561
Epoch: 185, Training Loss: 0.4335, Test Loss: 0.9720
Epoch: 186, Training Loss: 0.4313, Test Loss: 0.9695
Epoch: 187, Training Loss: 0.4286, Test Loss: 0.9628
Epoch: 188, Training Loss: 0.4256, Test Loss: 0.9535
Epoch: 189, Training Loss: 0.4222, Test Loss: 0.9445

 86%|███████████████████████████████████████████████████████████████████████████████████▊             | 216/250 [00:17<00:02, 11.64it/s, loss_test=0.990]
Epoch: 191, Training Loss: 0.4212, Test Loss: 0.9626
Epoch: 192, Training Loss: 0.4179, Test Loss: 0.9612
Epoch: 193, Training Loss: 0.4151, Test Loss: 0.9783
Epoch: 194, Training Loss: 0.4125, Test Loss: 0.9662
Epoch: 195, Training Loss: 0.4115, Test Loss: 0.9456
Epoch: 196, Training Loss: 0.4089, Test Loss: 0.9562
Epoch: 197, Training Loss: 0.4085, Test Loss: 0.9585
Epoch: 198, Training Loss: 0.4053, Test Loss: 0.9747
Epoch: 199, Training Loss: 0.4027, Test Loss: 0.9643
Epoch: 200, Training Loss: 0.3999, Test Loss: 0.9593
Epoch: 201, Training Loss: 0.3988, Test Loss: 0.9757
Epoch: 202, Training Loss: 0.3959, Test Loss: 0.9643
Epoch: 203, Training Loss: 0.3946, Test Loss: 0.9619
Epoch: 204, Training Loss: 0.3924, Test Loss: 0.9718
Epoch: 205, Training Loss: 0.3910, Test Loss: 0.9834
Epoch: 206, Training Loss: 0.3904, Test Loss: 0.9738
Epoch: 207, Training Loss: 0.3862, Test Loss: 0.9841
Epoch: 208, Training Loss: 0.3844, Test Loss: 0.9710
Epoch: 209, Training Loss: 0.3824, Test Loss: 0.9723
Epoch: 210, Training Loss: 0.3801, Test Loss: 0.9878
Epoch: 211, Training Loss: 0.3803, Test Loss: 0.9789
Epoch: 212, Training Loss: 0.3779, Test Loss: 0.9897
Epoch: 213, Training Loss: 0.3763, Test Loss: 0.9788

 96%|█████████████████████████████████████████████████████████████████████████████████████████████    | 240/250 [00:19<00:00, 11.99it/s, loss_test=0.998]
Epoch: 215, Training Loss: 0.3722, Test Loss: 0.9901
Epoch: 216, Training Loss: 0.3694, Test Loss: 0.9867
Epoch: 217, Training Loss: 0.3683, Test Loss: 0.9949
Epoch: 218, Training Loss: 0.3651, Test Loss: 0.9973
Epoch: 219, Training Loss: 0.3640, Test Loss: 0.9813
Epoch: 220, Training Loss: 0.3634, Test Loss: 0.9826
Epoch: 221, Training Loss: 0.3607, Test Loss: 0.9796
Epoch: 222, Training Loss: 0.3580, Test Loss: 0.9935
Epoch: 223, Training Loss: 0.3576, Test Loss: 0.9837
Epoch: 224, Training Loss: 0.3557, Test Loss: 0.9911
Epoch: 225, Training Loss: 0.3550, Test Loss: 0.9702
Epoch: 226, Training Loss: 0.3526, Test Loss: 0.9847
Epoch: 227, Training Loss: 0.3500, Test Loss: 0.9888
Epoch: 228, Training Loss: 0.3483, Test Loss: 0.9988
Epoch: 229, Training Loss: 0.3474, Test Loss: 1.0035
Epoch: 230, Training Loss: 0.3451, Test Loss: 1.0001
Epoch: 231, Training Loss: 0.3435, Test Loss: 0.9851
Epoch: 232, Training Loss: 0.3412, Test Loss: 0.9796
Epoch: 233, Training Loss: 0.3402, Test Loss: 0.9980
Epoch: 234, Training Loss: 0.3392, Test Loss: 0.9754
Epoch: 235, Training Loss: 0.3370, Test Loss: 1.0152
Epoch: 236, Training Loss: 0.3350, Test Loss: 0.9907
Epoch: 237, Training Loss: 0.3338, Test Loss: 1.0023

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.08it/s, loss_test=1.005]
Epoch: 239, Training Loss: 0.3301, Test Loss: 0.9983
Epoch: 240, Training Loss: 0.3294, Test Loss: 0.9966
Epoch: 241, Training Loss: 0.3270, Test Loss: 1.0042
Epoch: 242, Training Loss: 0.3252, Test Loss: 1.0109
Epoch: 243, Training Loss: 0.3231, Test Loss: 1.0036
Epoch: 244, Training Loss: 0.3231, Test Loss: 1.0211
Epoch: 245, Training Loss: 0.3216, Test Loss: 1.0086
Epoch: 246, Training Loss: 0.3191, Test Loss: 1.0211
Epoch: 247, Training Loss: 0.3189, Test Loss: 1.0047
Epoch: 248, Training Loss: 0.3172, Test Loss: 1.0091
Epoch: 249, Training Loss: 0.3159, Test Loss: 1.0050
Model saved as model_764270np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-1290000-764270np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9956853270530701, 0.9929722785949707, 0.9882529258728028, 0.9924789190292358, 0.9909050941467286, 0.9910235524177551, 0.9898972392082215, 0.9870620965957642, 0.988675844669342, 0.9877668261528015, 0.9868363022804261, 0.9869627237319947, 0.9852638840675354, 0.9817522883415222, 0.983601176738739, 0.986335813999176, 0.9853010892868042, 0.9816615223884583, 0.9878594517707825, 0.9814724326133728, 0.9867676258087158, 0.9799231886863708, 0.9835224390029907, 0.9811506032943725, 0.980735433101654, 0.9740663647651673, 0.982818853855133, 0.9757125854492188, 0.9730648398399353, 0.972380256652832, 0.9630694508552551, 0.9573101282119751, 0.948476767539978, 0.9366753339767456, 0.932580542564392, 0.9244690299034118, 0.9184903979301453, 0.9112371563911438, 0.9074825525283814, 0.9056527256965637, 0.8999520421028138, 0.8951793074607849, 0.896032178401947, 0.8904789924621582, 0.8881536722183228, 0.8843852281570435, 0.8776283025741577, 0.880289614200592, 0.8721904039382935, 0.8699979543685913, 0.8636421680450439, 0.864208436012268, 0.8582207560539246, 0.8515035152435303, 0.8506753087043762, 0.8468518733978272, 0.8453580617904664, 0.8404003262519837, 0.8363828063011169, 0.8319387435913086, 0.8269665002822876, 0.8229568362236023, 0.8188697457313537, 0.8129291534423828, 0.8093293070793152, 0.806529986858368, 0.8047553300857544, 0.7988506197929383, 0.7938678741455079, 0.7899615049362183, 0.7851165890693664, 0.7813334345817566, 0.7804781317710876, 0.7766462326049804, 0.7707171201705932, 0.7654804587364197, 0.7609236836433411, 0.7575832486152649, 0.7551748514175415, 0.7494640469551086, 0.7477959513664245, 0.7454926490783691, 0.7417868137359619, 0.7372968435287476, 0.7335065245628357, 0.7296039462089539, 0.7247290134429931, 0.723513376712799, 0.7230271339416504, 0.7147339940071106, 0.713072681427002, 0.7066842913627625, 0.7068075060844421, 0.7017488479614258, 0.7038477063179016, 0.696518087387085, 0.6939676523208618, 0.6908006429672241, 0.6865365862846374, 0.6818146586418152, 0.6819559574127197, 0.6778101563453675, 0.6747912287712097, 0.6725811243057251, 0.6700262427330017, 0.6638306260108948, 0.6602711796760559, 0.6590357780456543, 0.6571815490722657, 0.6537343621253967, 0.6487229108810425, 0.6481710910797119, 0.6433516263961792, 0.6393399596214294, 0.6386927843093873, 0.6357335209846496, 0.6309699773788452, 0.6245019793510437, 0.6235603332519531, 0.6243096947669983, 0.6188559889793396, 0.6157436847686768, 0.6109476685523987, 0.6079987764358521, 0.6029402494430542, 0.6041760206222534, 0.6007872104644776, 0.5949164986610412, 0.5913750410079956, 0.5908942699432373, 0.5860281348228454, 0.5848280429840088, 0.5810670375823974, 0.5781812071800232, 0.5756731510162354, 0.5698397278785705, 0.5658109307289123, 0.5645988583564758, 0.5625655651092529, 0.5615769743919372, 0.5556360244750976, 0.5528608441352845, 0.5498241424560547, 0.5455212950706482, 0.5438284158706665, 0.5423810243606567, 0.5363383769989014, 0.5343654870986938, 0.5317634582519531, 0.5304453611373902, 0.526288378238678, 0.5224257946014405, 0.5172455549240113, 0.5168600022792816, 0.5120045602321625, 0.5095036566257477, 0.5059093773365021, 0.5055687844753265, 0.5034252822399139, 0.4991232931613922, 0.4965591311454773, 0.4918327689170837, 0.4903985023498535, 0.48725247383117676, 0.4863233745098114, 0.4821936249732971, 0.4800256431102753, 0.47683300971984866, 0.47544609308242797, 0.47049859166145325, 0.46888633370399474, 0.46579872965812685, 0.46213723421096803, 0.462662136554718, 0.4594634175300598, 0.45633002519607546, 0.45382740497589114, 0.44990491271018984, 0.4503194332122803, 0.44680154919624326, 0.4435094952583313, 0.44298654794692993, 0.43743746280670165, 0.435084742307663, 0.4352638006210327, 0.4334770321846008, 0.43129608035087585, 0.4285532355308533, 0.4256016969680786, 0.42224995493888856, 0.42271363735198975, 0.42119738459587097, 0.41792844533920287, 0.41506298780441286, 0.4125388920307159, 0.41151813268661497, 0.4089281678199768, 0.40847334265708923, 0.4052861452102661, 0.40266271233558654, 0.39990785121917727, 0.3987776398658752, 0.3958896040916443, 0.3946274518966675, 0.39236043095588685, 0.3909511625766754, 0.3903623402118683, 0.3861771821975708, 0.38442572951316833, 0.38241746425628664, 0.3801133632659912, 0.3803434014320374, 0.37785097360610964, 0.3763409495353699, 0.3735100209712982, 0.3721551775932312, 0.36942331194877626, 0.36828250885009767, 0.3650511860847473, 0.36401107907295227, 0.3633541941642761, 0.36073346734046935, 0.35800384879112246, 0.35755449533462524, 0.3557363092899323, 0.355044287443161, 0.3526469230651855, 0.35000844597816466, 0.34830051064491274, 0.3474307954311371, 0.34506223201751707, 0.3435248792171478, 0.3412167072296143, 0.3402244448661804, 0.339179664850235, 0.33697750568389895, 0.33504865169525144, 0.3338169097900391, 0.3305421769618988, 0.33008530735969543, 0.3293868720531464, 0.3269624590873718, 0.32518022060394286, 0.32311334609985354, 0.32308331727981565, 0.3216348528862, 0.3191151976585388, 0.31891446709632876, 0.3172123610973358, 0.3158658266067505], 'loss_test': [1.0888652801513672, 1.084067940711975, 1.0882880687713623, 1.0935817956924438, 1.0867060422897339, 1.0845015048980713, 1.0720411539077759, 1.0800832509994507, 1.0846636295318604, 1.0842905044555664, 1.087856411933899, 1.0829683542251587, 1.071887493133545, 1.0705208778381348, 1.074026346206665, 1.0878472328186035, 1.0884697437286377, 1.0883375406265259, 1.1014716625213623, 1.090388298034668, 1.0759103298187256, 1.0988582372665405, 1.0851985216140747, 1.075705647468567, 1.0696803331375122, 1.0797494649887085, 1.0872704982757568, 1.0691674947738647, 1.0776475667953491, 1.0718952417373657, 1.0698201656341553, 1.0391720533370972, 1.0515183210372925, 1.0448554754257202, 1.0277425050735474, 1.0375875234603882, 1.0126397609710693, 1.003204345703125, 1.012587547302246, 0.9966112971305847, 1.0119504928588867, 0.9817743897438049, 1.006978988647461, 0.9804004430770874, 0.9842466115951538, 0.9802170991897583, 0.983535647392273, 0.9822959303855896, 0.9714183807373047, 0.9729226231575012, 0.9743838906288147, 0.9597592353820801, 0.9481605887413025, 0.9545596837997437, 0.9466536045074463, 0.9565587639808655, 0.9533768892288208, 0.9523530006408691, 0.9608966708183289, 0.9467927813529968, 0.9470131993293762, 0.924777090549469, 0.9435994625091553, 0.9387348890304565, 0.9312574863433838, 0.9264028072357178, 0.931374728679657, 0.928214967250824, 0.9191915392875671, 0.9275895953178406, 0.931196391582489, 0.9254190325737, 0.9086741209030151, 0.9122542142868042, 0.9277622103691101, 0.9143094420433044, 0.9125849008560181, 0.9194504022598267, 0.9103730916976929, 0.9071411490440369, 0.904634952545166, 0.9030506014823914, 0.8947820067405701, 0.9101854562759399, 0.9178488254547119, 0.9224965572357178, 0.9138006567955017, 0.9164566397666931, 0.9215412139892578, 0.9082735180854797, 0.8935566544532776, 0.9094960689544678, 0.904005229473114, 0.8903703093528748, 0.9074196219444275, 0.895047128200531, 0.8945127725601196, 0.9025765657424927, 0.903374433517456, 0.9021920561790466, 0.9128209948539734, 0.906325101852417, 0.9032780528068542, 0.9021795988082886, 0.8946704864501953, 0.9061981439590454, 0.9055819511413574, 0.9023061990737915, 0.9034194350242615, 0.912998616695404, 0.8956872224807739, 0.9023001194000244, 0.8998860716819763, 0.9140658378601074, 0.9145519733428955, 0.9083428382873535, 0.9146085381507874, 0.9129539132118225, 0.89259272813797, 0.8969032764434814, 0.9183974862098694, 0.89511638879776, 0.8997647166252136, 0.9005160927772522, 0.9053456783294678, 0.9082568883895874, 0.9139512777328491, 0.9011774659156799, 0.8998667597770691, 0.9034603834152222, 0.8978283405303955, 0.9090567827224731, 0.905799388885498, 0.9056647419929504, 0.910614013671875, 0.9044145345687866, 0.9138357639312744, 0.9162922501564026, 0.906497061252594, 0.900924563407898, 0.9071701169013977, 0.8927890658378601, 0.9232534170150757, 0.9104109406471252, 0.9252206087112427, 0.9132900834083557, 0.9216737747192383, 0.9136066436767578, 0.927419900894165, 0.9136704802513123, 0.9152889251708984, 0.935158908367157, 0.9312777519226074, 0.9330435991287231, 0.9137524962425232, 0.9406097531318665, 0.9303911924362183, 0.9265314340591431, 0.9357742667198181, 0.9392609000205994, 0.9420286417007446, 0.9170900583267212, 0.9244691729545593, 0.9291266798973083, 0.9396551847457886, 0.9373101592063904, 0.9466425180435181, 0.9355468153953552, 0.9177341461181641, 0.9247099161148071, 0.9368388056755066, 0.943716287612915, 0.9429824948310852, 0.9409195184707642, 0.9458026885986328, 0.952228844165802, 0.963712751865387, 0.942983865737915, 0.9350622892379761, 0.9457747936248779, 0.9523910880088806, 0.9455443024635315, 0.9482539892196655, 0.946174144744873, 0.9561213254928589, 0.971997082233429, 0.9695258736610413, 0.9627765417098999, 0.9535022377967834, 0.9445244669914246, 0.9560803174972534, 0.9626202583312988, 0.961195707321167, 0.9782789945602417, 0.9661951065063477, 0.9455680251121521, 0.9561806917190552, 0.9585118293762207, 0.9746512770652771, 0.9643007516860962, 0.9593196511268616, 0.9756647944450378, 0.9642605781555176, 0.9618929028511047, 0.9717850089073181, 0.9833698272705078, 0.973842978477478, 0.9841219186782837, 0.9710068106651306, 0.9723315834999084, 0.9878455400466919, 0.9789260029792786, 0.9897403717041016, 0.9788077473640442, 1.003976583480835, 0.990077555179596, 0.9866580963134766, 0.9949030876159668, 0.9973145723342896, 0.981260359287262, 0.9826001524925232, 0.9795987010002136, 0.993536651134491, 0.9837455153465271, 0.9910812377929688, 0.9701611995697021, 0.9847022294998169, 0.9887544512748718, 0.9987903237342834, 1.0035310983657837, 1.0000683069229126, 0.9850943684577942, 0.9795516133308411, 0.9980407357215881, 0.9753877520561218, 1.0152137279510498, 0.9907266497612, 1.0022730827331543, 0.997085452079773, 0.9983132481575012, 0.9966229200363159, 1.0041508674621582, 1.0108915567398071, 1.0035825967788696, 1.0210559368133545, 1.0085701942443848, 1.0210673809051514, 1.004692554473877, 1.009119987487793, 1.0049840211868286], 'identifier': '764270np'}