
  8%|███████▊                                                                                          | 20/250 [00:01<00:19, 11.75it/s, loss_test=1.084]
Epoch: 00, Training Loss: 0.9940, Test Loss: 1.1110
Epoch: 01, Training Loss: 0.9924, Test Loss: 1.0993
Epoch: 02, Training Loss: 0.9945, Test Loss: 1.0816
Epoch: 03, Training Loss: 0.9902, Test Loss: 1.0880
Epoch: 04, Training Loss: 0.9926, Test Loss: 1.0889
Epoch: 05, Training Loss: 0.9940, Test Loss: 1.0854
Epoch: 06, Training Loss: 0.9948, Test Loss: 1.0953
Epoch: 07, Training Loss: 0.9938, Test Loss: 1.1016
Epoch: 08, Training Loss: 0.9915, Test Loss: 1.0917
Epoch: 09, Training Loss: 0.9874, Test Loss: 1.0825
Epoch: 10, Training Loss: 0.9877, Test Loss: 1.0875
Epoch: 11, Training Loss: 0.9850, Test Loss: 1.0986
Epoch: 12, Training Loss: 0.9914, Test Loss: 1.0802
Epoch: 13, Training Loss: 0.9889, Test Loss: 1.0866
Epoch: 14, Training Loss: 0.9906, Test Loss: 1.0798
Epoch: 15, Training Loss: 0.9881, Test Loss: 1.0911
Epoch: 16, Training Loss: 0.9863, Test Loss: 1.0732
Epoch: 17, Training Loss: 0.9874, Test Loss: 1.0744
Epoch: 18, Training Loss: 0.9869, Test Loss: 1.0896

 18%|█████████████████▏                                                                                | 44/250 [00:03<00:16, 12.13it/s, loss_test=1.035]
Epoch: 20, Training Loss: 0.9825, Test Loss: 1.0885
Epoch: 21, Training Loss: 0.9868, Test Loss: 1.0799
Epoch: 22, Training Loss: 0.9842, Test Loss: 1.0758
Epoch: 23, Training Loss: 0.9849, Test Loss: 1.0699
Epoch: 24, Training Loss: 0.9840, Test Loss: 1.0801
Epoch: 25, Training Loss: 0.9784, Test Loss: 1.0764
Epoch: 26, Training Loss: 0.9834, Test Loss: 1.0810
Epoch: 27, Training Loss: 0.9812, Test Loss: 1.0713
Epoch: 28, Training Loss: 0.9835, Test Loss: 1.1006
Epoch: 29, Training Loss: 0.9762, Test Loss: 1.0879
Epoch: 30, Training Loss: 0.9808, Test Loss: 1.0786
Epoch: 31, Training Loss: 0.9786, Test Loss: 1.0729
Epoch: 32, Training Loss: 0.9734, Test Loss: 1.0864
Epoch: 33, Training Loss: 0.9716, Test Loss: 1.0822
Epoch: 34, Training Loss: 0.9699, Test Loss: 1.0802
Epoch: 35, Training Loss: 0.9671, Test Loss: 1.0735
Epoch: 36, Training Loss: 0.9657, Test Loss: 1.0579
Epoch: 37, Training Loss: 0.9578, Test Loss: 1.0593
Epoch: 38, Training Loss: 0.9531, Test Loss: 1.0486
Epoch: 39, Training Loss: 0.9547, Test Loss: 1.0445
Epoch: 40, Training Loss: 0.9470, Test Loss: 1.0277
Epoch: 41, Training Loss: 0.9432, Test Loss: 1.0322
Epoch: 42, Training Loss: 0.9382, Test Loss: 1.0347

 27%|██████████████████████████▋                                                                       | 68/250 [00:05<00:15, 11.87it/s, loss_test=0.951]
Epoch: 44, Training Loss: 0.9321, Test Loss: 1.0218
Epoch: 45, Training Loss: 0.9235, Test Loss: 1.0077
Epoch: 46, Training Loss: 0.9197, Test Loss: 1.0266
Epoch: 47, Training Loss: 0.9079, Test Loss: 1.0073
Epoch: 48, Training Loss: 0.9008, Test Loss: 0.9990
Epoch: 49, Training Loss: 0.8995, Test Loss: 0.9978
Epoch: 50, Training Loss: 0.8908, Test Loss: 0.9812
Epoch: 51, Training Loss: 0.8847, Test Loss: 0.9851
Epoch: 52, Training Loss: 0.8752, Test Loss: 0.9908
Epoch: 53, Training Loss: 0.8697, Test Loss: 0.9954
Epoch: 54, Training Loss: 0.8687, Test Loss: 0.9781
Epoch: 55, Training Loss: 0.8604, Test Loss: 0.9848
Epoch: 56, Training Loss: 0.8567, Test Loss: 0.9587
Epoch: 57, Training Loss: 0.8541, Test Loss: 0.9525
Epoch: 58, Training Loss: 0.8478, Test Loss: 0.9543
Epoch: 59, Training Loss: 0.8385, Test Loss: 0.9719
Epoch: 60, Training Loss: 0.8350, Test Loss: 0.9631
Epoch: 61, Training Loss: 0.8305, Test Loss: 0.9618
Epoch: 62, Training Loss: 0.8208, Test Loss: 0.9515
Epoch: 63, Training Loss: 0.8175, Test Loss: 0.9519
Epoch: 64, Training Loss: 0.8147, Test Loss: 0.9482
Epoch: 65, Training Loss: 0.8103, Test Loss: 0.9429
Epoch: 66, Training Loss: 0.8073, Test Loss: 0.9424

 38%|████████████████████████████████████▊                                                             | 94/250 [00:07<00:12, 12.24it/s, loss_test=0.906]
Epoch: 68, Training Loss: 0.7967, Test Loss: 0.9270
Epoch: 69, Training Loss: 0.7910, Test Loss: 0.9353
Epoch: 70, Training Loss: 0.7895, Test Loss: 0.9369
Epoch: 71, Training Loss: 0.7843, Test Loss: 0.9323
Epoch: 72, Training Loss: 0.7786, Test Loss: 0.9314
Epoch: 73, Training Loss: 0.7724, Test Loss: 0.9165
Epoch: 74, Training Loss: 0.7684, Test Loss: 0.9336
Epoch: 75, Training Loss: 0.7587, Test Loss: 0.9227
Epoch: 76, Training Loss: 0.7529, Test Loss: 0.9104
Epoch: 77, Training Loss: 0.7513, Test Loss: 0.9244
Epoch: 78, Training Loss: 0.7475, Test Loss: 0.9148
Epoch: 79, Training Loss: 0.7420, Test Loss: 0.9285
Epoch: 80, Training Loss: 0.7383, Test Loss: 0.9139
Epoch: 81, Training Loss: 0.7340, Test Loss: 0.9020
Epoch: 82, Training Loss: 0.7284, Test Loss: 0.9199
Epoch: 83, Training Loss: 0.7203, Test Loss: 0.9064
Epoch: 84, Training Loss: 0.7164, Test Loss: 0.9169
Epoch: 85, Training Loss: 0.7138, Test Loss: 0.9225
Epoch: 86, Training Loss: 0.7075, Test Loss: 0.9127
Epoch: 87, Training Loss: 0.7046, Test Loss: 0.9109
Epoch: 88, Training Loss: 0.7009, Test Loss: 0.9168
Epoch: 89, Training Loss: 0.6962, Test Loss: 0.9135
Epoch: 90, Training Loss: 0.6894, Test Loss: 0.9077
Epoch: 91, Training Loss: 0.6868, Test Loss: 0.9187

 47%|█████████████████████████████████████████████▊                                                   | 118/250 [00:09<00:11, 11.83it/s, loss_test=0.915]
Epoch: 93, Training Loss: 0.6759, Test Loss: 0.9056
Epoch: 94, Training Loss: 0.6760, Test Loss: 0.9109
Epoch: 95, Training Loss: 0.6726, Test Loss: 0.9188
Epoch: 96, Training Loss: 0.6656, Test Loss: 0.9064
Epoch: 97, Training Loss: 0.6646, Test Loss: 0.9129
Epoch: 98, Training Loss: 0.6580, Test Loss: 0.9069
Epoch: 99, Training Loss: 0.6547, Test Loss: 0.9158
Epoch: 100, Training Loss: 0.6517, Test Loss: 0.9165
Epoch: 101, Training Loss: 0.6457, Test Loss: 0.9092
Epoch: 102, Training Loss: 0.6429, Test Loss: 0.9242
Epoch: 103, Training Loss: 0.6361, Test Loss: 0.9127
Epoch: 104, Training Loss: 0.6365, Test Loss: 0.9144
Epoch: 105, Training Loss: 0.6349, Test Loss: 0.9290
Epoch: 106, Training Loss: 0.6306, Test Loss: 0.9099
Epoch: 107, Training Loss: 0.6259, Test Loss: 0.9134
Epoch: 108, Training Loss: 0.6216, Test Loss: 0.9274
Epoch: 109, Training Loss: 0.6178, Test Loss: 0.9290
Epoch: 110, Training Loss: 0.6155, Test Loss: 0.9234
Epoch: 111, Training Loss: 0.6121, Test Loss: 0.9226
Epoch: 112, Training Loss: 0.6078, Test Loss: 0.9096
Epoch: 113, Training Loss: 0.6058, Test Loss: 0.9289
Epoch: 114, Training Loss: 0.6015, Test Loss: 0.9179
Epoch: 115, Training Loss: 0.5989, Test Loss: 0.9376
Epoch: 116, Training Loss: 0.5943, Test Loss: 0.9309

 57%|███████████████████████████████████████████████████████                                          | 142/250 [00:11<00:08, 12.09it/s, loss_test=0.941]
Epoch: 118, Training Loss: 0.5861, Test Loss: 0.9142
Epoch: 119, Training Loss: 0.5853, Test Loss: 0.9237
Epoch: 120, Training Loss: 0.5847, Test Loss: 0.9397
Epoch: 121, Training Loss: 0.5793, Test Loss: 0.9332
Epoch: 122, Training Loss: 0.5751, Test Loss: 0.9203
Epoch: 123, Training Loss: 0.5729, Test Loss: 0.9212
Epoch: 124, Training Loss: 0.5691, Test Loss: 0.9363
Epoch: 125, Training Loss: 0.5673, Test Loss: 0.9156
Epoch: 126, Training Loss: 0.5661, Test Loss: 0.9228
Epoch: 127, Training Loss: 0.5608, Test Loss: 0.9350
Epoch: 128, Training Loss: 0.5588, Test Loss: 0.9256
Epoch: 129, Training Loss: 0.5539, Test Loss: 0.9211
Epoch: 130, Training Loss: 0.5522, Test Loss: 0.9171
Epoch: 131, Training Loss: 0.5484, Test Loss: 0.9371
Epoch: 132, Training Loss: 0.5451, Test Loss: 0.9174
Epoch: 133, Training Loss: 0.5423, Test Loss: 0.9272
Epoch: 134, Training Loss: 0.5393, Test Loss: 0.9304
Epoch: 135, Training Loss: 0.5372, Test Loss: 0.9299
Epoch: 136, Training Loss: 0.5337, Test Loss: 0.9453
Epoch: 137, Training Loss: 0.5339, Test Loss: 0.9167
Epoch: 138, Training Loss: 0.5289, Test Loss: 0.9448
Epoch: 139, Training Loss: 0.5218, Test Loss: 0.9275
Epoch: 140, Training Loss: 0.5239, Test Loss: 0.9218

 66%|███████████████████████████████████████████████████████████████▋                                 | 164/250 [00:13<00:08, 10.47it/s, loss_test=0.942]
Epoch: 142, Training Loss: 0.5167, Test Loss: 0.9409
Epoch: 143, Training Loss: 0.5152, Test Loss: 0.9305
Epoch: 144, Training Loss: 0.5119, Test Loss: 0.9255
Epoch: 145, Training Loss: 0.5095, Test Loss: 0.9087
Epoch: 146, Training Loss: 0.5075, Test Loss: 0.9467
Epoch: 147, Training Loss: 0.5046, Test Loss: 0.9449
Epoch: 148, Training Loss: 0.5028, Test Loss: 0.9313
Epoch: 149, Training Loss: 0.4992, Test Loss: 0.9323
Epoch: 150, Training Loss: 0.4951, Test Loss: 0.9384
Epoch: 151, Training Loss: 0.4931, Test Loss: 0.9319
Epoch: 152, Training Loss: 0.4909, Test Loss: 0.9245
Epoch: 153, Training Loss: 0.4896, Test Loss: 0.9306
Epoch: 154, Training Loss: 0.4874, Test Loss: 0.9444
Epoch: 155, Training Loss: 0.4830, Test Loss: 0.9373
Epoch: 156, Training Loss: 0.4811, Test Loss: 0.9486
Epoch: 157, Training Loss: 0.4785, Test Loss: 0.9459
Epoch: 158, Training Loss: 0.4751, Test Loss: 0.9483
Epoch: 159, Training Loss: 0.4722, Test Loss: 0.9325
Epoch: 160, Training Loss: 0.4717, Test Loss: 0.9444
Epoch: 161, Training Loss: 0.4693, Test Loss: 0.9464
Epoch: 162, Training Loss: 0.4654, Test Loss: 0.9409

 76%|█████████████████████████████████████████████████████████████████████████▋                       | 190/250 [00:15<00:04, 12.57it/s, loss_test=0.968]
Epoch: 164, Training Loss: 0.4604, Test Loss: 0.9416
Epoch: 165, Training Loss: 0.4591, Test Loss: 0.9584
Epoch: 166, Training Loss: 0.4569, Test Loss: 0.9448
Epoch: 167, Training Loss: 0.4545, Test Loss: 0.9567
Epoch: 168, Training Loss: 0.4516, Test Loss: 0.9319
Epoch: 169, Training Loss: 0.4500, Test Loss: 0.9612
Epoch: 170, Training Loss: 0.4469, Test Loss: 0.9505
Epoch: 171, Training Loss: 0.4422, Test Loss: 0.9520
Epoch: 172, Training Loss: 0.4410, Test Loss: 0.9398
Epoch: 173, Training Loss: 0.4388, Test Loss: 0.9583
Epoch: 174, Training Loss: 0.4383, Test Loss: 0.9463
Epoch: 175, Training Loss: 0.4354, Test Loss: 0.9552
Epoch: 176, Training Loss: 0.4326, Test Loss: 0.9572
Epoch: 177, Training Loss: 0.4305, Test Loss: 0.9578
Epoch: 178, Training Loss: 0.4296, Test Loss: 0.9656
Epoch: 179, Training Loss: 0.4263, Test Loss: 0.9479
Epoch: 180, Training Loss: 0.4236, Test Loss: 0.9579
Epoch: 181, Training Loss: 0.4216, Test Loss: 0.9489
Epoch: 182, Training Loss: 0.4187, Test Loss: 0.9582
Epoch: 183, Training Loss: 0.4183, Test Loss: 0.9513
Epoch: 184, Training Loss: 0.4156, Test Loss: 0.9555
Epoch: 185, Training Loss: 0.4128, Test Loss: 0.9623
Epoch: 186, Training Loss: 0.4111, Test Loss: 0.9596
Epoch: 187, Training Loss: 0.4095, Test Loss: 0.9693
Epoch: 188, Training Loss: 0.4060, Test Loss: 0.9596

 86%|███████████████████████████████████████████████████████████████████████████████████              | 214/250 [00:17<00:02, 12.09it/s, loss_test=0.989]
Epoch: 190, Training Loss: 0.4028, Test Loss: 0.9684
Epoch: 191, Training Loss: 0.4011, Test Loss: 0.9746
Epoch: 192, Training Loss: 0.3989, Test Loss: 0.9736
Epoch: 193, Training Loss: 0.3963, Test Loss: 0.9545
Epoch: 194, Training Loss: 0.3935, Test Loss: 0.9558
Epoch: 195, Training Loss: 0.3928, Test Loss: 0.9658
Epoch: 196, Training Loss: 0.3904, Test Loss: 0.9582
Epoch: 197, Training Loss: 0.3896, Test Loss: 0.9597
Epoch: 198, Training Loss: 0.3867, Test Loss: 0.9732
Epoch: 199, Training Loss: 0.3840, Test Loss: 0.9533
Epoch: 200, Training Loss: 0.3814, Test Loss: 0.9726
Epoch: 201, Training Loss: 0.3801, Test Loss: 0.9803
Epoch: 202, Training Loss: 0.3790, Test Loss: 0.9834
Epoch: 203, Training Loss: 0.3774, Test Loss: 0.9756
Epoch: 204, Training Loss: 0.3757, Test Loss: 0.9744
Epoch: 205, Training Loss: 0.3726, Test Loss: 0.9671
Epoch: 206, Training Loss: 0.3710, Test Loss: 0.9721
Epoch: 207, Training Loss: 0.3699, Test Loss: 0.9784
Epoch: 208, Training Loss: 0.3683, Test Loss: 0.9690
Epoch: 209, Training Loss: 0.3654, Test Loss: 0.9889
Epoch: 210, Training Loss: 0.3650, Test Loss: 0.9822
Epoch: 211, Training Loss: 0.3612, Test Loss: 0.9849
Epoch: 212, Training Loss: 0.3601, Test Loss: 0.9760

 96%|█████████████████████████████████████████████████████████████████████████████████████████████    | 240/250 [00:19<00:00, 12.53it/s, loss_test=1.013]
Epoch: 214, Training Loss: 0.3580, Test Loss: 0.9894
Epoch: 215, Training Loss: 0.3549, Test Loss: 0.9847
Epoch: 216, Training Loss: 0.3552, Test Loss: 0.9889
Epoch: 217, Training Loss: 0.3522, Test Loss: 0.9873
Epoch: 218, Training Loss: 0.3501, Test Loss: 0.9766
Epoch: 219, Training Loss: 0.3482, Test Loss: 0.9840
Epoch: 220, Training Loss: 0.3479, Test Loss: 0.9911
Epoch: 221, Training Loss: 0.3452, Test Loss: 0.9722
Epoch: 222, Training Loss: 0.3438, Test Loss: 0.9845
Epoch: 223, Training Loss: 0.3404, Test Loss: 0.9807
Epoch: 224, Training Loss: 0.3409, Test Loss: 0.9932
Epoch: 225, Training Loss: 0.3398, Test Loss: 0.9901
Epoch: 226, Training Loss: 0.3362, Test Loss: 0.9899
Epoch: 227, Training Loss: 0.3364, Test Loss: 0.9949
Epoch: 228, Training Loss: 0.3346, Test Loss: 0.9749
Epoch: 229, Training Loss: 0.3335, Test Loss: 0.9916
Epoch: 230, Training Loss: 0.3316, Test Loss: 0.9860
Epoch: 231, Training Loss: 0.3293, Test Loss: 1.0165
Epoch: 232, Training Loss: 0.3271, Test Loss: 0.9829
Epoch: 233, Training Loss: 0.3265, Test Loss: 0.9881
Epoch: 234, Training Loss: 0.3247, Test Loss: 0.9778
Epoch: 235, Training Loss: 0.3233, Test Loss: 1.0149
Epoch: 236, Training Loss: 0.3216, Test Loss: 0.9958
Epoch: 237, Training Loss: 0.3215, Test Loss: 1.0061

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.12it/s, loss_test=1.013]
Epoch: 239, Training Loss: 0.3183, Test Loss: 0.9926
Epoch: 240, Training Loss: 0.3160, Test Loss: 1.0131
Epoch: 241, Training Loss: 0.3154, Test Loss: 0.9998
Epoch: 242, Training Loss: 0.3136, Test Loss: 0.9884
Epoch: 243, Training Loss: 0.3127, Test Loss: 0.9946
Epoch: 244, Training Loss: 0.3104, Test Loss: 1.0088
Epoch: 245, Training Loss: 0.3107, Test Loss: 1.0057
Epoch: 246, Training Loss: 0.3081, Test Loss: 1.0030
Epoch: 247, Training Loss: 0.3070, Test Loss: 1.0113
Epoch: 248, Training Loss: 0.3048, Test Loss: 0.9966
Epoch: 249, Training Loss: 0.3030, Test Loss: 1.0128
Model saved as model_7376613np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-129000-7376613np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9939539670944214, 0.9923506617546082, 0.9945237636566162, 0.9902162551879883, 0.9925743818283081, 0.9939748287200928, 0.9948363542556763, 0.9937691807746887, 0.9914722681045532, 0.9874429106712341, 0.9877121090888977, 0.9850060701370239, 0.9914409160614014, 0.9888806939125061, 0.9906327366828919, 0.9881196856498718, 0.9862743616104126, 0.9874499917030335, 0.9869016170501709, 0.9895206212997436, 0.9824890851974487, 0.9867651462554932, 0.9841529965400696, 0.9849050998687744, 0.9839807987213135, 0.9783969998359681, 0.983373486995697, 0.9811996459960938, 0.983507239818573, 0.9761680960655212, 0.9808294773101807, 0.9785515546798706, 0.9733650326728821, 0.9715858817100524, 0.9699019193649292, 0.9671202301979065, 0.9656780123710632, 0.9577903985977173, 0.9531436920166015, 0.9546603679656982, 0.9469869017601014, 0.9431607007980347, 0.9381875038146973, 0.9305582880973816, 0.9321181535720825, 0.9235451459884644, 0.9197457909584046, 0.9079044342041016, 0.9007705688476563, 0.8994749546051025, 0.8908439874649048, 0.8847318053245544, 0.8751530170440673, 0.8696912169456482, 0.8687195181846619, 0.8604029774665832, 0.856717872619629, 0.8540902018547059, 0.847822368144989, 0.8384936213493347, 0.8349858522415161, 0.8305237770080567, 0.8208107590675354, 0.8174945712089539, 0.8146963000297547, 0.8103177309036255, 0.807304048538208, 0.8021075963973999, 0.7967322707176209, 0.7910305380821228, 0.7895216584205628, 0.7843393802642822, 0.7785640597343445, 0.7723772048950195, 0.7683886170387269, 0.7586921334266663, 0.7529351592063904, 0.7512840867042542, 0.7475290060043335, 0.7420387983322143, 0.7383456230163574, 0.7340133428573609, 0.7283763289451599, 0.7203350305557251, 0.7164128422737122, 0.713796877861023, 0.7074862837791442, 0.7045708179473877, 0.7008500576019288, 0.6962293744087219, 0.6893596768379211, 0.6867523193359375, 0.6832609295845031, 0.6759194970130921, 0.6759846091270447, 0.6725537538528442, 0.6655573725700379, 0.6646097898483276, 0.6579606294631958, 0.6547292947769165, 0.6516500115394592, 0.6457168221473694, 0.6428834915161132, 0.6361059665679931, 0.6364812731742859, 0.6348721742630005, 0.6306387901306152, 0.6259469151496887, 0.6215714097023011, 0.6177912950515747, 0.6154981851577759, 0.6121229887008667, 0.6077807307243347, 0.6057684421539307, 0.6015228748321533, 0.5989394187927246, 0.5943415403366089, 0.5914573669433594, 0.5860620498657226, 0.5853171110153198, 0.5846995949745178, 0.5792519092559815, 0.5751097679138184, 0.5729144930839538, 0.5690768480300903, 0.5672675132751465, 0.5661096334457397, 0.5607621192932128, 0.5588143110275269, 0.5539084911346436, 0.5522369980812073, 0.5483572125434876, 0.5450637578964234, 0.5422577023506164, 0.5392980337142944, 0.5372321844100952, 0.5336605548858643, 0.5338701009750366, 0.5289297819137573, 0.5217561960220337, 0.5238634467124939, 0.5178731560707093, 0.5167449355125427, 0.5151549100875854, 0.511938887834549, 0.509456467628479, 0.5074701070785522, 0.504647147655487, 0.5027990937232971, 0.4992038905620575, 0.49509509801864626, 0.49310309886932374, 0.4909333109855652, 0.48960510492324827, 0.4874164640903473, 0.48296716809272766, 0.4810577094554901, 0.4785451292991638, 0.4750676453113556, 0.47220317721366883, 0.47173221707344054, 0.4693123996257782, 0.46539207696914675, 0.4644691824913025, 0.46038219928741453, 0.4591173827648163, 0.45693633556365965, 0.4545316696166992, 0.4516218602657318, 0.4499557614326477, 0.446931391954422, 0.4421508967876434, 0.4409501552581787, 0.4388261497020721, 0.4383219540119171, 0.43544477224349976, 0.432635509967804, 0.4304709851741791, 0.4296097159385681, 0.4262986063957214, 0.42356239557266234, 0.42161338925361636, 0.41871038675308225, 0.41831969022750853, 0.41561729907989503, 0.4127509295940399, 0.41105822324752805, 0.40952442288398744, 0.4059934437274933, 0.40485153198242185, 0.4028414309024811, 0.4010693311691284, 0.39893542528152465, 0.3962622880935669, 0.3935295820236206, 0.39282941818237305, 0.3904473066329956, 0.38955485820770264, 0.3867473781108856, 0.3840376019477844, 0.3813984334468842, 0.38006999492645266, 0.37903563380241395, 0.377362984418869, 0.3757019639015198, 0.3726012945175171, 0.37102268934249877, 0.36991413235664367, 0.3682784616947174, 0.36537225246429444, 0.36503940224647524, 0.3611979722976685, 0.36011268496513366, 0.3592588722705841, 0.35801826119422914, 0.3549013078212738, 0.3552381157875061, 0.35215352177619935, 0.3500990211963654, 0.3481948137283325, 0.3478559374809265, 0.3451746702194214, 0.34376338720321653, 0.34035958647727965, 0.3409048140048981, 0.3397729814052582, 0.3361791133880615, 0.33642515540122986, 0.33455086350440977, 0.33348512649536133, 0.33156455159187315, 0.3293269634246826, 0.32713263034820556, 0.32653496265411375, 0.32471755146980286, 0.3232630670070648, 0.32164366245269777, 0.32147631645202634, 0.3193166196346283, 0.31825162172317506, 0.31601730585098264, 0.3153665721416473, 0.3135985732078552, 0.31271365880966184, 0.31037846207618713, 0.31066772937774656, 0.30805850625038145, 0.3070147275924683, 0.30475099086761476, 0.302988737821579], 'loss_test': [1.1110273599624634, 1.0993335247039795, 1.081559181213379, 1.0879720449447632, 1.0889376401901245, 1.0854336023330688, 1.095286250114441, 1.1016305685043335, 1.09172785282135, 1.0825388431549072, 1.087490200996399, 1.0986359119415283, 1.080174446105957, 1.0865987539291382, 1.079789400100708, 1.0911328792572021, 1.0732214450836182, 1.0743751525878906, 1.0895801782608032, 1.0837335586547852, 1.0884852409362793, 1.0799428224563599, 1.0757865905761719, 1.0699241161346436, 1.0801366567611694, 1.076404333114624, 1.0809779167175293, 1.071289300918579, 1.1006337404251099, 1.0879034996032715, 1.0785714387893677, 1.0729233026504517, 1.0864062309265137, 1.0821704864501953, 1.080215334892273, 1.0735292434692383, 1.0578999519348145, 1.0592515468597412, 1.048567533493042, 1.0445058345794678, 1.0277392864227295, 1.0322428941726685, 1.034665584564209, 1.0346537828445435, 1.0217524766921997, 1.007712960243225, 1.026610016822815, 1.0073323249816895, 0.9989914298057556, 0.9977564811706543, 0.981168806552887, 0.9850667715072632, 0.9908350706100464, 0.9953731298446655, 0.9780880808830261, 0.9847775101661682, 0.9587138295173645, 0.9524959921836853, 0.9543241262435913, 0.9718540906906128, 0.9631048440933228, 0.9618172645568848, 0.9514831900596619, 0.9519408345222473, 0.9481623768806458, 0.9428949952125549, 0.9423538446426392, 0.9510166049003601, 0.9269669651985168, 0.9352511167526245, 0.9369181394577026, 0.9323227405548096, 0.931364119052887, 0.9165251851081848, 0.9335829019546509, 0.9226922392845154, 0.9104463458061218, 0.9244474768638611, 0.9147928953170776, 0.9285110831260681, 0.9139395952224731, 0.9020450115203857, 0.9198969602584839, 0.9064431190490723, 0.9168536067008972, 0.9224825501441956, 0.9127075672149658, 0.9108886122703552, 0.9168094992637634, 0.9135435819625854, 0.9076699614524841, 0.9186996817588806, 0.9085779786109924, 0.905613899230957, 0.9108631014823914, 0.9188029170036316, 0.9064266681671143, 0.9129301309585571, 0.9069486856460571, 0.9158369302749634, 0.9165378212928772, 0.9091888666152954, 0.9241945147514343, 0.9126753211021423, 0.9144246578216553, 0.9290313720703125, 0.909866213798523, 0.9134231805801392, 0.9273554086685181, 0.9289851784706116, 0.9234119057655334, 0.9226322174072266, 0.9096390604972839, 0.9288513660430908, 0.9178991913795471, 0.9375742673873901, 0.930905282497406, 0.915135383605957, 0.9142338633537292, 0.9237335324287415, 0.9397212862968445, 0.9331761002540588, 0.9202547073364258, 0.9211668372154236, 0.9363057017326355, 0.9156336784362793, 0.9227866530418396, 0.934962809085846, 0.9256237745285034, 0.9211244583129883, 0.9170559644699097, 0.9370560050010681, 0.9173728227615356, 0.9272147417068481, 0.9304177165031433, 0.9299004673957825, 0.9452893137931824, 0.9166699051856995, 0.9447725415229797, 0.9274614453315735, 0.9218072891235352, 0.924751877784729, 0.9408629536628723, 0.9305230379104614, 0.9254595041275024, 0.9087401032447815, 0.9467036128044128, 0.9448627233505249, 0.9312992095947266, 0.9322786331176758, 0.9383706450462341, 0.9318668246269226, 0.924523115158081, 0.9306439757347107, 0.9443895220756531, 0.9372904896736145, 0.9486493468284607, 0.945864200592041, 0.9483284950256348, 0.9324516654014587, 0.9443690180778503, 0.9463903307914734, 0.9408687353134155, 0.9385794401168823, 0.9416283965110779, 0.9583505392074585, 0.9448217153549194, 0.956718385219574, 0.9318946003913879, 0.9612470269203186, 0.9505462646484375, 0.9519996047019958, 0.9398257732391357, 0.9583035111427307, 0.9462904334068298, 0.9552090167999268, 0.9571540355682373, 0.9578164219856262, 0.9656172394752502, 0.9478698968887329, 0.9579017758369446, 0.9489121437072754, 0.9581896066665649, 0.9513043165206909, 0.9554621577262878, 0.9623392224311829, 0.9596348404884338, 0.9693027138710022, 0.9595683217048645, 0.9574729204177856, 0.9684212803840637, 0.974571168422699, 0.9735797047615051, 0.9544501900672913, 0.9557569026947021, 0.9657917618751526, 0.9582059383392334, 0.9596611857414246, 0.9731608033180237, 0.9533385038375854, 0.972550630569458, 0.9803054332733154, 0.9833563566207886, 0.9755878448486328, 0.9744023680686951, 0.967147707939148, 0.9720648527145386, 0.9783991575241089, 0.9689720273017883, 0.9888936877250671, 0.9822189807891846, 0.984866201877594, 0.9760116934776306, 0.9738906025886536, 0.9894265532493591, 0.9847322702407837, 0.9889129996299744, 0.9873248934745789, 0.9765738844871521, 0.9839832186698914, 0.9910615086555481, 0.9722288846969604, 0.9845247268676758, 0.9807264804840088, 0.9931973218917847, 0.9901407361030579, 0.989937424659729, 0.994888424873352, 0.9749453663825989, 0.9916316866874695, 0.9860497713088989, 1.0164715051651, 0.9828987121582031, 0.9881271719932556, 0.9778244495391846, 1.014869213104248, 0.9958220720291138, 1.0061169862747192, 1.0113075971603394, 0.9926214814186096, 1.0131350755691528, 0.9998204708099365, 0.9883686900138855, 0.9946451783180237, 1.0088378190994263, 1.0056804418563843, 1.0030157566070557, 1.0113377571105957, 0.9966332316398621, 1.0127583742141724], 'identifier': '7376613np'}