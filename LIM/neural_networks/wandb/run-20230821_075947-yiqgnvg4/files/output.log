Epoch: 00, Training Loss: 0.9896, Test Loss: 1.0890
Epoch: 01, Training Loss: 0.9967, Test Loss: 1.0697
Epoch: 02, Training Loss: 0.9945, Test Loss: 1.0814
Epoch: 03, Training Loss: 0.9968, Test Loss: 1.0803
Epoch: 04, Training Loss: 0.9909, Test Loss: 1.0867
Epoch: 05, Training Loss: 0.9934, Test Loss: 1.0829
Epoch: 06, Training Loss: 0.9910, Test Loss: 1.0801
Epoch: 07, Training Loss: 0.9878, Test Loss: 1.0911
Epoch: 08, Training Loss: 0.9882, Test Loss: 1.0881
Epoch: 09, Training Loss: 0.9898, Test Loss: 1.0733
Epoch: 10, Training Loss: 0.9871, Test Loss: 1.0769
Epoch: 11, Training Loss: 0.9861, Test Loss: 1.0797
Epoch: 12, Training Loss: 0.9860, Test Loss: 1.0883
Epoch: 13, Training Loss: 0.9908, Test Loss: 1.0862
Epoch: 14, Training Loss: 0.9840, Test Loss: 1.0695
Epoch: 15, Training Loss: 0.9854, Test Loss: 1.0808
Epoch: 16, Training Loss: 0.9905, Test Loss: 1.0823
Epoch: 17, Training Loss: 0.9884, Test Loss: 1.0793
Epoch: 18, Training Loss: 0.9862, Test Loss: 1.0899
Epoch: 19, Training Loss: 0.9871, Test Loss: 1.0892
  8%|███████▊                                                                                          | 20/250 [00:01<00:19, 12.04it/s, loss_test=1.089]
Epoch: 20, Training Loss: 0.9848, Test Loss: 1.0745
Epoch: 21, Training Loss: 0.9874, Test Loss: 1.0719
Epoch: 22, Training Loss: 0.9807, Test Loss: 1.0828
Epoch: 23, Training Loss: 0.9804, Test Loss: 1.0912
Epoch: 24, Training Loss: 0.9811, Test Loss: 1.0748
Epoch: 25, Training Loss: 0.9774, Test Loss: 1.0840
Epoch: 26, Training Loss: 0.9805, Test Loss: 1.0714
Epoch: 27, Training Loss: 0.9818, Test Loss: 1.0841
Epoch: 28, Training Loss: 0.9767, Test Loss: 1.0947
Epoch: 29, Training Loss: 0.9729, Test Loss: 1.0709
Epoch: 30, Training Loss: 0.9682, Test Loss: 1.0734
Epoch: 31, Training Loss: 0.9623, Test Loss: 1.0588
Epoch: 32, Training Loss: 0.9519, Test Loss: 1.0470
Epoch: 33, Training Loss: 0.9426, Test Loss: 1.0350
Epoch: 34, Training Loss: 0.9325, Test Loss: 1.0326
Epoch: 35, Training Loss: 0.9252, Test Loss: 1.0238
Epoch: 36, Training Loss: 0.9199, Test Loss: 1.0225
Epoch: 37, Training Loss: 0.9147, Test Loss: 1.0029
Epoch: 38, Training Loss: 0.9111, Test Loss: 1.0197
Epoch: 39, Training Loss: 0.9043, Test Loss: 1.0023
Epoch: 40, Training Loss: 0.9030, Test Loss: 0.9925
Epoch: 41, Training Loss: 0.8992, Test Loss: 0.9917
Epoch: 42, Training Loss: 0.8962, Test Loss: 0.9978
Epoch: 43, Training Loss: 0.8904, Test Loss: 0.9808


 27%|██████████████████████████▋                                                                       | 68/250 [00:05<00:15, 12.11it/s, loss_test=0.906]
Epoch: 45, Training Loss: 0.8860, Test Loss: 0.9822
Epoch: 46, Training Loss: 0.8796, Test Loss: 0.9849
Epoch: 47, Training Loss: 0.8742, Test Loss: 0.9815
Epoch: 48, Training Loss: 0.8729, Test Loss: 0.9795
Epoch: 49, Training Loss: 0.8680, Test Loss: 0.9625
Epoch: 50, Training Loss: 0.8641, Test Loss: 0.9715
Epoch: 51, Training Loss: 0.8603, Test Loss: 0.9380
Epoch: 52, Training Loss: 0.8565, Test Loss: 0.9692
Epoch: 53, Training Loss: 0.8515, Test Loss: 0.9543
Epoch: 54, Training Loss: 0.8515, Test Loss: 0.9598
Epoch: 55, Training Loss: 0.8418, Test Loss: 0.9373
Epoch: 56, Training Loss: 0.8437, Test Loss: 0.9288
Epoch: 57, Training Loss: 0.8373, Test Loss: 0.9341
Epoch: 58, Training Loss: 0.8327, Test Loss: 0.9377
Epoch: 59, Training Loss: 0.8288, Test Loss: 0.9268
Epoch: 60, Training Loss: 0.8223, Test Loss: 0.9323
Epoch: 61, Training Loss: 0.8178, Test Loss: 0.9337
Epoch: 62, Training Loss: 0.8170, Test Loss: 0.9270
Epoch: 63, Training Loss: 0.8102, Test Loss: 0.9056
Epoch: 64, Training Loss: 0.8085, Test Loss: 0.9195
Epoch: 65, Training Loss: 0.8040, Test Loss: 0.9171
Epoch: 66, Training Loss: 0.7973, Test Loss: 0.9239
Epoch: 67, Training Loss: 0.7938, Test Loss: 0.9112
Epoch: 68, Training Loss: 0.7918, Test Loss: 0.9056
Epoch: 69, Training Loss: 0.7884, Test Loss: 0.9082
Epoch: 70, Training Loss: 0.7831, Test Loss: 0.9106
Epoch: 71, Training Loss: 0.7781, Test Loss: 0.9042
Epoch: 72, Training Loss: 0.7772, Test Loss: 0.8959
Epoch: 73, Training Loss: 0.7706, Test Loss: 0.8880
Epoch: 74, Training Loss: 0.7678, Test Loss: 0.8963
Epoch: 75, Training Loss: 0.7646, Test Loss: 0.8999
Epoch: 76, Training Loss: 0.7593, Test Loss: 0.8969
Epoch: 77, Training Loss: 0.7573, Test Loss: 0.8941
Epoch: 78, Training Loss: 0.7542, Test Loss: 0.8943
Epoch: 79, Training Loss: 0.7500, Test Loss: 0.8864
Epoch: 80, Training Loss: 0.7461, Test Loss: 0.8848
Epoch: 81, Training Loss: 0.7424, Test Loss: 0.9008
Epoch: 82, Training Loss: 0.7379, Test Loss: 0.8897
Epoch: 83, Training Loss: 0.7313, Test Loss: 0.8926
Epoch: 84, Training Loss: 0.7302, Test Loss: 0.8828
Epoch: 85, Training Loss: 0.7295, Test Loss: 0.8822
Epoch: 86, Training Loss: 0.7223, Test Loss: 0.8797
Epoch: 87, Training Loss: 0.7187, Test Loss: 0.8919
Epoch: 88, Training Loss: 0.7158, Test Loss: 0.8745
Epoch: 89, Training Loss: 0.7107, Test Loss: 0.8876
Epoch: 90, Training Loss: 0.7098, Test Loss: 0.8891
Epoch: 91, Training Loss: 0.7021, Test Loss: 0.8732
Epoch: 92, Training Loss: 0.6975, Test Loss: 0.8958

 38%|████████████████████████████████████▊                                                             | 94/250 [00:07<00:12, 12.61it/s, loss_test=0.888]
Epoch: 94, Training Loss: 0.6900, Test Loss: 0.8739
Epoch: 95, Training Loss: 0.6890, Test Loss: 0.8898
Epoch: 96, Training Loss: 0.6859, Test Loss: 0.8754
Epoch: 97, Training Loss: 0.6798, Test Loss: 0.8958
Epoch: 98, Training Loss: 0.6777, Test Loss: 0.8725
Epoch: 99, Training Loss: 0.6721, Test Loss: 0.8872
Epoch: 100, Training Loss: 0.6717, Test Loss: 0.9006
Epoch: 101, Training Loss: 0.6659, Test Loss: 0.8966
Epoch: 102, Training Loss: 0.6624, Test Loss: 0.8923
Epoch: 103, Training Loss: 0.6578, Test Loss: 0.8902
Epoch: 104, Training Loss: 0.6560, Test Loss: 0.8915
Epoch: 105, Training Loss: 0.6526, Test Loss: 0.8937
Epoch: 106, Training Loss: 0.6506, Test Loss: 0.8935
Epoch: 107, Training Loss: 0.6455, Test Loss: 0.8906
Epoch: 108, Training Loss: 0.6433, Test Loss: 0.9060
Epoch: 109, Training Loss: 0.6396, Test Loss: 0.9058
Epoch: 110, Training Loss: 0.6336, Test Loss: 0.8919
Epoch: 111, Training Loss: 0.6305, Test Loss: 0.8969
Epoch: 112, Training Loss: 0.6288, Test Loss: 0.8931
Epoch: 113, Training Loss: 0.6244, Test Loss: 0.8954
Epoch: 114, Training Loss: 0.6224, Test Loss: 0.9058
Epoch: 115, Training Loss: 0.6208, Test Loss: 0.9064
Epoch: 116, Training Loss: 0.6164, Test Loss: 0.8996
Epoch: 117, Training Loss: 0.6131, Test Loss: 0.9022

 47%|█████████████████████████████████████████████▊                                                   | 118/250 [00:09<00:11, 11.93it/s, loss_test=0.916]
Epoch: 119, Training Loss: 0.6062, Test Loss: 0.9102
Epoch: 120, Training Loss: 0.6044, Test Loss: 0.8994
Epoch: 121, Training Loss: 0.5998, Test Loss: 0.9055
Epoch: 122, Training Loss: 0.5974, Test Loss: 0.9062
Epoch: 123, Training Loss: 0.5927, Test Loss: 0.9002
Epoch: 124, Training Loss: 0.5906, Test Loss: 0.9095
Epoch: 125, Training Loss: 0.5880, Test Loss: 0.9157
Epoch: 126, Training Loss: 0.5851, Test Loss: 0.9203
Epoch: 127, Training Loss: 0.5836, Test Loss: 0.9185
Epoch: 128, Training Loss: 0.5791, Test Loss: 0.9136
Epoch: 129, Training Loss: 0.5774, Test Loss: 0.9152
Epoch: 130, Training Loss: 0.5742, Test Loss: 0.9149
Epoch: 131, Training Loss: 0.5712, Test Loss: 0.9220
Epoch: 132, Training Loss: 0.5672, Test Loss: 0.9030
Epoch: 133, Training Loss: 0.5641, Test Loss: 0.9088
Epoch: 134, Training Loss: 0.5617, Test Loss: 0.9110
Epoch: 135, Training Loss: 0.5580, Test Loss: 0.9096
Epoch: 136, Training Loss: 0.5565, Test Loss: 0.9249
Epoch: 137, Training Loss: 0.5509, Test Loss: 0.9222
Epoch: 138, Training Loss: 0.5501, Test Loss: 0.9206
Epoch: 139, Training Loss: 0.5462, Test Loss: 0.9117
Epoch: 140, Training Loss: 0.5414, Test Loss: 0.9122
Epoch: 141, Training Loss: 0.5413, Test Loss: 0.9301


 67%|█████████████████████████████████████████████████████████████████▏                               | 168/250 [00:13<00:06, 12.12it/s, loss_test=0.949]
Epoch: 143, Training Loss: 0.5345, Test Loss: 0.9246
Epoch: 144, Training Loss: 0.5331, Test Loss: 0.9189
Epoch: 145, Training Loss: 0.5311, Test Loss: 0.9142
Epoch: 146, Training Loss: 0.5278, Test Loss: 0.9270
Epoch: 147, Training Loss: 0.5272, Test Loss: 0.9158
Epoch: 148, Training Loss: 0.5237, Test Loss: 0.9374
Epoch: 149, Training Loss: 0.5197, Test Loss: 0.9216
Epoch: 150, Training Loss: 0.5164, Test Loss: 0.9311
Epoch: 151, Training Loss: 0.5127, Test Loss: 0.9271
Epoch: 152, Training Loss: 0.5100, Test Loss: 0.9293
Epoch: 153, Training Loss: 0.5090, Test Loss: 0.9321
Epoch: 154, Training Loss: 0.5057, Test Loss: 0.9312
Epoch: 155, Training Loss: 0.5055, Test Loss: 0.9399
Epoch: 156, Training Loss: 0.5022, Test Loss: 0.9432
Epoch: 157, Training Loss: 0.4981, Test Loss: 0.9468
Epoch: 158, Training Loss: 0.4954, Test Loss: 0.9293
Epoch: 159, Training Loss: 0.4952, Test Loss: 0.9466
Epoch: 160, Training Loss: 0.4902, Test Loss: 0.9378
Epoch: 161, Training Loss: 0.4884, Test Loss: 0.9347
Epoch: 162, Training Loss: 0.4852, Test Loss: 0.9424
Epoch: 163, Training Loss: 0.4841, Test Loss: 0.9439
Epoch: 164, Training Loss: 0.4805, Test Loss: 0.9514
Epoch: 165, Training Loss: 0.4790, Test Loss: 0.9415
Epoch: 166, Training Loss: 0.4780, Test Loss: 0.9484

 77%|██████████████████████████████████████████████████████████████████████████▍                      | 192/250 [00:15<00:04, 11.62it/s, loss_test=0.972]
Epoch: 168, Training Loss: 0.4707, Test Loss: 0.9494
Epoch: 169, Training Loss: 0.4686, Test Loss: 0.9516
Epoch: 170, Training Loss: 0.4672, Test Loss: 0.9464
Epoch: 171, Training Loss: 0.4633, Test Loss: 0.9679
Epoch: 172, Training Loss: 0.4633, Test Loss: 0.9417
Epoch: 173, Training Loss: 0.4588, Test Loss: 0.9494
Epoch: 174, Training Loss: 0.4564, Test Loss: 0.9505
Epoch: 175, Training Loss: 0.4554, Test Loss: 0.9514
Epoch: 176, Training Loss: 0.4525, Test Loss: 0.9557
Epoch: 177, Training Loss: 0.4496, Test Loss: 0.9391
Epoch: 178, Training Loss: 0.4479, Test Loss: 0.9551
Epoch: 179, Training Loss: 0.4442, Test Loss: 0.9621
Epoch: 180, Training Loss: 0.4425, Test Loss: 0.9717
Epoch: 181, Training Loss: 0.4404, Test Loss: 0.9528
Epoch: 182, Training Loss: 0.4386, Test Loss: 0.9557
Epoch: 183, Training Loss: 0.4343, Test Loss: 0.9682
Epoch: 184, Training Loss: 0.4337, Test Loss: 0.9672
Epoch: 185, Training Loss: 0.4315, Test Loss: 0.9701
Epoch: 186, Training Loss: 0.4318, Test Loss: 0.9531
Epoch: 187, Training Loss: 0.4281, Test Loss: 0.9787
Epoch: 188, Training Loss: 0.4251, Test Loss: 0.9730
Epoch: 189, Training Loss: 0.4221, Test Loss: 0.9694
Epoch: 190, Training Loss: 0.4195, Test Loss: 0.9797
Epoch: 191, Training Loss: 0.4182, Test Loss: 0.9780
Epoch: 192, Training Loss: 0.4175, Test Loss: 0.9715
Epoch: 193, Training Loss: 0.4159, Test Loss: 0.9801
Epoch: 194, Training Loss: 0.4109, Test Loss: 0.9690
Epoch: 195, Training Loss: 0.4105, Test Loss: 0.9844
Epoch: 196, Training Loss: 0.4079, Test Loss: 0.9530
Epoch: 197, Training Loss: 0.4057, Test Loss: 0.9764
Epoch: 198, Training Loss: 0.4049, Test Loss: 0.9780
Epoch: 199, Training Loss: 0.4020, Test Loss: 0.9719
Epoch: 200, Training Loss: 0.4008, Test Loss: 0.9841
Epoch: 201, Training Loss: 0.3986, Test Loss: 0.9765
Epoch: 202, Training Loss: 0.3963, Test Loss: 0.9889
Epoch: 203, Training Loss: 0.3931, Test Loss: 0.9990
Epoch: 204, Training Loss: 0.3913, Test Loss: 0.9922
Epoch: 205, Training Loss: 0.3910, Test Loss: 0.9946
Epoch: 206, Training Loss: 0.3894, Test Loss: 0.9819
Epoch: 207, Training Loss: 0.3865, Test Loss: 0.9771
Epoch: 208, Training Loss: 0.3822, Test Loss: 0.9793
Epoch: 209, Training Loss: 0.3814, Test Loss: 0.9928
Epoch: 210, Training Loss: 0.3781, Test Loss: 0.9815
Epoch: 211, Training Loss: 0.3773, Test Loss: 0.9934
Epoch: 212, Training Loss: 0.3755, Test Loss: 0.9916
Epoch: 213, Training Loss: 0.3722, Test Loss: 0.9738

 86%|███████████████████████████████████████████████████████████████████████████████████▊             | 216/250 [00:17<00:02, 11.87it/s, loss_test=0.980]
Epoch: 215, Training Loss: 0.3707, Test Loss: 0.9801
Epoch: 216, Training Loss: 0.3683, Test Loss: 0.9912
Epoch: 217, Training Loss: 0.3664, Test Loss: 0.9991
Epoch: 218, Training Loss: 0.3641, Test Loss: 0.9828
Epoch: 219, Training Loss: 0.3634, Test Loss: 0.9869
Epoch: 220, Training Loss: 0.3609, Test Loss: 1.0041
Epoch: 221, Training Loss: 0.3588, Test Loss: 1.0014
Epoch: 222, Training Loss: 0.3587, Test Loss: 0.9974
Epoch: 223, Training Loss: 0.3562, Test Loss: 0.9879
Epoch: 224, Training Loss: 0.3521, Test Loss: 0.9938
Epoch: 225, Training Loss: 0.3515, Test Loss: 0.9842
Epoch: 226, Training Loss: 0.3504, Test Loss: 1.0024
Epoch: 227, Training Loss: 0.3462, Test Loss: 0.9903
Epoch: 228, Training Loss: 0.3455, Test Loss: 0.9963
Epoch: 229, Training Loss: 0.3452, Test Loss: 1.0085
Epoch: 230, Training Loss: 0.3440, Test Loss: 0.9999
Epoch: 231, Training Loss: 0.3405, Test Loss: 1.0069
Epoch: 232, Training Loss: 0.3392, Test Loss: 0.9989
Epoch: 233, Training Loss: 0.3371, Test Loss: 0.9999
Epoch: 234, Training Loss: 0.3366, Test Loss: 1.0009
Epoch: 235, Training Loss: 0.3355, Test Loss: 1.0073
Epoch: 236, Training Loss: 0.3333, Test Loss: 1.0150
Epoch: 237, Training Loss: 0.3312, Test Loss: 1.0027
Epoch: 238, Training Loss: 0.3287, Test Loss: 1.0204


100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.18it/s, loss_test=1.010]
Epoch: 240, Training Loss: 0.3257, Test Loss: 1.0182
Epoch: 241, Training Loss: 0.3252, Test Loss: 1.0028
Epoch: 242, Training Loss: 0.3239, Test Loss: 1.0088
Epoch: 243, Training Loss: 0.3201, Test Loss: 1.0224
Epoch: 244, Training Loss: 0.3193, Test Loss: 1.0154
Epoch: 245, Training Loss: 0.3188, Test Loss: 1.0155
Epoch: 246, Training Loss: 0.3168, Test Loss: 1.0102
Epoch: 247, Training Loss: 0.3144, Test Loss: 1.0284
Epoch: 248, Training Loss: 0.3136, Test Loss: 1.0209
Epoch: 249, Training Loss: 0.3111, Test Loss: 1.0103
Model saved as model_1109752np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-12130000-1109752np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9896193742752075, 0.996717894077301, 0.9945029735565185, 0.9967800736427307, 0.9908692002296448, 0.9934209942817688, 0.991010332107544, 0.9877650141716003, 0.9881778001785279, 0.9897667407989502, 0.9870889544486999, 0.9861154556274414, 0.9860391259193421, 0.9907789468765259, 0.9840325713157654, 0.9854122400283813, 0.9904749751091003, 0.9884136080741882, 0.9862399935722351, 0.9870721220970153, 0.9847943782806396, 0.9873684763908386, 0.9806553721427917, 0.9803940773010253, 0.9810783267021179, 0.9774363279342652, 0.9805144667625427, 0.9818402767181397, 0.9767329692840576, 0.9728549599647522, 0.9682018399238587, 0.9622604727745057, 0.9518552780151367, 0.9425792217254638, 0.9324768900871276, 0.9251980900764465, 0.9199452996253967, 0.9146820664405823, 0.9110790252685547, 0.9042694926261902, 0.9029567122459412, 0.8992322206497192, 0.8962244868278504, 0.8903765082359314, 0.8863943696022034, 0.885964548587799, 0.879591190814972, 0.8741985321044922, 0.8729183912277222, 0.8679775357246399, 0.8640711307525635, 0.8603237509727478, 0.8564922094345093, 0.8514885425567627, 0.8514567255973816, 0.8418022274971009, 0.8437049865722657, 0.8373340010643006, 0.8326851963996887, 0.8288107633590698, 0.8222856402397156, 0.8178362011909485, 0.8169909954071045, 0.8102429509162903, 0.8084739446640015, 0.8039884328842163, 0.7972724437713623, 0.7938409686088562, 0.7918125867843628, 0.7884197473526001, 0.783066201210022, 0.7780566334724426, 0.7771957039833068, 0.7706380486488342, 0.7678147315979004, 0.7646021962165832, 0.7593353390693665, 0.7573299765586853, 0.754166579246521, 0.7500494480133056, 0.7461345076560975, 0.7423574090003967, 0.73790602684021, 0.7312908172607422, 0.7301967978477478, 0.7294758796691895, 0.7223381161689758, 0.7187041282653809, 0.7158069014549255, 0.7106967329978943, 0.7098411083221435, 0.7021180987358093, 0.6974796175956726, 0.6962755441665649, 0.6899784088134766, 0.6890253186225891, 0.6858737468719482, 0.679805600643158, 0.6777332425117493, 0.6721139669418335, 0.6717130780220032, 0.665878689289093, 0.6624297142028809, 0.6577551484107971, 0.6560208559036255, 0.6525813341140747, 0.6506303548812866, 0.6455347537994385, 0.6432860612869262, 0.6396039843559265, 0.6336000204086304, 0.630450177192688, 0.6287501335144043, 0.6243984937667847, 0.6224063634872437, 0.6208405137062073, 0.6164279818534851, 0.6131240725517273, 0.6118028163909912, 0.606215500831604, 0.6044448375701904, 0.59976247549057, 0.597441577911377, 0.5926982760429382, 0.5905881881713867, 0.5879584789276123, 0.5851284742355347, 0.5835868835449218, 0.5791345477104187, 0.577435827255249, 0.5741614818572998, 0.571206521987915, 0.5671618819236756, 0.5640987634658814, 0.5617336630821228, 0.5579921841621399, 0.5564890265464782, 0.5509170651435852, 0.550120735168457, 0.5461997747421264, 0.5414039492607117, 0.5412722229957581, 0.5377890110015869, 0.534504234790802, 0.5331058025360107, 0.5310891032218933, 0.52780100107193, 0.527227783203125, 0.5237053275108338, 0.5196854293346405, 0.516424036026001, 0.5126718163490296, 0.5099657535552978, 0.5089665174484252, 0.5057455778121949, 0.5054599046707153, 0.5022438943386078, 0.49810728430747986, 0.4954197585582733, 0.49523773193359377, 0.49018285274505613, 0.4883820712566376, 0.4852141559123993, 0.48405868411064146, 0.4804651439189911, 0.47896204590797425, 0.4780445396900177, 0.47304638028144835, 0.47072869539260864, 0.4685753345489502, 0.46717352867126466, 0.46328848600387573, 0.46327237486839296, 0.45876931548118594, 0.45636852383613585, 0.45538089871406556, 0.4524815618991852, 0.44959765672683716, 0.44791550040245054, 0.4442071974277496, 0.44247175455093385, 0.4404072940349579, 0.43860440254211425, 0.434349513053894, 0.4337026238441467, 0.43153700828552244, 0.4317531645298004, 0.42807350754737855, 0.42510714530944826, 0.4221470057964325, 0.41953442692756654, 0.41819533705711365, 0.41753876209259033, 0.415857720375061, 0.41092670559883115, 0.410520201921463, 0.4079430878162384, 0.4057230830192566, 0.40493024587631227, 0.4019789218902588, 0.4007952570915222, 0.3985604107379913, 0.39629400968551637, 0.39306109547615053, 0.39134552478790285, 0.3909733176231384, 0.38942626118659973, 0.3865480124950409, 0.38221083879470824, 0.38140118718147276, 0.3781326115131378, 0.37730408310890196, 0.37547165155410767, 0.3722204267978668, 0.3709401547908783, 0.3707274794578552, 0.3683491230010986, 0.36637129783630373, 0.36412320733070375, 0.36338031888008115, 0.3609302520751953, 0.35883480310440063, 0.3586929202079773, 0.3562296152114868, 0.3521350920200348, 0.35151092410087587, 0.35037180185317995, 0.3461784303188324, 0.3455223202705383, 0.34523230195045473, 0.34398636817932127, 0.34052449464797974, 0.3391767740249634, 0.337139892578125, 0.33656412959098814, 0.33553757667541506, 0.33331936597824097, 0.3312247633934021, 0.3287070274353027, 0.32784347534179686, 0.32568944096565244, 0.32520453333854676, 0.32391286492347715, 0.32010107636451723, 0.31933985352516175, 0.3187882602214813, 0.31675344705581665, 0.314350825548172, 0.3136132061481476, 0.3110592305660248], 'loss_test': [1.0889997482299805, 1.0696672201156616, 1.0814441442489624, 1.0803245306015015, 1.0867213010787964, 1.082865595817566, 1.0801223516464233, 1.0911309719085693, 1.0881237983703613, 1.0733163356781006, 1.0768760442733765, 1.0796669721603394, 1.08830988407135, 1.08620285987854, 1.0694655179977417, 1.0808436870574951, 1.0822546482086182, 1.0792845487594604, 1.0898733139038086, 1.0891659259796143, 1.0745494365692139, 1.0718541145324707, 1.082771897315979, 1.0912121534347534, 1.0747720003128052, 1.083953857421875, 1.071370244026184, 1.0841273069381714, 1.0947165489196777, 1.0709158182144165, 1.0733838081359863, 1.0587574243545532, 1.0469661951065063, 1.0350085496902466, 1.0325555801391602, 1.0237849950790405, 1.0225427150726318, 1.0028927326202393, 1.0196706056594849, 1.00225031375885, 0.9925456047058105, 0.9916754364967346, 0.9978244304656982, 0.9808300733566284, 0.9674457311630249, 0.9821534156799316, 0.9849352836608887, 0.981459379196167, 0.9794822335243225, 0.9624940156936646, 0.9714774489402771, 0.93801349401474, 0.9691845774650574, 0.9542535543441772, 0.9597687125205994, 0.9372882843017578, 0.9287672638893127, 0.9341466426849365, 0.937664806842804, 0.9267854690551758, 0.932296633720398, 0.9336710572242737, 0.9269989132881165, 0.9055975675582886, 0.9194965958595276, 0.9171407222747803, 0.923933207988739, 0.9111974239349365, 0.9055534601211548, 0.908158540725708, 0.9105913043022156, 0.9042009115219116, 0.8959138989448547, 0.8880066275596619, 0.8963168859481812, 0.8999460935592651, 0.896896243095398, 0.89407879114151, 0.8943120241165161, 0.886430561542511, 0.8848318457603455, 0.9007558226585388, 0.8896737098693848, 0.8926228880882263, 0.8828293681144714, 0.8822294473648071, 0.8796864748001099, 0.8919069170951843, 0.8745027780532837, 0.8876305818557739, 0.8891032338142395, 0.8732225298881531, 0.8957709670066833, 0.8875818252563477, 0.8739466667175293, 0.8898254632949829, 0.8753840327262878, 0.8958466053009033, 0.8724655508995056, 0.8871980905532837, 0.9006425738334656, 0.8965892791748047, 0.8923033475875854, 0.8901904821395874, 0.8915232419967651, 0.8936742544174194, 0.8935219049453735, 0.8906409740447998, 0.9059792757034302, 0.9058000445365906, 0.891909122467041, 0.8969137668609619, 0.8930873870849609, 0.8954053521156311, 0.9058420062065125, 0.9064226150512695, 0.8996214270591736, 0.9022316932678223, 0.9158394932746887, 0.9102089405059814, 0.8993948698043823, 0.9054931998252869, 0.9061672687530518, 0.9002441763877869, 0.9095046520233154, 0.9156636595726013, 0.9202589392662048, 0.9184824228286743, 0.9135809540748596, 0.9152169227600098, 0.9149183630943298, 0.9220430254936218, 0.9030100703239441, 0.9088372588157654, 0.9110285639762878, 0.9095669388771057, 0.9249368906021118, 0.922244131565094, 0.9206321239471436, 0.9117382764816284, 0.9121725559234619, 0.9300928711891174, 0.913588285446167, 0.9245905876159668, 0.9189243316650391, 0.9141829013824463, 0.9270234107971191, 0.9158435463905334, 0.9373739957809448, 0.9215635657310486, 0.9311364889144897, 0.9270910024642944, 0.9293387532234192, 0.9321011304855347, 0.9312422275543213, 0.9398776888847351, 0.9432111978530884, 0.9467890858650208, 0.9292628765106201, 0.9466328024864197, 0.9378276467323303, 0.9346528053283691, 0.9423543810844421, 0.9438685178756714, 0.951368510723114, 0.9414562582969666, 0.9483529329299927, 0.9473410844802856, 0.949396550655365, 0.9515525102615356, 0.9463882446289062, 0.9678753614425659, 0.9417151808738708, 0.9494208693504333, 0.9505462050437927, 0.9514283537864685, 0.9557439684867859, 0.9390647411346436, 0.9551168084144592, 0.9621390700340271, 0.9716635942459106, 0.9528473019599915, 0.9557315707206726, 0.9682300090789795, 0.967170238494873, 0.9701038002967834, 0.9530681371688843, 0.9787004590034485, 0.9730111360549927, 0.9693692922592163, 0.9796924591064453, 0.978003740310669, 0.9715105891227722, 0.980087161064148, 0.9690338373184204, 0.9843859672546387, 0.9529674053192139, 0.9763690829277039, 0.9780056476593018, 0.9718508720397949, 0.9840762615203857, 0.9765267372131348, 0.9889101982116699, 0.9990090727806091, 0.9921673536300659, 0.9945786595344543, 0.9819210767745972, 0.97707200050354, 0.9792664051055908, 0.9928467273712158, 0.9814843535423279, 0.9933868646621704, 0.9915640950202942, 0.9738054275512695, 0.988814651966095, 0.9800869822502136, 0.9911612868309021, 0.9991191625595093, 0.9828128814697266, 0.9869316220283508, 1.0041377544403076, 1.001418113708496, 0.9974479079246521, 0.9879449605941772, 0.9937924146652222, 0.9842424988746643, 1.002402663230896, 0.9902993440628052, 0.9962669610977173, 1.0084855556488037, 0.999941349029541, 1.0068953037261963, 0.9989385008811951, 0.9999039173126221, 1.0008749961853027, 1.0072658061981201, 1.0150026082992554, 1.0027018785476685, 1.0204379558563232, 1.0285344123840332, 1.018179178237915, 1.0028046369552612, 1.008751630783081, 1.0223995447158813, 1.0154458284378052, 1.015529751777649, 1.0102483034133911, 1.0284396409988403, 1.0209381580352783, 1.0103398561477661], 'identifier': '1109752np'}