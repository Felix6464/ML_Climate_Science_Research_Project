
  8%|███████▊                                                                                          | 20/250 [00:01<00:19, 11.69it/s, loss_test=1.073]
Epoch: 00, Training Loss: 0.9910, Test Loss: 1.0868
Epoch: 01, Training Loss: 0.9950, Test Loss: 1.0978
Epoch: 02, Training Loss: 0.9956, Test Loss: 1.0893
Epoch: 03, Training Loss: 0.9890, Test Loss: 1.0708
Epoch: 04, Training Loss: 0.9970, Test Loss: 1.0902
Epoch: 05, Training Loss: 0.9900, Test Loss: 1.0829
Epoch: 06, Training Loss: 0.9898, Test Loss: 1.0635
Epoch: 07, Training Loss: 0.9915, Test Loss: 1.0771
Epoch: 08, Training Loss: 0.9877, Test Loss: 1.1013
Epoch: 09, Training Loss: 0.9930, Test Loss: 1.0657
Epoch: 10, Training Loss: 0.9883, Test Loss: 1.0801
Epoch: 11, Training Loss: 0.9858, Test Loss: 1.0821
Epoch: 12, Training Loss: 0.9887, Test Loss: 1.0686
Epoch: 13, Training Loss: 0.9856, Test Loss: 1.0985
Epoch: 14, Training Loss: 0.9880, Test Loss: 1.0822
Epoch: 15, Training Loss: 0.9840, Test Loss: 1.0947
Epoch: 16, Training Loss: 0.9904, Test Loss: 1.0731
Epoch: 17, Training Loss: 0.9866, Test Loss: 1.0866
Epoch: 18, Training Loss: 0.9869, Test Loss: 1.0699
Epoch: 19, Training Loss: 0.9893, Test Loss: 1.0810

 18%|█████████████████▏                                                                                | 44/250 [00:03<00:16, 12.53it/s, loss_test=1.019]
Epoch: 21, Training Loss: 0.9844, Test Loss: 1.0942
Epoch: 22, Training Loss: 0.9865, Test Loss: 1.0940
Epoch: 23, Training Loss: 0.9854, Test Loss: 1.0843
Epoch: 24, Training Loss: 0.9757, Test Loss: 1.0763
Epoch: 25, Training Loss: 0.9807, Test Loss: 1.0797
Epoch: 26, Training Loss: 0.9804, Test Loss: 1.0777
Epoch: 27, Training Loss: 0.9760, Test Loss: 1.0782
Epoch: 28, Training Loss: 0.9725, Test Loss: 1.0821
Epoch: 29, Training Loss: 0.9677, Test Loss: 1.0766
Epoch: 30, Training Loss: 0.9658, Test Loss: 1.0610
Epoch: 31, Training Loss: 0.9638, Test Loss: 1.0650
Epoch: 32, Training Loss: 0.9551, Test Loss: 1.0742
Epoch: 33, Training Loss: 0.9551, Test Loss: 1.0404
Epoch: 34, Training Loss: 0.9500, Test Loss: 1.0457
Epoch: 35, Training Loss: 0.9503, Test Loss: 1.0416
Epoch: 36, Training Loss: 0.9459, Test Loss: 1.0456
Epoch: 37, Training Loss: 0.9383, Test Loss: 1.0579
Epoch: 38, Training Loss: 0.9383, Test Loss: 1.0339
Epoch: 39, Training Loss: 0.9331, Test Loss: 1.0288
Epoch: 40, Training Loss: 0.9285, Test Loss: 1.0344
Epoch: 41, Training Loss: 0.9289, Test Loss: 1.0274
Epoch: 42, Training Loss: 0.9186, Test Loss: 1.0212
Epoch: 43, Training Loss: 0.9166, Test Loss: 1.0248

 28%|███████████████████████████▍                                                                      | 70/250 [00:05<00:14, 12.35it/s, loss_test=0.944]
Epoch: 45, Training Loss: 0.9045, Test Loss: 1.0208
Epoch: 46, Training Loss: 0.9026, Test Loss: 1.0023
Epoch: 47, Training Loss: 0.8938, Test Loss: 1.0039
Epoch: 48, Training Loss: 0.8944, Test Loss: 0.9829
Epoch: 49, Training Loss: 0.8866, Test Loss: 0.9839
Epoch: 50, Training Loss: 0.8871, Test Loss: 0.9795
Epoch: 51, Training Loss: 0.8829, Test Loss: 0.9913
Epoch: 52, Training Loss: 0.8807, Test Loss: 0.9777
Epoch: 53, Training Loss: 0.8739, Test Loss: 0.9852
Epoch: 54, Training Loss: 0.8706, Test Loss: 0.9692
Epoch: 55, Training Loss: 0.8645, Test Loss: 0.9744
Epoch: 56, Training Loss: 0.8623, Test Loss: 0.9645
Epoch: 57, Training Loss: 0.8585, Test Loss: 0.9736
Epoch: 58, Training Loss: 0.8567, Test Loss: 0.9643
Epoch: 59, Training Loss: 0.8527, Test Loss: 0.9661
Epoch: 60, Training Loss: 0.8484, Test Loss: 0.9659
Epoch: 61, Training Loss: 0.8429, Test Loss: 0.9603
Epoch: 62, Training Loss: 0.8384, Test Loss: 0.9565
Epoch: 63, Training Loss: 0.8362, Test Loss: 0.9648
Epoch: 64, Training Loss: 0.8315, Test Loss: 0.9499
Epoch: 65, Training Loss: 0.8265, Test Loss: 0.9429
Epoch: 66, Training Loss: 0.8276, Test Loss: 0.9531
Epoch: 67, Training Loss: 0.8243, Test Loss: 0.9452
Epoch: 68, Training Loss: 0.8195, Test Loss: 0.9447

 38%|████████████████████████████████████▊                                                             | 94/250 [00:07<00:13, 11.99it/s, loss_test=0.904]
Epoch: 70, Training Loss: 0.8118, Test Loss: 0.9563
Epoch: 71, Training Loss: 0.8084, Test Loss: 0.9467
Epoch: 72, Training Loss: 0.8025, Test Loss: 0.9378
Epoch: 73, Training Loss: 0.8029, Test Loss: 0.9290
Epoch: 74, Training Loss: 0.7965, Test Loss: 0.9252
Epoch: 75, Training Loss: 0.7933, Test Loss: 0.9294
Epoch: 76, Training Loss: 0.7900, Test Loss: 0.9328
Epoch: 77, Training Loss: 0.7887, Test Loss: 0.9310
Epoch: 78, Training Loss: 0.7818, Test Loss: 0.9248
Epoch: 79, Training Loss: 0.7822, Test Loss: 0.9205
Epoch: 80, Training Loss: 0.7750, Test Loss: 0.9288
Epoch: 81, Training Loss: 0.7739, Test Loss: 0.9272
Epoch: 82, Training Loss: 0.7721, Test Loss: 0.9193
Epoch: 83, Training Loss: 0.7657, Test Loss: 0.9245
Epoch: 84, Training Loss: 0.7640, Test Loss: 0.9120
Epoch: 85, Training Loss: 0.7599, Test Loss: 0.9110
Epoch: 86, Training Loss: 0.7554, Test Loss: 0.9140
Epoch: 87, Training Loss: 0.7504, Test Loss: 0.9151
Epoch: 88, Training Loss: 0.7468, Test Loss: 0.9094
Epoch: 89, Training Loss: 0.7460, Test Loss: 0.9080
Epoch: 90, Training Loss: 0.7428, Test Loss: 0.9030
Epoch: 91, Training Loss: 0.7414, Test Loss: 0.9022
Epoch: 92, Training Loss: 0.7348, Test Loss: 0.9026

 47%|█████████████████████████████████████████████▊                                                   | 118/250 [00:09<00:10, 12.35it/s, loss_test=0.890]
Epoch: 94, Training Loss: 0.7285, Test Loss: 0.9004
Epoch: 95, Training Loss: 0.7256, Test Loss: 0.8956
Epoch: 96, Training Loss: 0.7182, Test Loss: 0.9004
Epoch: 97, Training Loss: 0.7175, Test Loss: 0.8966
Epoch: 98, Training Loss: 0.7126, Test Loss: 0.8885
Epoch: 99, Training Loss: 0.7075, Test Loss: 0.9037
Epoch: 100, Training Loss: 0.7071, Test Loss: 0.9049
Epoch: 101, Training Loss: 0.7018, Test Loss: 0.8833
Epoch: 102, Training Loss: 0.6985, Test Loss: 0.8965
Epoch: 103, Training Loss: 0.6891, Test Loss: 0.8927
Epoch: 104, Training Loss: 0.6875, Test Loss: 0.9034
Epoch: 105, Training Loss: 0.6833, Test Loss: 0.8857
Epoch: 106, Training Loss: 0.6757, Test Loss: 0.8881
Epoch: 107, Training Loss: 0.6734, Test Loss: 0.8906
Epoch: 108, Training Loss: 0.6729, Test Loss: 0.8868
Epoch: 109, Training Loss: 0.6660, Test Loss: 0.8929
Epoch: 110, Training Loss: 0.6581, Test Loss: 0.8917
Epoch: 111, Training Loss: 0.6557, Test Loss: 0.8960
Epoch: 112, Training Loss: 0.6497, Test Loss: 0.8946
Epoch: 113, Training Loss: 0.6469, Test Loss: 0.8833
Epoch: 114, Training Loss: 0.6435, Test Loss: 0.8963
Epoch: 115, Training Loss: 0.6388, Test Loss: 0.8832
Epoch: 116, Training Loss: 0.6327, Test Loss: 0.8907

 57%|███████████████████████████████████████████████████████                                          | 142/250 [00:11<00:08, 12.01it/s, loss_test=0.893]
Epoch: 118, Training Loss: 0.6269, Test Loss: 0.8902
Epoch: 119, Training Loss: 0.6196, Test Loss: 0.8959
Epoch: 120, Training Loss: 0.6166, Test Loss: 0.8879
Epoch: 121, Training Loss: 0.6117, Test Loss: 0.8875
Epoch: 122, Training Loss: 0.6103, Test Loss: 0.9028
Epoch: 123, Training Loss: 0.6037, Test Loss: 0.8901
Epoch: 124, Training Loss: 0.6000, Test Loss: 0.8873
Epoch: 125, Training Loss: 0.5984, Test Loss: 0.8942
Epoch: 126, Training Loss: 0.5918, Test Loss: 0.8923
Epoch: 127, Training Loss: 0.5882, Test Loss: 0.8906
Epoch: 128, Training Loss: 0.5833, Test Loss: 0.8851
Epoch: 129, Training Loss: 0.5820, Test Loss: 0.8892
Epoch: 130, Training Loss: 0.5755, Test Loss: 0.8873
Epoch: 131, Training Loss: 0.5727, Test Loss: 0.8928
Epoch: 132, Training Loss: 0.5707, Test Loss: 0.9083
Epoch: 133, Training Loss: 0.5663, Test Loss: 0.9019
Epoch: 134, Training Loss: 0.5627, Test Loss: 0.8884
Epoch: 135, Training Loss: 0.5594, Test Loss: 0.9051
Epoch: 136, Training Loss: 0.5573, Test Loss: 0.8885
Epoch: 137, Training Loss: 0.5544, Test Loss: 0.9035
Epoch: 138, Training Loss: 0.5499, Test Loss: 0.8951
Epoch: 139, Training Loss: 0.5463, Test Loss: 0.8972
Epoch: 140, Training Loss: 0.5450, Test Loss: 0.9035

 67%|█████████████████████████████████████████████████████████████████▏                               | 168/250 [00:13<00:06, 12.35it/s, loss_test=0.914]
Epoch: 142, Training Loss: 0.5369, Test Loss: 0.8931
Epoch: 143, Training Loss: 0.5339, Test Loss: 0.8912
Epoch: 144, Training Loss: 0.5289, Test Loss: 0.9078
Epoch: 145, Training Loss: 0.5290, Test Loss: 0.9076
Epoch: 146, Training Loss: 0.5241, Test Loss: 0.9194
Epoch: 147, Training Loss: 0.5210, Test Loss: 0.9071
Epoch: 148, Training Loss: 0.5191, Test Loss: 0.8993
Epoch: 149, Training Loss: 0.5148, Test Loss: 0.9020
Epoch: 150, Training Loss: 0.5123, Test Loss: 0.9085
Epoch: 151, Training Loss: 0.5094, Test Loss: 0.9078
Epoch: 152, Training Loss: 0.5079, Test Loss: 0.9091
Epoch: 153, Training Loss: 0.5048, Test Loss: 0.9120
Epoch: 154, Training Loss: 0.5032, Test Loss: 0.9091
Epoch: 155, Training Loss: 0.4991, Test Loss: 0.9092
Epoch: 156, Training Loss: 0.4970, Test Loss: 0.9098
Epoch: 157, Training Loss: 0.4933, Test Loss: 0.9153
Epoch: 158, Training Loss: 0.4919, Test Loss: 0.9045
Epoch: 159, Training Loss: 0.4874, Test Loss: 0.9119
Epoch: 160, Training Loss: 0.4864, Test Loss: 0.9165
Epoch: 161, Training Loss: 0.4827, Test Loss: 0.9193
Epoch: 162, Training Loss: 0.4814, Test Loss: 0.9054
Epoch: 163, Training Loss: 0.4793, Test Loss: 0.9194
Epoch: 164, Training Loss: 0.4743, Test Loss: 0.9214
Epoch: 165, Training Loss: 0.4734, Test Loss: 0.9126

 77%|██████████████████████████████████████████████████████████████████████████▍                      | 192/250 [00:15<00:04, 11.99it/s, loss_test=0.926]
Epoch: 167, Training Loss: 0.4683, Test Loss: 0.9138
Epoch: 168, Training Loss: 0.4667, Test Loss: 0.9199
Epoch: 169, Training Loss: 0.4636, Test Loss: 0.9312
Epoch: 170, Training Loss: 0.4611, Test Loss: 0.9205
Epoch: 171, Training Loss: 0.4582, Test Loss: 0.9202
Epoch: 172, Training Loss: 0.4559, Test Loss: 0.9057
Epoch: 173, Training Loss: 0.4554, Test Loss: 0.9255
Epoch: 174, Training Loss: 0.4517, Test Loss: 0.9248
Epoch: 175, Training Loss: 0.4489, Test Loss: 0.9130
Epoch: 176, Training Loss: 0.4461, Test Loss: 0.9270
Epoch: 177, Training Loss: 0.4447, Test Loss: 0.9270
Epoch: 178, Training Loss: 0.4425, Test Loss: 0.9379
Epoch: 179, Training Loss: 0.4404, Test Loss: 0.9270
Epoch: 180, Training Loss: 0.4372, Test Loss: 0.9227
Epoch: 181, Training Loss: 0.4354, Test Loss: 0.9313
Epoch: 182, Training Loss: 0.4336, Test Loss: 0.9359
Epoch: 183, Training Loss: 0.4286, Test Loss: 0.9282
Epoch: 184, Training Loss: 0.4288, Test Loss: 0.9223
Epoch: 185, Training Loss: 0.4282, Test Loss: 0.9342
Epoch: 186, Training Loss: 0.4244, Test Loss: 0.9212
Epoch: 187, Training Loss: 0.4213, Test Loss: 0.9318
Epoch: 188, Training Loss: 0.4205, Test Loss: 0.9333
Epoch: 189, Training Loss: 0.4184, Test Loss: 0.9254
Epoch: 190, Training Loss: 0.4163, Test Loss: 0.9281

 86%|███████████████████████████████████████████████████████████████████████████████████▊             | 216/250 [00:17<00:02, 11.90it/s, loss_test=0.947]
Epoch: 192, Training Loss: 0.4117, Test Loss: 0.9260
Epoch: 193, Training Loss: 0.4089, Test Loss: 0.9317
Epoch: 194, Training Loss: 0.4099, Test Loss: 0.9301
Epoch: 195, Training Loss: 0.4054, Test Loss: 0.9477
Epoch: 196, Training Loss: 0.4042, Test Loss: 0.9261
Epoch: 197, Training Loss: 0.4020, Test Loss: 0.9276
Epoch: 198, Training Loss: 0.3995, Test Loss: 0.9413
Epoch: 199, Training Loss: 0.3984, Test Loss: 0.9299
Epoch: 200, Training Loss: 0.3964, Test Loss: 0.9435
Epoch: 201, Training Loss: 0.3938, Test Loss: 0.9508
Epoch: 202, Training Loss: 0.3913, Test Loss: 0.9543
Epoch: 203, Training Loss: 0.3891, Test Loss: 0.9354
Epoch: 204, Training Loss: 0.3879, Test Loss: 0.9435
Epoch: 205, Training Loss: 0.3867, Test Loss: 0.9428
Epoch: 206, Training Loss: 0.3853, Test Loss: 0.9344
Epoch: 207, Training Loss: 0.3825, Test Loss: 0.9451
Epoch: 208, Training Loss: 0.3826, Test Loss: 0.9323
Epoch: 209, Training Loss: 0.3780, Test Loss: 0.9400
Epoch: 210, Training Loss: 0.3781, Test Loss: 0.9425
Epoch: 211, Training Loss: 0.3757, Test Loss: 0.9420
Epoch: 212, Training Loss: 0.3748, Test Loss: 0.9562
Epoch: 213, Training Loss: 0.3720, Test Loss: 0.9496
Epoch: 214, Training Loss: 0.3703, Test Loss: 0.9548

 96%|█████████████████████████████████████████████████████████████████████████████████████████████    | 240/250 [00:19<00:00, 11.89it/s, loss_test=0.972]
Epoch: 216, Training Loss: 0.3676, Test Loss: 0.9472
Epoch: 217, Training Loss: 0.3637, Test Loss: 0.9577
Epoch: 218, Training Loss: 0.3623, Test Loss: 0.9542
Epoch: 219, Training Loss: 0.3613, Test Loss: 0.9508
Epoch: 220, Training Loss: 0.3581, Test Loss: 0.9489
Epoch: 221, Training Loss: 0.3579, Test Loss: 0.9652
Epoch: 222, Training Loss: 0.3558, Test Loss: 0.9501
Epoch: 223, Training Loss: 0.3544, Test Loss: 0.9573
Epoch: 224, Training Loss: 0.3539, Test Loss: 0.9635
Epoch: 225, Training Loss: 0.3504, Test Loss: 0.9525
Epoch: 226, Training Loss: 0.3493, Test Loss: 0.9643
Epoch: 227, Training Loss: 0.3482, Test Loss: 0.9411
Epoch: 228, Training Loss: 0.3454, Test Loss: 0.9649
Epoch: 229, Training Loss: 0.3438, Test Loss: 0.9515
Epoch: 230, Training Loss: 0.3433, Test Loss: 0.9577
Epoch: 231, Training Loss: 0.3409, Test Loss: 0.9580
Epoch: 232, Training Loss: 0.3400, Test Loss: 0.9588
Epoch: 233, Training Loss: 0.3385, Test Loss: 0.9658
Epoch: 234, Training Loss: 0.3357, Test Loss: 0.9681
Epoch: 235, Training Loss: 0.3340, Test Loss: 0.9487
Epoch: 236, Training Loss: 0.3334, Test Loss: 0.9634
Epoch: 237, Training Loss: 0.3316, Test Loss: 0.9724
Epoch: 238, Training Loss: 0.3296, Test Loss: 0.9698

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.13it/s, loss_test=0.990]
Epoch: 240, Training Loss: 0.3256, Test Loss: 0.9721
Epoch: 241, Training Loss: 0.3258, Test Loss: 0.9667
Epoch: 242, Training Loss: 0.3231, Test Loss: 0.9666
Epoch: 243, Training Loss: 0.3231, Test Loss: 0.9769
Epoch: 244, Training Loss: 0.3202, Test Loss: 0.9676
Epoch: 245, Training Loss: 0.3186, Test Loss: 0.9843
Epoch: 246, Training Loss: 0.3178, Test Loss: 0.9813
Epoch: 247, Training Loss: 0.3165, Test Loss: 0.9777
Epoch: 248, Training Loss: 0.3147, Test Loss: 0.9685
Epoch: 249, Training Loss: 0.3139, Test Loss: 0.9901
Model saved as model_8036395np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-12180000-8036395np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9910437226295471, 0.9949594020843506, 0.9956344962120056, 0.989039397239685, 0.997028911113739, 0.9899988174438477, 0.9897642016410828, 0.9915256500244141, 0.9877447247505188, 0.9930233001708985, 0.988327932357788, 0.9857635259628296, 0.9886738419532776, 0.9856205821037293, 0.988026750087738, 0.984027361869812, 0.9904459357261658, 0.9865704536437988, 0.9868789792060852, 0.9892769694328308, 0.9805475354194642, 0.9844250917434693, 0.9864838004112244, 0.9854448437690735, 0.9756553053855896, 0.9807406783103942, 0.9803554296493531, 0.9759552597999572, 0.9725345492362976, 0.9676537036895752, 0.9657679200172424, 0.963775360584259, 0.9550731539726257, 0.9550900340080262, 0.9500011444091797, 0.9503125667572021, 0.9459096312522888, 0.9382980346679688, 0.9383445620536804, 0.9331216096878052, 0.9284870743751525, 0.9288623690605163, 0.9186367392539978, 0.91664137840271, 0.9100602030754089, 0.9044611692428589, 0.9025801420211792, 0.8938450574874878, 0.894361960887909, 0.8865741133689881, 0.8870799303054809, 0.8829484701156616, 0.8807047009468079, 0.8738856792449952, 0.8706025958061219, 0.8644965291023254, 0.8623315811157226, 0.8584572434425354, 0.8566890478134155, 0.8526911973953247, 0.8483653903007508, 0.8429397106170654, 0.838420295715332, 0.8361517071723938, 0.8314548015594483, 0.8264891982078553, 0.8276442170143128, 0.8243345022201538, 0.8195248246192932, 0.8134325861930847, 0.8118074536323547, 0.8083802223205566, 0.8024819254875183, 0.802902626991272, 0.796536660194397, 0.7933257818222046, 0.7900441527366638, 0.7887346029281617, 0.7818162083625794, 0.7822338700294494, 0.7750157356262207, 0.7738963961601257, 0.7721339583396911, 0.7656656384468079, 0.7640148401260376, 0.7598603248596192, 0.755424690246582, 0.7504317760467529, 0.7467586040496826, 0.7460444211959839, 0.7427539229393005, 0.7414125442504883, 0.7347689270973206, 0.730293619632721, 0.7284521460533142, 0.7255628108978271, 0.7182493925094604, 0.7174647688865662, 0.7126148819923401, 0.7075481057167053, 0.7070605516433716, 0.7017751455307006, 0.6984549283981323, 0.6891181468963623, 0.6874937891960144, 0.6832596302032471, 0.6756611227989197, 0.6734132647514344, 0.6728896379470826, 0.6659825325012207, 0.6580607175827027, 0.6557425498962403, 0.6497308373451233, 0.6468676328659058, 0.6434892773628235, 0.6388185024261475, 0.632741117477417, 0.629477322101593, 0.6269140839576721, 0.619644296169281, 0.6165654301643372, 0.6116793036460877, 0.6102703094482422, 0.6036635756492614, 0.5999805569648743, 0.5983508348464965, 0.5918183565139771, 0.5882304549217224, 0.5832677364349366, 0.5820365071296691, 0.575487244129181, 0.5727226614952088, 0.5706697344779968, 0.5663341283798218, 0.5626748442649842, 0.5594157576560974, 0.5572826147079468, 0.5544406652450562, 0.5498924374580383, 0.5462773680686951, 0.545026445388794, 0.5425336480140686, 0.5368884325027465, 0.5338927865028381, 0.5288972735404969, 0.5289521098136902, 0.5240668296813965, 0.52102130651474, 0.5190632104873657, 0.5147658586502075, 0.5123315811157226, 0.5094063401222229, 0.5079368472099304, 0.5048244893550873, 0.5031662523746491, 0.49908403754234315, 0.4970491886138916, 0.49330244660377504, 0.49193713068962097, 0.48735528588294985, 0.48635653257369993, 0.48269842863082885, 0.4814366638660431, 0.47930723428726196, 0.4742771804332733, 0.4733785629272461, 0.4688076972961426, 0.46825828552246096, 0.4666536033153534, 0.4636339008808136, 0.46109970211982726, 0.4582431375980377, 0.455923855304718, 0.4554412305355072, 0.45171072483062746, 0.4488761007785797, 0.44614781737327575, 0.4447487831115723, 0.4424992024898529, 0.44042218327522276, 0.4371586561203003, 0.4353860437870026, 0.4336455464363098, 0.42857702970504763, 0.42876580357551575, 0.42817034125328063, 0.4244313597679138, 0.42129798531532286, 0.42051592469215393, 0.4184076964855194, 0.4163030207157135, 0.4128419578075409, 0.41172994375228883, 0.40888685584068296, 0.4098652064800262, 0.40535485148429873, 0.40416921973228453, 0.4019517540931702, 0.3994919300079346, 0.39836412072181704, 0.3963849425315857, 0.39379056692123415, 0.39129101037979125, 0.38914080858230593, 0.38789366483688353, 0.38666513562202454, 0.38528024554252627, 0.3825203239917755, 0.38263349533081054, 0.37796197533607484, 0.3781218767166138, 0.3757287859916687, 0.37483459115028384, 0.37201237082481386, 0.37025654315948486, 0.3679081439971924, 0.3676394522190094, 0.36373762488365174, 0.36230010390281675, 0.36130678057670595, 0.35806441903114317, 0.35785804986953734, 0.3558094024658203, 0.35442292094230654, 0.35390896201133726, 0.3504430949687958, 0.3493265450000763, 0.3482455611228943, 0.3454188764095306, 0.343813157081604, 0.34326825737953187, 0.34092724323272705, 0.339952278137207, 0.33851823806762693, 0.33569309711456297, 0.3339834094047546, 0.33341735005378725, 0.3316127181053162, 0.3296308875083923, 0.3291268765926361, 0.32563350200653074, 0.32579246163368225, 0.3230684757232666, 0.32309749722480774, 0.3202114701271057, 0.3186037063598633, 0.3177608251571655, 0.3164540469646454, 0.3147208333015442, 0.3139342188835144], 'loss_test': [1.0867741107940674, 1.0978282690048218, 1.089251160621643, 1.0707613229751587, 1.0901843309402466, 1.082902193069458, 1.0635203123092651, 1.0771448612213135, 1.1012941598892212, 1.0656867027282715, 1.0801435708999634, 1.0821313858032227, 1.0685603618621826, 1.0984529256820679, 1.0821919441223145, 1.0946989059448242, 1.0731462240219116, 1.0865918397903442, 1.069876790046692, 1.0809658765792847, 1.0732855796813965, 1.0942096710205078, 1.094046950340271, 1.0842841863632202, 1.0763158798217773, 1.0796501636505127, 1.0776846408843994, 1.0781779289245605, 1.0820949077606201, 1.0766178369522095, 1.0610183477401733, 1.0649938583374023, 1.074188470840454, 1.040360927581787, 1.0457179546356201, 1.0416145324707031, 1.045626163482666, 1.0579097270965576, 1.0338894128799438, 1.0287556648254395, 1.0343836545944214, 1.0274178981781006, 1.0212016105651855, 1.0248128175735474, 1.018907070159912, 1.0207775831222534, 1.002317190170288, 1.0038708448410034, 0.9829251170158386, 0.983889639377594, 0.9794953465461731, 0.9912528395652771, 0.9776911735534668, 0.9851623773574829, 0.9692235589027405, 0.9743682146072388, 0.9645119905471802, 0.9736170768737793, 0.9643259644508362, 0.9661017656326294, 0.9659224152565002, 0.9603329300880432, 0.9565221667289734, 0.9648392200469971, 0.9499132037162781, 0.9429255127906799, 0.953111469745636, 0.9451718926429749, 0.9447440505027771, 0.9436664581298828, 0.9562572836875916, 0.9466660022735596, 0.9377882480621338, 0.9289590120315552, 0.9251756072044373, 0.9294499754905701, 0.9327707886695862, 0.9310182332992554, 0.9247741103172302, 0.9204671382904053, 0.9288249015808105, 0.9271650910377502, 0.9192598462104797, 0.9245373606681824, 0.9120190143585205, 0.910964846611023, 0.9139919281005859, 0.915113627910614, 0.9093679189682007, 0.9079506993293762, 0.90304034948349, 0.902167022228241, 0.9025560617446899, 0.904266357421875, 0.9004356861114502, 0.8956172466278076, 0.9004207253456116, 0.896609365940094, 0.8885031342506409, 0.9036734700202942, 0.9049180746078491, 0.8832762241363525, 0.8965426087379456, 0.8927170038223267, 0.9034070372581482, 0.8857429027557373, 0.8881383538246155, 0.8906368017196655, 0.8868499994277954, 0.8929150104522705, 0.8916838765144348, 0.8960327506065369, 0.8946107625961304, 0.8833469748497009, 0.8962706923484802, 0.8831638097763062, 0.8906530737876892, 0.8957550525665283, 0.8901709914207458, 0.8958930373191833, 0.8878812193870544, 0.887496829032898, 0.9027906060218811, 0.89005047082901, 0.8873312473297119, 0.8941699862480164, 0.8923399448394775, 0.8905761241912842, 0.8851327896118164, 0.8892342448234558, 0.8873410820960999, 0.892805814743042, 0.9082555174827576, 0.9018829464912415, 0.8883780241012573, 0.9051358103752136, 0.8885121941566467, 0.9035308957099915, 0.895115077495575, 0.897153913974762, 0.9035282731056213, 0.9074268341064453, 0.8931494355201721, 0.8912448883056641, 0.9078364372253418, 0.9076223969459534, 0.9193678498268127, 0.9071494340896606, 0.8992674946784973, 0.9019797444343567, 0.908549427986145, 0.9078112244606018, 0.9090735912322998, 0.9119784832000732, 0.909123957157135, 0.9091677665710449, 0.9097554087638855, 0.9152761101722717, 0.9044903516769409, 0.9118828773498535, 0.9165295362472534, 0.9192752838134766, 0.9053502678871155, 0.9193508625030518, 0.9213675260543823, 0.912582278251648, 0.9167432188987732, 0.9138032793998718, 0.9199435114860535, 0.9312061667442322, 0.9205164909362793, 0.9202017188072205, 0.905651867389679, 0.9255489110946655, 0.9248397350311279, 0.9130106568336487, 0.9270182251930237, 0.9270014762878418, 0.9378989338874817, 0.9270480871200562, 0.9226617217063904, 0.9313380122184753, 0.9358794689178467, 0.9281519651412964, 0.9223452210426331, 0.9341877698898315, 0.9211839437484741, 0.9318183064460754, 0.9333479404449463, 0.9253605604171753, 0.9280731678009033, 0.9273800253868103, 0.9259998202323914, 0.931746244430542, 0.9300554394721985, 0.9476737380027771, 0.9260598421096802, 0.9275563955307007, 0.9413228631019592, 0.9298846125602722, 0.9434807896614075, 0.9508371353149414, 0.9542519450187683, 0.9354207515716553, 0.9434641599655151, 0.9428349137306213, 0.9344470500946045, 0.9451200366020203, 0.9323439002037048, 0.9400243163108826, 0.9424811601638794, 0.9420222043991089, 0.9561893939971924, 0.9495846629142761, 0.9547929763793945, 0.9502962827682495, 0.9472354650497437, 0.9577377438545227, 0.9542123079299927, 0.9507607817649841, 0.9488659501075745, 0.9651570916175842, 0.9500643014907837, 0.9573337435722351, 0.9635226726531982, 0.952519953250885, 0.9643248319625854, 0.9410990476608276, 0.964864194393158, 0.9515317678451538, 0.9577029943466187, 0.957997739315033, 0.9587580561637878, 0.9657670855522156, 0.9680534601211548, 0.9486902356147766, 0.9634183645248413, 0.9723586440086365, 0.9698072671890259, 0.9781308174133301, 0.9721179008483887, 0.9666809439659119, 0.9665769934654236, 0.9768736362457275, 0.9676358103752136, 0.9842561483383179, 0.9812819957733154, 0.9776828289031982, 0.9685119986534119, 0.990058422088623], 'identifier': '8036395np'}