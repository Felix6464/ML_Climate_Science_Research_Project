
  8%|███████▊                                                                                          | 20/250 [00:01<00:18, 12.28it/s, loss_test=1.066]
Epoch: 00, Training Loss: 0.9953, Test Loss: 1.0765
Epoch: 01, Training Loss: 0.9937, Test Loss: 1.0823
Epoch: 02, Training Loss: 0.9939, Test Loss: 1.0756
Epoch: 03, Training Loss: 0.9929, Test Loss: 1.0744
Epoch: 04, Training Loss: 0.9943, Test Loss: 1.0704
Epoch: 05, Training Loss: 0.9876, Test Loss: 1.0742
Epoch: 06, Training Loss: 0.9924, Test Loss: 1.0747
Epoch: 07, Training Loss: 0.9871, Test Loss: 1.0924
Epoch: 08, Training Loss: 0.9936, Test Loss: 1.0732
Epoch: 09, Training Loss: 0.9904, Test Loss: 1.0823
Epoch: 10, Training Loss: 0.9892, Test Loss: 1.0712
Epoch: 11, Training Loss: 0.9923, Test Loss: 1.0763
Epoch: 12, Training Loss: 0.9863, Test Loss: 1.0838
Epoch: 13, Training Loss: 0.9839, Test Loss: 1.0737
Epoch: 14, Training Loss: 0.9871, Test Loss: 1.0827
Epoch: 15, Training Loss: 0.9852, Test Loss: 1.0836
Epoch: 16, Training Loss: 0.9856, Test Loss: 1.0733
Epoch: 17, Training Loss: 0.9888, Test Loss: 1.0830
Epoch: 18, Training Loss: 0.9880, Test Loss: 1.0758

 18%|█████████████████▏                                                                                | 44/250 [00:03<00:16, 12.31it/s, loss_test=1.000]
Epoch: 20, Training Loss: 0.9861, Test Loss: 1.0659
Epoch: 21, Training Loss: 0.9803, Test Loss: 1.0796
Epoch: 22, Training Loss: 0.9835, Test Loss: 1.0773
Epoch: 23, Training Loss: 0.9849, Test Loss: 1.0758
Epoch: 24, Training Loss: 0.9800, Test Loss: 1.0675
Epoch: 25, Training Loss: 0.9798, Test Loss: 1.0726
Epoch: 26, Training Loss: 0.9779, Test Loss: 1.0892
Epoch: 27, Training Loss: 0.9738, Test Loss: 1.0689
Epoch: 28, Training Loss: 0.9708, Test Loss: 1.0524
Epoch: 29, Training Loss: 0.9676, Test Loss: 1.0869
Epoch: 30, Training Loss: 0.9578, Test Loss: 1.0807
Epoch: 31, Training Loss: 0.9499, Test Loss: 1.0488
Epoch: 32, Training Loss: 0.9454, Test Loss: 1.0381
Epoch: 33, Training Loss: 0.9369, Test Loss: 1.0458
Epoch: 34, Training Loss: 0.9367, Test Loss: 1.0326
Epoch: 35, Training Loss: 0.9278, Test Loss: 1.0219
Epoch: 36, Training Loss: 0.9249, Test Loss: 1.0282
Epoch: 37, Training Loss: 0.9186, Test Loss: 1.0062
Epoch: 38, Training Loss: 0.9146, Test Loss: 1.0018
Epoch: 39, Training Loss: 0.9109, Test Loss: 1.0165
Epoch: 40, Training Loss: 0.9032, Test Loss: 1.0044
Epoch: 41, Training Loss: 0.9048, Test Loss: 1.0013
Epoch: 42, Training Loss: 0.8984, Test Loss: 1.0248
Epoch: 43, Training Loss: 0.8941, Test Loss: 0.9969

 27%|██████████████████████████▋                                                                       | 68/250 [00:05<00:15, 11.89it/s, loss_test=0.927]
Epoch: 45, Training Loss: 0.8837, Test Loss: 0.9968
Epoch: 46, Training Loss: 0.8826, Test Loss: 1.0015
Epoch: 47, Training Loss: 0.8806, Test Loss: 0.9735
Epoch: 48, Training Loss: 0.8763, Test Loss: 0.9937
Epoch: 49, Training Loss: 0.8717, Test Loss: 0.9918
Epoch: 50, Training Loss: 0.8711, Test Loss: 0.9722
Epoch: 51, Training Loss: 0.8675, Test Loss: 0.9717
Epoch: 52, Training Loss: 0.8665, Test Loss: 0.9658
Epoch: 53, Training Loss: 0.8593, Test Loss: 0.9762
Epoch: 54, Training Loss: 0.8551, Test Loss: 0.9549
Epoch: 55, Training Loss: 0.8526, Test Loss: 0.9636
Epoch: 56, Training Loss: 0.8496, Test Loss: 0.9506
Epoch: 57, Training Loss: 0.8488, Test Loss: 0.9632
Epoch: 58, Training Loss: 0.8442, Test Loss: 0.9757
Epoch: 59, Training Loss: 0.8447, Test Loss: 0.9520
Epoch: 60, Training Loss: 0.8388, Test Loss: 0.9543
Epoch: 61, Training Loss: 0.8352, Test Loss: 0.9330
Epoch: 62, Training Loss: 0.8340, Test Loss: 0.9302
Epoch: 63, Training Loss: 0.8303, Test Loss: 0.9581
Epoch: 64, Training Loss: 0.8250, Test Loss: 0.9468
Epoch: 65, Training Loss: 0.8236, Test Loss: 0.9408
Epoch: 66, Training Loss: 0.8220, Test Loss: 0.9541
Epoch: 67, Training Loss: 0.8194, Test Loss: 0.9428

 38%|████████████████████████████████████▊                                                             | 94/250 [00:07<00:12, 12.03it/s, loss_test=0.892]
Epoch: 69, Training Loss: 0.8147, Test Loss: 0.9272
Epoch: 70, Training Loss: 0.8089, Test Loss: 0.9347
Epoch: 71, Training Loss: 0.8082, Test Loss: 0.9326
Epoch: 72, Training Loss: 0.8023, Test Loss: 0.9430
Epoch: 73, Training Loss: 0.8008, Test Loss: 0.9379
Epoch: 74, Training Loss: 0.7966, Test Loss: 0.9269
Epoch: 75, Training Loss: 0.7929, Test Loss: 0.9320
Epoch: 76, Training Loss: 0.7902, Test Loss: 0.9256
Epoch: 77, Training Loss: 0.7924, Test Loss: 0.9283
Epoch: 78, Training Loss: 0.7858, Test Loss: 0.9239
Epoch: 79, Training Loss: 0.7851, Test Loss: 0.9076
Epoch: 80, Training Loss: 0.7823, Test Loss: 0.9226
Epoch: 81, Training Loss: 0.7771, Test Loss: 0.9136
Epoch: 82, Training Loss: 0.7737, Test Loss: 0.9121
Epoch: 83, Training Loss: 0.7703, Test Loss: 0.9066
Epoch: 84, Training Loss: 0.7660, Test Loss: 0.9026
Epoch: 85, Training Loss: 0.7636, Test Loss: 0.9276
Epoch: 86, Training Loss: 0.7611, Test Loss: 0.9148
Epoch: 87, Training Loss: 0.7604, Test Loss: 0.9162
Epoch: 88, Training Loss: 0.7565, Test Loss: 0.9086
Epoch: 89, Training Loss: 0.7518, Test Loss: 0.9158
Epoch: 90, Training Loss: 0.7454, Test Loss: 0.9041
Epoch: 91, Training Loss: 0.7465, Test Loss: 0.8895
Epoch: 92, Training Loss: 0.7405, Test Loss: 0.9073

 47%|█████████████████████████████████████████████▊                                                   | 118/250 [00:09<00:11, 12.00it/s, loss_test=0.872]
Epoch: 94, Training Loss: 0.7308, Test Loss: 0.8924
Epoch: 95, Training Loss: 0.7336, Test Loss: 0.9097
Epoch: 96, Training Loss: 0.7282, Test Loss: 0.9040
Epoch: 97, Training Loss: 0.7244, Test Loss: 0.8961
Epoch: 98, Training Loss: 0.7222, Test Loss: 0.8896
Epoch: 99, Training Loss: 0.7208, Test Loss: 0.8787
Epoch: 100, Training Loss: 0.7158, Test Loss: 0.8903
Epoch: 101, Training Loss: 0.7130, Test Loss: 0.8785
Epoch: 102, Training Loss: 0.7080, Test Loss: 0.8909
Epoch: 103, Training Loss: 0.7059, Test Loss: 0.8905
Epoch: 104, Training Loss: 0.7031, Test Loss: 0.8987
Epoch: 105, Training Loss: 0.6984, Test Loss: 0.8766
Epoch: 106, Training Loss: 0.6989, Test Loss: 0.8822
Epoch: 107, Training Loss: 0.6962, Test Loss: 0.8734
Epoch: 108, Training Loss: 0.6911, Test Loss: 0.8836
Epoch: 109, Training Loss: 0.6860, Test Loss: 0.8788
Epoch: 110, Training Loss: 0.6842, Test Loss: 0.8783
Epoch: 111, Training Loss: 0.6809, Test Loss: 0.8846
Epoch: 112, Training Loss: 0.6767, Test Loss: 0.8835
Epoch: 113, Training Loss: 0.6713, Test Loss: 0.8835
Epoch: 114, Training Loss: 0.6692, Test Loss: 0.8744
Epoch: 115, Training Loss: 0.6681, Test Loss: 0.8859
Epoch: 116, Training Loss: 0.6642, Test Loss: 0.8837
Epoch: 117, Training Loss: 0.6601, Test Loss: 0.8876

 57%|███████████████████████████████████████████████████████                                          | 142/250 [00:11<00:08, 12.19it/s, loss_test=0.886]
Epoch: 119, Training Loss: 0.6557, Test Loss: 0.8793
Epoch: 120, Training Loss: 0.6511, Test Loss: 0.8726
Epoch: 121, Training Loss: 0.6477, Test Loss: 0.8787
Epoch: 122, Training Loss: 0.6439, Test Loss: 0.8742
Epoch: 123, Training Loss: 0.6399, Test Loss: 0.8803
Epoch: 124, Training Loss: 0.6361, Test Loss: 0.8753
Epoch: 125, Training Loss: 0.6328, Test Loss: 0.8750
Epoch: 126, Training Loss: 0.6305, Test Loss: 0.8730
Epoch: 127, Training Loss: 0.6283, Test Loss: 0.8884
Epoch: 128, Training Loss: 0.6214, Test Loss: 0.8806
Epoch: 129, Training Loss: 0.6232, Test Loss: 0.8743
Epoch: 130, Training Loss: 0.6184, Test Loss: 0.8763
Epoch: 131, Training Loss: 0.6138, Test Loss: 0.8705
Epoch: 132, Training Loss: 0.6118, Test Loss: 0.8809
Epoch: 133, Training Loss: 0.6074, Test Loss: 0.8720
Epoch: 134, Training Loss: 0.6079, Test Loss: 0.8805
Epoch: 135, Training Loss: 0.6021, Test Loss: 0.8738
Epoch: 136, Training Loss: 0.6009, Test Loss: 0.8838
Epoch: 137, Training Loss: 0.5957, Test Loss: 0.8821
Epoch: 138, Training Loss: 0.5933, Test Loss: 0.8671
Epoch: 139, Training Loss: 0.5885, Test Loss: 0.8852
Epoch: 140, Training Loss: 0.5867, Test Loss: 0.8810
Epoch: 141, Training Loss: 0.5834, Test Loss: 0.8669

 67%|█████████████████████████████████████████████████████████████████▏                               | 168/250 [00:13<00:06, 12.23it/s, loss_test=0.903]
Epoch: 143, Training Loss: 0.5783, Test Loss: 0.8855
Epoch: 144, Training Loss: 0.5746, Test Loss: 0.8932
Epoch: 145, Training Loss: 0.5724, Test Loss: 0.8909
Epoch: 146, Training Loss: 0.5673, Test Loss: 0.8875
Epoch: 147, Training Loss: 0.5641, Test Loss: 0.8914
Epoch: 148, Training Loss: 0.5614, Test Loss: 0.8840
Epoch: 149, Training Loss: 0.5595, Test Loss: 0.8789
Epoch: 150, Training Loss: 0.5562, Test Loss: 0.8949
Epoch: 151, Training Loss: 0.5505, Test Loss: 0.8943
Epoch: 152, Training Loss: 0.5480, Test Loss: 0.8929
Epoch: 153, Training Loss: 0.5453, Test Loss: 0.8842
Epoch: 154, Training Loss: 0.5423, Test Loss: 0.8991
Epoch: 155, Training Loss: 0.5400, Test Loss: 0.8887
Epoch: 156, Training Loss: 0.5371, Test Loss: 0.8911
Epoch: 157, Training Loss: 0.5347, Test Loss: 0.9003
Epoch: 158, Training Loss: 0.5312, Test Loss: 0.9031
Epoch: 159, Training Loss: 0.5283, Test Loss: 0.9011
Epoch: 160, Training Loss: 0.5262, Test Loss: 0.9067
Epoch: 161, Training Loss: 0.5232, Test Loss: 0.8933
Epoch: 162, Training Loss: 0.5196, Test Loss: 0.8919
Epoch: 163, Training Loss: 0.5153, Test Loss: 0.8982
Epoch: 164, Training Loss: 0.5142, Test Loss: 0.9111
Epoch: 165, Training Loss: 0.5083, Test Loss: 0.8923
Epoch: 166, Training Loss: 0.5091, Test Loss: 0.9075
Epoch: 167, Training Loss: 0.5049, Test Loss: 0.9031
Epoch: 168, Training Loss: 0.5011, Test Loss: 0.9068
Epoch: 169, Training Loss: 0.5010, Test Loss: 0.9087
Epoch: 170, Training Loss: 0.4969, Test Loss: 0.8994
Epoch: 171, Training Loss: 0.4938, Test Loss: 0.9033
Epoch: 172, Training Loss: 0.4922, Test Loss: 0.9098
Epoch: 173, Training Loss: 0.4885, Test Loss: 0.9092
Epoch: 174, Training Loss: 0.4863, Test Loss: 0.9169
Epoch: 175, Training Loss: 0.4863, Test Loss: 0.9085
Epoch: 176, Training Loss: 0.4821, Test Loss: 0.8995
Epoch: 177, Training Loss: 0.4766, Test Loss: 0.9163
Epoch: 178, Training Loss: 0.4761, Test Loss: 0.9108
Epoch: 179, Training Loss: 0.4721, Test Loss: 0.9006
Epoch: 180, Training Loss: 0.4699, Test Loss: 0.9131
Epoch: 181, Training Loss: 0.4673, Test Loss: 0.9178
Epoch: 182, Training Loss: 0.4668, Test Loss: 0.9236
Epoch: 183, Training Loss: 0.4631, Test Loss: 0.9114
Epoch: 184, Training Loss: 0.4606, Test Loss: 0.9189
Epoch: 185, Training Loss: 0.4580, Test Loss: 0.9283
Epoch: 186, Training Loss: 0.4555, Test Loss: 0.9133
Epoch: 187, Training Loss: 0.4534, Test Loss: 0.9213
Epoch: 188, Training Loss: 0.4510, Test Loss: 0.9288
Epoch: 189, Training Loss: 0.4466, Test Loss: 0.9257
Epoch: 190, Training Loss: 0.4466, Test Loss: 0.9256


 86%|███████████████████████████████████████████████████████████████████████████████████▊             | 216/250 [00:17<00:02, 11.92it/s, loss_test=0.938]
Epoch: 192, Training Loss: 0.4414, Test Loss: 0.9256
Epoch: 193, Training Loss: 0.4394, Test Loss: 0.9249
Epoch: 194, Training Loss: 0.4376, Test Loss: 0.9166
Epoch: 195, Training Loss: 0.4351, Test Loss: 0.9206
Epoch: 196, Training Loss: 0.4323, Test Loss: 0.9182
Epoch: 197, Training Loss: 0.4292, Test Loss: 0.9382
Epoch: 198, Training Loss: 0.4276, Test Loss: 0.9308
Epoch: 199, Training Loss: 0.4253, Test Loss: 0.9338
Epoch: 200, Training Loss: 0.4228, Test Loss: 0.9320
Epoch: 201, Training Loss: 0.4211, Test Loss: 0.9396
Epoch: 202, Training Loss: 0.4180, Test Loss: 0.9163
Epoch: 203, Training Loss: 0.4139, Test Loss: 0.9358
Epoch: 204, Training Loss: 0.4128, Test Loss: 0.9400
Epoch: 205, Training Loss: 0.4117, Test Loss: 0.9453
Epoch: 206, Training Loss: 0.4109, Test Loss: 0.9473
Epoch: 207, Training Loss: 0.4059, Test Loss: 0.9282
Epoch: 208, Training Loss: 0.4036, Test Loss: 0.9382
Epoch: 209, Training Loss: 0.4032, Test Loss: 0.9445
Epoch: 210, Training Loss: 0.4003, Test Loss: 0.9436
Epoch: 211, Training Loss: 0.3995, Test Loss: 0.9436
Epoch: 212, Training Loss: 0.3966, Test Loss: 0.9402
Epoch: 213, Training Loss: 0.3930, Test Loss: 0.9450
Epoch: 214, Training Loss: 0.3915, Test Loss: 0.9415

 97%|█████████████████████████████████████████████████████████████████████████████████████████████▉   | 242/250 [00:19<00:00, 12.44it/s, loss_test=0.962]
Epoch: 216, Training Loss: 0.3876, Test Loss: 0.9379
Epoch: 217, Training Loss: 0.3867, Test Loss: 0.9561
Epoch: 218, Training Loss: 0.3836, Test Loss: 0.9427
Epoch: 219, Training Loss: 0.3832, Test Loss: 0.9449
Epoch: 220, Training Loss: 0.3804, Test Loss: 0.9315
Epoch: 221, Training Loss: 0.3794, Test Loss: 0.9648
Epoch: 222, Training Loss: 0.3777, Test Loss: 0.9408
Epoch: 223, Training Loss: 0.3743, Test Loss: 0.9493
Epoch: 224, Training Loss: 0.3735, Test Loss: 0.9503
Epoch: 225, Training Loss: 0.3696, Test Loss: 0.9455
Epoch: 226, Training Loss: 0.3697, Test Loss: 0.9547
Epoch: 227, Training Loss: 0.3671, Test Loss: 0.9534
Epoch: 228, Training Loss: 0.3648, Test Loss: 0.9501
Epoch: 229, Training Loss: 0.3631, Test Loss: 0.9507
Epoch: 230, Training Loss: 0.3612, Test Loss: 0.9482
Epoch: 231, Training Loss: 0.3604, Test Loss: 0.9510
Epoch: 232, Training Loss: 0.3595, Test Loss: 0.9534
Epoch: 233, Training Loss: 0.3562, Test Loss: 0.9545
Epoch: 234, Training Loss: 0.3558, Test Loss: 0.9540
Epoch: 235, Training Loss: 0.3526, Test Loss: 0.9728
Epoch: 236, Training Loss: 0.3502, Test Loss: 0.9733
Epoch: 237, Training Loss: 0.3485, Test Loss: 0.9748
Epoch: 238, Training Loss: 0.3481, Test Loss: 0.9756
Epoch: 239, Training Loss: 0.3456, Test Loss: 0.9599

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.20it/s, loss_test=0.960]
Epoch: 241, Training Loss: 0.3428, Test Loss: 0.9622
Epoch: 242, Training Loss: 0.3410, Test Loss: 0.9746
Epoch: 243, Training Loss: 0.3401, Test Loss: 0.9744
Epoch: 244, Training Loss: 0.3364, Test Loss: 0.9728
Epoch: 245, Training Loss: 0.3359, Test Loss: 0.9767
Epoch: 246, Training Loss: 0.3335, Test Loss: 0.9573
Epoch: 247, Training Loss: 0.3334, Test Loss: 0.9745
Epoch: 248, Training Loss: 0.3326, Test Loss: 0.9674
Epoch: 249, Training Loss: 0.3301, Test Loss: 0.9599
Model saved as model_8057785np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-1230000-8057785np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9952764391899109, 0.9937171220779419, 0.9938950061798095, 0.9929214596748352, 0.9942826390266418, 0.9876177549362183, 0.992392098903656, 0.9870986342430115, 0.9935556411743164, 0.9904021620750427, 0.9891660928726196, 0.9923256039619446, 0.9863319516181945, 0.9838866949081421, 0.9870883703231812, 0.9851675510406495, 0.9856366753578186, 0.9887521982192993, 0.9880171656608582, 0.9858274221420288, 0.986075508594513, 0.9803396940231324, 0.9834683418273926, 0.9849024415016174, 0.9800315022468566, 0.9797789216041565, 0.9778612971305847, 0.9738025069236755, 0.9707978248596192, 0.9676306009292602, 0.9578482985496521, 0.9499390125274658, 0.9453557729721069, 0.9368532538414002, 0.9366748452186584, 0.9278491139411926, 0.9248933315277099, 0.9185676574707031, 0.91462482213974, 0.910914957523346, 0.9032256484031678, 0.9048439621925354, 0.8984471797943115, 0.8941204190254212, 0.8857918739318847, 0.8836505889892579, 0.8826407432556153, 0.8806381702423096, 0.8763035058975219, 0.8717336654663086, 0.8710952162742615, 0.8674989461898803, 0.8665400147438049, 0.8593003153800964, 0.8550825476646423, 0.8525676488876343, 0.8495940446853638, 0.8488314986228943, 0.8441688537597656, 0.8447103500366211, 0.8388386368751526, 0.8351696729660034, 0.834037470817566, 0.8302982211112976, 0.8249686598777771, 0.8235932111740112, 0.8220417022705078, 0.8194068551063538, 0.8156612515449524, 0.8146604537963867, 0.8089372992515564, 0.8082282662391662, 0.8022590517997742, 0.8008107423782349, 0.7965806365013123, 0.7928604006767273, 0.7902104973793029, 0.792401397228241, 0.7858394980430603, 0.7850526332855224, 0.7822834968566894, 0.7770731925964356, 0.7737128138542175, 0.77032630443573, 0.7659965038299561, 0.7636019468307496, 0.7610607266426086, 0.760368037223816, 0.7565085649490356, 0.7518314719200134, 0.7453833103179932, 0.7464767813682556, 0.7404860138893128, 0.7379494190216065, 0.7308029055595398, 0.7336382508277893, 0.7282001137733459, 0.7243720173835755, 0.7222301006317139, 0.7208483219146729, 0.7158328890800476, 0.7129570484161377, 0.7079756736755372, 0.7059218049049377, 0.703082287311554, 0.6984351396560669, 0.6988605976104736, 0.6962468743324279, 0.6910640954971313, 0.686000657081604, 0.6841542959213257, 0.6808695077896119, 0.6766957521438599, 0.6712941646575927, 0.6692157745361328, 0.6681029081344605, 0.6642464160919189, 0.6601455926895141, 0.6561608433723449, 0.655677342414856, 0.6511267423629761, 0.6477114796638489, 0.6439141869544983, 0.6399019002914429, 0.6361438155174255, 0.6327799081802368, 0.6305125594139099, 0.6283124089241028, 0.6214075565338135, 0.6232346415519714, 0.6183988571166992, 0.6138337850570679, 0.6117682099342346, 0.607391619682312, 0.6079381942749024, 0.6020788550376892, 0.6009117484092712, 0.5956755399703979, 0.5932778120040894, 0.5884625911712646, 0.5866653680801391, 0.5833944320678711, 0.5794171094894409, 0.5782849431037903, 0.5746282696723938, 0.5723944067955017, 0.567327082157135, 0.5640766382217407, 0.5614035844802856, 0.5595397353172302, 0.5562092304229737, 0.5504969239234925, 0.5479634046554566, 0.5452848553657532, 0.5422839403152466, 0.5399751305580139, 0.5370682597160339, 0.5347194910049439, 0.5312271118164062, 0.5283085465431213, 0.5261996865272522, 0.5232030749320984, 0.5195654153823852, 0.5153278231620788, 0.5141640305519104, 0.5082509517669678, 0.5091119170188904, 0.5049201607704162, 0.5011211931705475, 0.5010404586791992, 0.49687259197235106, 0.49383193254470825, 0.4922317981719971, 0.48847944140434263, 0.48631383776664733, 0.4863203585147858, 0.48206812143325806, 0.47660552263259887, 0.4761034488677979, 0.4720664918422699, 0.4698715448379517, 0.467343670129776, 0.4667729794979095, 0.4631043791770935, 0.46064765453338624, 0.4579716920852661, 0.4554796636104584, 0.4533749520778656, 0.45102897882461546, 0.4465857207775116, 0.44664222598075864, 0.4413515031337738, 0.44143170714378355, 0.4394271671772003, 0.43760865926742554, 0.4350598335266113, 0.43228747844696047, 0.4292055070400238, 0.42756917476654055, 0.42527264952659605, 0.4228048622608185, 0.42108151912689207, 0.4179526388645172, 0.4139163315296173, 0.4128146588802338, 0.4117270350456238, 0.4108541250228882, 0.4058741211891174, 0.4035520374774933, 0.4031993329524994, 0.4003038465976715, 0.39950206875801086, 0.39663861989974974, 0.39303053021430967, 0.39150252342224123, 0.3892017602920532, 0.38760352730751035, 0.38669965863227845, 0.3836004137992859, 0.38318395614624023, 0.3804182529449463, 0.3794397711753845, 0.3777491867542267, 0.37432876229286194, 0.3734963655471802, 0.3696016609668732, 0.3697041511535645, 0.3671461582183838, 0.3647608578205109, 0.3630629122257233, 0.3611552655696869, 0.36044313907623293, 0.359455806016922, 0.3562448740005493, 0.3558247804641724, 0.35260112285614015, 0.3502163589000702, 0.3485297679901123, 0.3481364130973816, 0.345614367723465, 0.34369449615478515, 0.3428416192531586, 0.34099888801574707, 0.34007784724235535, 0.3363610923290253, 0.33592666387557985, 0.3335393011569977, 0.3334104776382446, 0.3325501799583435, 0.33005948066711427], 'loss_test': [1.0764905214309692, 1.0823091268539429, 1.0755788087844849, 1.074430227279663, 1.0704002380371094, 1.0742367506027222, 1.0747445821762085, 1.092403531074524, 1.0732121467590332, 1.0823252201080322, 1.0711562633514404, 1.0762603282928467, 1.0837689638137817, 1.073687195777893, 1.082658290863037, 1.0835649967193604, 1.0732678174972534, 1.0829710960388184, 1.0757683515548706, 1.085984230041504, 1.0658782720565796, 1.0795520544052124, 1.0772852897644043, 1.075827717781067, 1.0675381422042847, 1.0725549459457397, 1.0891947746276855, 1.0688728094100952, 1.0523569583892822, 1.0868693590164185, 1.0806578397750854, 1.0487931966781616, 1.0381243228912354, 1.0458362102508545, 1.0326228141784668, 1.0218682289123535, 1.028227686882019, 1.0061691999435425, 1.0018424987792969, 1.0165488719940186, 1.0044053792953491, 1.0012547969818115, 1.0248119831085205, 0.9968543648719788, 0.9997767210006714, 0.9967855215072632, 1.0014679431915283, 0.9735296964645386, 0.9937393665313721, 0.9917828440666199, 0.9722171425819397, 0.9717406034469604, 0.9658059477806091, 0.976243793964386, 0.9548714756965637, 0.9635976552963257, 0.9505811929702759, 0.9632272720336914, 0.9757197499275208, 0.9520496129989624, 0.9543104767799377, 0.9330040216445923, 0.9301971793174744, 0.9580967426300049, 0.9468403458595276, 0.9408360123634338, 0.9540863037109375, 0.9427892565727234, 0.9526090025901794, 0.9271530508995056, 0.9347381591796875, 0.9326308369636536, 0.9429963827133179, 0.9379372596740723, 0.9269155263900757, 0.9320094585418701, 0.9255545735359192, 0.9283376336097717, 0.9239183664321899, 0.9076309204101562, 0.9225742816925049, 0.9136351346969604, 0.9121268391609192, 0.9065768718719482, 0.9025790691375732, 0.9276323318481445, 0.9148274064064026, 0.916237473487854, 0.9085938334465027, 0.9157704710960388, 0.9040753841400146, 0.8895072340965271, 0.9073107838630676, 0.8963533639907837, 0.8923959732055664, 0.909742534160614, 0.9040285348892212, 0.8961394429206848, 0.8896170854568481, 0.8786820769309998, 0.8902896046638489, 0.8785375356674194, 0.8908874988555908, 0.8904526233673096, 0.8987315893173218, 0.8765659332275391, 0.8821573853492737, 0.8734433054924011, 0.8835663199424744, 0.8788130283355713, 0.8782559037208557, 0.8845891356468201, 0.8835421204566956, 0.8834599852561951, 0.874442994594574, 0.8858955502510071, 0.8837156891822815, 0.8875796794891357, 0.8720389008522034, 0.8793231844902039, 0.8725719451904297, 0.8786620497703552, 0.8742166757583618, 0.8802798390388489, 0.8753155469894409, 0.8750419020652771, 0.8730221390724182, 0.8884387016296387, 0.8806092143058777, 0.8743296265602112, 0.8762559294700623, 0.8705036044120789, 0.8809217810630798, 0.8719550371170044, 0.8805106282234192, 0.873769998550415, 0.8838382363319397, 0.8821398615837097, 0.8671101927757263, 0.8852386474609375, 0.881020188331604, 0.8668736815452576, 0.8864777088165283, 0.8855369091033936, 0.8932351469993591, 0.8908522725105286, 0.8874972462654114, 0.8913655877113342, 0.8839576244354248, 0.8788679838180542, 0.8949128985404968, 0.8943390250205994, 0.8928547501564026, 0.8841749429702759, 0.899095356464386, 0.8887314796447754, 0.8911170363426208, 0.9002740979194641, 0.9031391143798828, 0.9010728597640991, 0.9066814184188843, 0.8932712078094482, 0.8919228911399841, 0.8981804847717285, 0.9111438393592834, 0.8923252820968628, 0.9074756503105164, 0.9031184315681458, 0.9067676067352295, 0.9086612462997437, 0.8994110226631165, 0.9033172726631165, 0.9097985625267029, 0.909171998500824, 0.9169230461120605, 0.9084898233413696, 0.8995229005813599, 0.9163316488265991, 0.9107504487037659, 0.900607168674469, 0.9130944609642029, 0.9178370237350464, 0.9236323237419128, 0.9113744497299194, 0.9188830852508545, 0.9282746315002441, 0.9132723808288574, 0.9212555289268494, 0.9287775754928589, 0.9256842136383057, 0.9256324172019958, 0.9249190092086792, 0.9256432056427002, 0.9248972535133362, 0.9166020750999451, 0.920647382736206, 0.918212890625, 0.9382244944572449, 0.9307875037193298, 0.9337913990020752, 0.93195641040802, 0.9396218657493591, 0.9162803292274475, 0.935782790184021, 0.9399933218955994, 0.9453283548355103, 0.9472744464874268, 0.9281556010246277, 0.9382094740867615, 0.9445093870162964, 0.9436038136482239, 0.943648636341095, 0.9402087330818176, 0.9450287222862244, 0.9415189623832703, 0.9435484409332275, 0.9379319548606873, 0.9560566544532776, 0.942739725112915, 0.944863498210907, 0.9315244555473328, 0.9647902250289917, 0.940784215927124, 0.9493435621261597, 0.9502875804901123, 0.9455004334449768, 0.9546990394592285, 0.9533964395523071, 0.950103759765625, 0.9507258534431458, 0.9482355117797852, 0.9509831070899963, 0.9534198641777039, 0.9544892311096191, 0.9540039300918579, 0.972778856754303, 0.9733448028564453, 0.9747888445854187, 0.9755690097808838, 0.9598873853683472, 0.9594248533248901, 0.9622319936752319, 0.9746069312095642, 0.9743891358375549, 0.9728037118911743, 0.9766762256622314, 0.9572803974151611, 0.9744681715965271, 0.9674232602119446, 0.9599477648735046], 'identifier': '8057785np'}