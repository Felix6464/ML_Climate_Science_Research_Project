
 20%|███████████████████▌                                                                              | 20/100 [00:01<00:06, 12.70it/s, loss_test=1.067]
Epoch: 00, Training Loss: 0.9943, Test Loss: 1.0511
Epoch: 01, Training Loss: 0.9952, Test Loss: 1.0793
Epoch: 02, Training Loss: 0.9864, Test Loss: 1.0647
Epoch: 03, Training Loss: 0.9932, Test Loss: 1.0624
Epoch: 04, Training Loss: 0.9938, Test Loss: 1.0828
Epoch: 05, Training Loss: 0.9936, Test Loss: 1.1047
Epoch: 06, Training Loss: 0.9921, Test Loss: 1.0688
Epoch: 07, Training Loss: 0.9839, Test Loss: 1.0883
Epoch: 08, Training Loss: 0.9886, Test Loss: 1.0865
Epoch: 09, Training Loss: 0.9875, Test Loss: 1.0708
Epoch: 10, Training Loss: 0.9876, Test Loss: 1.0902
Epoch: 11, Training Loss: 0.9889, Test Loss: 1.0751
Epoch: 12, Training Loss: 0.9872, Test Loss: 1.0789
Epoch: 13, Training Loss: 0.9861, Test Loss: 1.0779
Epoch: 14, Training Loss: 0.9885, Test Loss: 1.0814
Epoch: 15, Training Loss: 0.9841, Test Loss: 1.0656
Epoch: 16, Training Loss: 0.9896, Test Loss: 1.0963
Epoch: 17, Training Loss: 0.9853, Test Loss: 1.0878
Epoch: 18, Training Loss: 0.9882, Test Loss: 1.0713
Epoch: 19, Training Loss: 0.9839, Test Loss: 1.0864

 44%|███████████████████████████████████████████                                                       | 44/100 [00:03<00:04, 11.77it/s, loss_test=1.012]
Epoch: 21, Training Loss: 0.9834, Test Loss: 1.0734
Epoch: 22, Training Loss: 0.9811, Test Loss: 1.0875
Epoch: 23, Training Loss: 0.9831, Test Loss: 1.0879
Epoch: 24, Training Loss: 0.9844, Test Loss: 1.0978
Epoch: 25, Training Loss: 0.9854, Test Loss: 1.0792
Epoch: 26, Training Loss: 0.9763, Test Loss: 1.0757
Epoch: 27, Training Loss: 0.9812, Test Loss: 1.0732
Epoch: 28, Training Loss: 0.9797, Test Loss: 1.0672
Epoch: 29, Training Loss: 0.9788, Test Loss: 1.0596
Epoch: 30, Training Loss: 0.9810, Test Loss: 1.0672
Epoch: 31, Training Loss: 0.9707, Test Loss: 1.0909
Epoch: 32, Training Loss: 0.9747, Test Loss: 1.0778
Epoch: 33, Training Loss: 0.9692, Test Loss: 1.0646
Epoch: 34, Training Loss: 0.9566, Test Loss: 1.0572
Epoch: 35, Training Loss: 0.9590, Test Loss: 1.0719
Epoch: 36, Training Loss: 0.9482, Test Loss: 1.0741
Epoch: 37, Training Loss: 0.9410, Test Loss: 1.0666
Epoch: 38, Training Loss: 0.9312, Test Loss: 1.0475
Epoch: 39, Training Loss: 0.9210, Test Loss: 1.0602
Epoch: 40, Training Loss: 0.9158, Test Loss: 1.0342
Epoch: 41, Training Loss: 0.9044, Test Loss: 1.0385
Epoch: 42, Training Loss: 0.8973, Test Loss: 1.0270

 68%|██████████████████████████████████████████████████████████████████▋                               | 68/100 [00:05<00:02, 11.94it/s, loss_test=0.924]
Epoch: 44, Training Loss: 0.8825, Test Loss: 1.0124
Epoch: 45, Training Loss: 0.8740, Test Loss: 1.0115
Epoch: 46, Training Loss: 0.8664, Test Loss: 1.0082
Epoch: 47, Training Loss: 0.8571, Test Loss: 1.0038
Epoch: 48, Training Loss: 0.8532, Test Loss: 0.9769
Epoch: 49, Training Loss: 0.8455, Test Loss: 0.9772
Epoch: 50, Training Loss: 0.8444, Test Loss: 0.9667
Epoch: 51, Training Loss: 0.8387, Test Loss: 0.9742
Epoch: 52, Training Loss: 0.8301, Test Loss: 0.9655
Epoch: 53, Training Loss: 0.8258, Test Loss: 0.9588
Epoch: 54, Training Loss: 0.8205, Test Loss: 0.9505
Epoch: 55, Training Loss: 0.8179, Test Loss: 0.9726
Epoch: 56, Training Loss: 0.8142, Test Loss: 0.9250
Epoch: 57, Training Loss: 0.8064, Test Loss: 0.9533
Epoch: 58, Training Loss: 0.8027, Test Loss: 0.9337
Epoch: 59, Training Loss: 0.7967, Test Loss: 0.9399
Epoch: 60, Training Loss: 0.7905, Test Loss: 0.9260
Epoch: 61, Training Loss: 0.7878, Test Loss: 0.9317
Epoch: 62, Training Loss: 0.7877, Test Loss: 0.9428
Epoch: 63, Training Loss: 0.7815, Test Loss: 0.9329
Epoch: 64, Training Loss: 0.7797, Test Loss: 0.9427
Epoch: 65, Training Loss: 0.7705, Test Loss: 0.9318
Epoch: 66, Training Loss: 0.7693, Test Loss: 0.9358
Epoch: 67, Training Loss: 0.7642, Test Loss: 0.9147

 94%|████████████████████████████████████████████████████████████████████████████████████████████      | 94/100 [00:07<00:00, 12.54it/s, loss_test=0.903]
Epoch: 69, Training Loss: 0.7559, Test Loss: 0.9169
Epoch: 70, Training Loss: 0.7493, Test Loss: 0.9134
Epoch: 71, Training Loss: 0.7470, Test Loss: 0.9315
Epoch: 72, Training Loss: 0.7423, Test Loss: 0.9168
Epoch: 73, Training Loss: 0.7360, Test Loss: 0.9137
Epoch: 74, Training Loss: 0.7321, Test Loss: 0.9117
Epoch: 75, Training Loss: 0.7299, Test Loss: 0.9070
Epoch: 76, Training Loss: 0.7266, Test Loss: 0.9188
Epoch: 77, Training Loss: 0.7229, Test Loss: 0.9080
Epoch: 78, Training Loss: 0.7187, Test Loss: 0.9123
Epoch: 79, Training Loss: 0.7131, Test Loss: 0.9071
Epoch: 80, Training Loss: 0.7086, Test Loss: 0.9096
Epoch: 81, Training Loss: 0.7033, Test Loss: 0.9203
Epoch: 82, Training Loss: 0.7005, Test Loss: 0.9184
Epoch: 83, Training Loss: 0.6940, Test Loss: 0.9113
Epoch: 84, Training Loss: 0.6926, Test Loss: 0.9051
Epoch: 85, Training Loss: 0.6835, Test Loss: 0.9096
Epoch: 86, Training Loss: 0.6815, Test Loss: 0.9126
Epoch: 87, Training Loss: 0.6795, Test Loss: 0.8996
Epoch: 88, Training Loss: 0.6763, Test Loss: 0.9273
Epoch: 89, Training Loss: 0.6691, Test Loss: 0.9097
Epoch: 90, Training Loss: 0.6681, Test Loss: 0.9219
Epoch: 91, Training Loss: 0.6645, Test Loss: 0.9208
Epoch: 92, Training Loss: 0.6616, Test Loss: 0.9190

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.18it/s, loss_test=0.903]
Epoch: 94, Training Loss: 0.6550, Test Loss: 0.9168
Epoch: 95, Training Loss: 0.6497, Test Loss: 0.9054
Epoch: 96, Training Loss: 0.6483, Test Loss: 0.9100
Epoch: 97, Training Loss: 0.6408, Test Loss: 0.9068
Epoch: 98, Training Loss: 0.6375, Test Loss: 0.9135
Epoch: 99, Training Loss: 0.6353, Test Loss: 0.9034
Model saved as model_3293526np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-12100000-3293526np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 100, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9942685484886169, 0.9952407956123352, 0.9864194989204407, 0.9931950449943543, 0.993761169910431, 0.9936330199241639, 0.9921440482139587, 0.9838691830635071, 0.9886156797409058, 0.9874927997589111, 0.9875860214233398, 0.9889427065849304, 0.9872144937515259, 0.9860677599906922, 0.9885313868522644, 0.9840875506401062, 0.9895533442497253, 0.9852639555931091, 0.9882367491722107, 0.9838547587394715, 0.9819626092910767, 0.9833834886550903, 0.9810984015464783, 0.9830866694450379, 0.9843881368637085, 0.9853559851646423, 0.9762587547302246, 0.9812323689460755, 0.9796776056289673, 0.978789222240448, 0.9810356616973877, 0.9707108736038208, 0.9746549963951111, 0.9691768765449524, 0.9566039204597473, 0.9589725971221924, 0.9481574296951294, 0.9409653782844544, 0.9311721444129943, 0.9210108160972595, 0.9158424377441406, 0.9043864250183106, 0.8972590446472168, 0.8917291641235352, 0.8825085282325744, 0.8739894986152649, 0.866388738155365, 0.857116448879242, 0.8531800866127014, 0.8454943060874939, 0.8443731546401978, 0.8386898279190064, 0.8300716280937195, 0.8257932901382447, 0.8204792499542236, 0.8178851366043091, 0.8141636729240418, 0.8063750028610229, 0.8027386665344238, 0.7966807723045349, 0.7904743552207947, 0.7878002762794495, 0.7877452731132507, 0.78149893283844, 0.7797426104545593, 0.7705326318740845, 0.7692752599716186, 0.7641576528549194, 0.760958480834961, 0.7559305429458618, 0.7492822885513306, 0.7469613432884217, 0.7423179268836975, 0.736009955406189, 0.7320544123649597, 0.7299370169639587, 0.7266185641288757, 0.7229303359985352, 0.7187429547309876, 0.713071858882904, 0.7085802912712097, 0.7033098459243774, 0.700483763217926, 0.6939779877662658, 0.6926027536392212, 0.6835323572158813, 0.6815475821495056, 0.6794956445693969, 0.6762769460678101, 0.6690550088882447, 0.6680931329727173, 0.6644591689109802, 0.6616471290588379, 0.6571981191635132, 0.6550393581390381, 0.6497188806533813, 0.6482747673988343, 0.6408133506774902, 0.6375291466712951, 0.6352579116821289], 'loss_test': [1.051056981086731, 1.0792855024337769, 1.0646591186523438, 1.062387228012085, 1.0828237533569336, 1.1046526432037354, 1.0687620639801025, 1.0883362293243408, 1.0864659547805786, 1.0707876682281494, 1.0901970863342285, 1.0750911235809326, 1.0789034366607666, 1.0779019594192505, 1.08141028881073, 1.0656253099441528, 1.0963367223739624, 1.087761402130127, 1.0713285207748413, 1.0863583087921143, 1.0671145915985107, 1.0733736753463745, 1.087480068206787, 1.0879170894622803, 1.0977872610092163, 1.079232931137085, 1.0757005214691162, 1.0732163190841675, 1.0672448873519897, 1.0595660209655762, 1.0672030448913574, 1.0909042358398438, 1.0778229236602783, 1.0646095275878906, 1.057182788848877, 1.0719271898269653, 1.0740954875946045, 1.0665628910064697, 1.0475181341171265, 1.0602381229400635, 1.0341603755950928, 1.038474440574646, 1.027048110961914, 1.0182085037231445, 1.0124210119247437, 1.0115338563919067, 1.0082441568374634, 1.003762125968933, 0.9769228100776672, 0.9771766066551208, 0.9666845202445984, 0.9741784334182739, 0.965543806552887, 0.9587879776954651, 0.9505230784416199, 0.9726055860519409, 0.9250218272209167, 0.9533369541168213, 0.9337236285209656, 0.9399436116218567, 0.9259939789772034, 0.9317259192466736, 0.9427939653396606, 0.9329253435134888, 0.9426815509796143, 0.9318240284919739, 0.9358096718788147, 0.914716899394989, 0.9242045879364014, 0.9168604612350464, 0.9134304523468018, 0.9315019845962524, 0.9167793393135071, 0.913670003414154, 0.9117079377174377, 0.9069721698760986, 0.9187780022621155, 0.9080383777618408, 0.912269115447998, 0.9070865511894226, 0.909622073173523, 0.9203091263771057, 0.918402910232544, 0.9112881422042847, 0.9051398038864136, 0.9095588326454163, 0.9126288890838623, 0.8996171951293945, 0.9273090958595276, 0.909656286239624, 0.9218558669090271, 0.9208084940910339, 0.9190381765365601, 0.9028943777084351, 0.9167548418045044, 0.9053598642349243, 0.9099511504173279, 0.9068430662155151, 0.9134825468063354, 0.9033939242362976], 'identifier': '3293526np'}