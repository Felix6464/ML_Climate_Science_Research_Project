
  8%|███████▊                                                                                          | 20/250 [00:01<00:18, 12.36it/s, loss_test=1.076]
Epoch: 00, Training Loss: 0.9974, Test Loss: 1.0816
Epoch: 01, Training Loss: 0.9944, Test Loss: 1.0808
Epoch: 02, Training Loss: 0.9907, Test Loss: 1.0873
Epoch: 03, Training Loss: 0.9875, Test Loss: 1.0978
Epoch: 04, Training Loss: 0.9875, Test Loss: 1.0688
Epoch: 05, Training Loss: 0.9937, Test Loss: 1.1016
Epoch: 06, Training Loss: 0.9866, Test Loss: 1.0557
Epoch: 07, Training Loss: 0.9906, Test Loss: 1.0566
Epoch: 08, Training Loss: 0.9900, Test Loss: 1.0984
Epoch: 09, Training Loss: 0.9921, Test Loss: 1.0824
Epoch: 10, Training Loss: 0.9884, Test Loss: 1.0859
Epoch: 11, Training Loss: 0.9857, Test Loss: 1.0778
Epoch: 12, Training Loss: 0.9794, Test Loss: 1.0825
Epoch: 13, Training Loss: 0.9908, Test Loss: 1.0830
Epoch: 14, Training Loss: 0.9907, Test Loss: 1.0692
Epoch: 15, Training Loss: 0.9855, Test Loss: 1.0718
Epoch: 16, Training Loss: 0.9877, Test Loss: 1.0832
Epoch: 17, Training Loss: 0.9870, Test Loss: 1.0937
Epoch: 18, Training Loss: 0.9879, Test Loss: 1.0685

 18%|█████████████████▏                                                                                | 44/250 [00:03<00:16, 12.56it/s, loss_test=0.970]
Epoch: 20, Training Loss: 0.9851, Test Loss: 1.0690
Epoch: 21, Training Loss: 0.9834, Test Loss: 1.0761
Epoch: 22, Training Loss: 0.9853, Test Loss: 1.0752
Epoch: 23, Training Loss: 0.9853, Test Loss: 1.0740
Epoch: 24, Training Loss: 0.9829, Test Loss: 1.0758
Epoch: 25, Training Loss: 0.9837, Test Loss: 1.0851
Epoch: 26, Training Loss: 0.9807, Test Loss: 1.0702
Epoch: 27, Training Loss: 0.9804, Test Loss: 1.0762
Epoch: 28, Training Loss: 0.9776, Test Loss: 1.0768
Epoch: 29, Training Loss: 0.9762, Test Loss: 1.0761
Epoch: 30, Training Loss: 0.9718, Test Loss: 1.0634
Epoch: 31, Training Loss: 0.9731, Test Loss: 1.0770
Epoch: 32, Training Loss: 0.9678, Test Loss: 1.0589
Epoch: 33, Training Loss: 0.9600, Test Loss: 1.0561
Epoch: 34, Training Loss: 0.9519, Test Loss: 1.0450
Epoch: 35, Training Loss: 0.9440, Test Loss: 1.0464
Epoch: 36, Training Loss: 0.9380, Test Loss: 1.0095
Epoch: 37, Training Loss: 0.9338, Test Loss: 1.0221
Epoch: 38, Training Loss: 0.9250, Test Loss: 1.0080
Epoch: 39, Training Loss: 0.9200, Test Loss: 0.9949
Epoch: 40, Training Loss: 0.9159, Test Loss: 0.9894
Epoch: 41, Training Loss: 0.9101, Test Loss: 0.9921
Epoch: 42, Training Loss: 0.9054, Test Loss: 0.9755
Epoch: 43, Training Loss: 0.9019, Test Loss: 0.9814

 28%|███████████████████████████▍                                                                      | 70/250 [00:05<00:14, 12.55it/s, loss_test=0.923]
Epoch: 45, Training Loss: 0.8935, Test Loss: 0.9867
Epoch: 46, Training Loss: 0.8907, Test Loss: 0.9675
Epoch: 47, Training Loss: 0.8876, Test Loss: 0.9766
Epoch: 48, Training Loss: 0.8823, Test Loss: 0.9566
Epoch: 49, Training Loss: 0.8803, Test Loss: 0.9547
Epoch: 50, Training Loss: 0.8765, Test Loss: 0.9503
Epoch: 51, Training Loss: 0.8725, Test Loss: 0.9507
Epoch: 52, Training Loss: 0.8669, Test Loss: 0.9375
Epoch: 53, Training Loss: 0.8650, Test Loss: 0.9582
Epoch: 54, Training Loss: 0.8615, Test Loss: 0.9567
Epoch: 55, Training Loss: 0.8524, Test Loss: 0.9502
Epoch: 56, Training Loss: 0.8488, Test Loss: 0.9567
Epoch: 57, Training Loss: 0.8434, Test Loss: 0.9357
Epoch: 58, Training Loss: 0.8389, Test Loss: 0.9298
Epoch: 59, Training Loss: 0.8359, Test Loss: 0.9447
Epoch: 60, Training Loss: 0.8308, Test Loss: 0.9490
Epoch: 61, Training Loss: 0.8257, Test Loss: 0.9384
Epoch: 62, Training Loss: 0.8199, Test Loss: 0.9397
Epoch: 63, Training Loss: 0.8198, Test Loss: 0.9330
Epoch: 64, Training Loss: 0.8112, Test Loss: 0.9308
Epoch: 65, Training Loss: 0.8085, Test Loss: 0.9363
Epoch: 66, Training Loss: 0.7980, Test Loss: 0.9296
Epoch: 67, Training Loss: 0.7974, Test Loss: 0.9372

 38%|████████████████████████████████████▊                                                             | 94/250 [00:07<00:13, 11.95it/s, loss_test=0.903]
Epoch: 69, Training Loss: 0.7852, Test Loss: 0.9233
Epoch: 70, Training Loss: 0.7840, Test Loss: 0.9193
Epoch: 71, Training Loss: 0.7776, Test Loss: 0.9229
Epoch: 72, Training Loss: 0.7711, Test Loss: 0.9231
Epoch: 73, Training Loss: 0.7664, Test Loss: 0.9297
Epoch: 74, Training Loss: 0.7592, Test Loss: 0.9127
Epoch: 75, Training Loss: 0.7566, Test Loss: 0.9207
Epoch: 76, Training Loss: 0.7536, Test Loss: 0.9041
Epoch: 77, Training Loss: 0.7480, Test Loss: 0.9249
Epoch: 78, Training Loss: 0.7436, Test Loss: 0.9122
Epoch: 79, Training Loss: 0.7410, Test Loss: 0.9061
Epoch: 80, Training Loss: 0.7351, Test Loss: 0.9203
Epoch: 81, Training Loss: 0.7332, Test Loss: 0.9130
Epoch: 82, Training Loss: 0.7315, Test Loss: 0.9050
Epoch: 83, Training Loss: 0.7281, Test Loss: 0.9173
Epoch: 84, Training Loss: 0.7224, Test Loss: 0.9175
Epoch: 85, Training Loss: 0.7165, Test Loss: 0.9041
Epoch: 86, Training Loss: 0.7122, Test Loss: 0.9031
Epoch: 87, Training Loss: 0.7108, Test Loss: 0.8984
Epoch: 88, Training Loss: 0.7078, Test Loss: 0.9108
Epoch: 89, Training Loss: 0.7012, Test Loss: 0.9033
Epoch: 90, Training Loss: 0.7027, Test Loss: 0.8950
Epoch: 91, Training Loss: 0.6957, Test Loss: 0.8919
Epoch: 92, Training Loss: 0.6928, Test Loss: 0.9202

 47%|█████████████████████████████████████████████▊                                                   | 118/250 [00:09<00:10, 12.29it/s, loss_test=0.908]
Epoch: 94, Training Loss: 0.6866, Test Loss: 0.9010
Epoch: 95, Training Loss: 0.6821, Test Loss: 0.9028
Epoch: 96, Training Loss: 0.6802, Test Loss: 0.9086
Epoch: 97, Training Loss: 0.6750, Test Loss: 0.9074
Epoch: 98, Training Loss: 0.6717, Test Loss: 0.9022
Epoch: 99, Training Loss: 0.6705, Test Loss: 0.9081
Epoch: 100, Training Loss: 0.6677, Test Loss: 0.8988
Epoch: 101, Training Loss: 0.6617, Test Loss: 0.9030
Epoch: 102, Training Loss: 0.6588, Test Loss: 0.9084
Epoch: 103, Training Loss: 0.6555, Test Loss: 0.9091
Epoch: 104, Training Loss: 0.6510, Test Loss: 0.9093
Epoch: 105, Training Loss: 0.6467, Test Loss: 0.9065
Epoch: 106, Training Loss: 0.6447, Test Loss: 0.9064
Epoch: 107, Training Loss: 0.6390, Test Loss: 0.8968
Epoch: 108, Training Loss: 0.6368, Test Loss: 0.9139
Epoch: 109, Training Loss: 0.6333, Test Loss: 0.9018
Epoch: 110, Training Loss: 0.6316, Test Loss: 0.9083
Epoch: 111, Training Loss: 0.6287, Test Loss: 0.9021
Epoch: 112, Training Loss: 0.6226, Test Loss: 0.9110
Epoch: 113, Training Loss: 0.6188, Test Loss: 0.9204
Epoch: 114, Training Loss: 0.6169, Test Loss: 0.9128
Epoch: 115, Training Loss: 0.6132, Test Loss: 0.9133

 57%|███████████████████████████████████████████████████████                                          | 142/250 [00:11<00:09, 11.81it/s, loss_test=0.924]
Epoch: 117, Training Loss: 0.6046, Test Loss: 0.9083
Epoch: 118, Training Loss: 0.6012, Test Loss: 0.9148
Epoch: 119, Training Loss: 0.5980, Test Loss: 0.9191
Epoch: 120, Training Loss: 0.5943, Test Loss: 0.9178
Epoch: 121, Training Loss: 0.5913, Test Loss: 0.9320
Epoch: 122, Training Loss: 0.5867, Test Loss: 0.9262
Epoch: 123, Training Loss: 0.5834, Test Loss: 0.9254
Epoch: 124, Training Loss: 0.5806, Test Loss: 0.9174
Epoch: 125, Training Loss: 0.5784, Test Loss: 0.9262
Epoch: 126, Training Loss: 0.5746, Test Loss: 0.9165
Epoch: 127, Training Loss: 0.5707, Test Loss: 0.9113
Epoch: 128, Training Loss: 0.5668, Test Loss: 0.9177
Epoch: 129, Training Loss: 0.5646, Test Loss: 0.9269
Epoch: 130, Training Loss: 0.5587, Test Loss: 0.9194
Epoch: 131, Training Loss: 0.5575, Test Loss: 0.9283
Epoch: 132, Training Loss: 0.5526, Test Loss: 0.9188
Epoch: 133, Training Loss: 0.5485, Test Loss: 0.9189
Epoch: 134, Training Loss: 0.5490, Test Loss: 0.9323
Epoch: 135, Training Loss: 0.5442, Test Loss: 0.9231
Epoch: 136, Training Loss: 0.5395, Test Loss: 0.9286
Epoch: 137, Training Loss: 0.5377, Test Loss: 0.9312
Epoch: 138, Training Loss: 0.5325, Test Loss: 0.9132
Epoch: 139, Training Loss: 0.5308, Test Loss: 0.9253
Epoch: 140, Training Loss: 0.5266, Test Loss: 0.9381
Epoch: 141, Training Loss: 0.5267, Test Loss: 0.9243
Epoch: 142, Training Loss: 0.5209, Test Loss: 0.9197
Epoch: 143, Training Loss: 0.5190, Test Loss: 0.9291
Epoch: 144, Training Loss: 0.5168, Test Loss: 0.9375
Epoch: 145, Training Loss: 0.5127, Test Loss: 0.9341
Epoch: 146, Training Loss: 0.5115, Test Loss: 0.9297
Epoch: 147, Training Loss: 0.5076, Test Loss: 0.9396
Epoch: 148, Training Loss: 0.5032, Test Loss: 0.9365
Epoch: 149, Training Loss: 0.5024, Test Loss: 0.9426
Epoch: 150, Training Loss: 0.4990, Test Loss: 0.9301
Epoch: 151, Training Loss: 0.4964, Test Loss: 0.9480
Epoch: 152, Training Loss: 0.4935, Test Loss: 0.9521
Epoch: 153, Training Loss: 0.4900, Test Loss: 0.9411
Epoch: 154, Training Loss: 0.4867, Test Loss: 0.9517
Epoch: 155, Training Loss: 0.4838, Test Loss: 0.9429
Epoch: 156, Training Loss: 0.4823, Test Loss: 0.9436
Epoch: 157, Training Loss: 0.4798, Test Loss: 0.9471
Epoch: 158, Training Loss: 0.4766, Test Loss: 0.9421
Epoch: 159, Training Loss: 0.4730, Test Loss: 0.9366
Epoch: 160, Training Loss: 0.4718, Test Loss: 0.9542
Epoch: 161, Training Loss: 0.4697, Test Loss: 0.9442
Epoch: 162, Training Loss: 0.4647, Test Loss: 0.9519
Epoch: 163, Training Loss: 0.4645, Test Loss: 0.9500
Epoch: 164, Training Loss: 0.4609, Test Loss: 0.9470

 66%|████████████████████████████████████████████████████████████████▍                                | 166/250 [00:13<00:06, 12.49it/s, loss_test=0.958]
Epoch: 166, Training Loss: 0.4576, Test Loss: 0.9584
Epoch: 167, Training Loss: 0.4542, Test Loss: 0.9597
Epoch: 168, Training Loss: 0.4515, Test Loss: 0.9667
Epoch: 169, Training Loss: 0.4493, Test Loss: 0.9559
Epoch: 170, Training Loss: 0.4449, Test Loss: 0.9544
Epoch: 171, Training Loss: 0.4440, Test Loss: 0.9590
Epoch: 172, Training Loss: 0.4403, Test Loss: 0.9516
Epoch: 173, Training Loss: 0.4407, Test Loss: 0.9541
Epoch: 174, Training Loss: 0.4387, Test Loss: 0.9613
Epoch: 175, Training Loss: 0.4344, Test Loss: 0.9578
Epoch: 176, Training Loss: 0.4290, Test Loss: 0.9505
Epoch: 177, Training Loss: 0.4274, Test Loss: 0.9642
Epoch: 178, Training Loss: 0.4265, Test Loss: 0.9750
Epoch: 179, Training Loss: 0.4241, Test Loss: 0.9727
Epoch: 180, Training Loss: 0.4222, Test Loss: 0.9713
Epoch: 181, Training Loss: 0.4207, Test Loss: 0.9690
Epoch: 182, Training Loss: 0.4179, Test Loss: 0.9774
Epoch: 183, Training Loss: 0.4155, Test Loss: 0.9745
Epoch: 184, Training Loss: 0.4126, Test Loss: 0.9801
Epoch: 185, Training Loss: 0.4128, Test Loss: 0.9706
Epoch: 186, Training Loss: 0.4089, Test Loss: 0.9909
Epoch: 187, Training Loss: 0.4080, Test Loss: 0.9778
Epoch: 188, Training Loss: 0.4052, Test Loss: 0.9707

 77%|██████████████████████████████████████████████████████████████████████████▍                      | 192/250 [00:15<00:04, 12.05it/s, loss_test=0.975]
Epoch: 190, Training Loss: 0.4011, Test Loss: 0.9793
Epoch: 191, Training Loss: 0.3999, Test Loss: 0.9753
Epoch: 192, Training Loss: 0.3962, Test Loss: 0.9703
Epoch: 193, Training Loss: 0.3941, Test Loss: 0.9783
Epoch: 194, Training Loss: 0.3921, Test Loss: 0.9700
Epoch: 195, Training Loss: 0.3905, Test Loss: 0.9682
Epoch: 196, Training Loss: 0.3880, Test Loss: 0.9845
Epoch: 197, Training Loss: 0.3852, Test Loss: 0.9953
Epoch: 198, Training Loss: 0.3852, Test Loss: 0.9751
Epoch: 199, Training Loss: 0.3825, Test Loss: 0.9905
Epoch: 200, Training Loss: 0.3813, Test Loss: 1.0054
Epoch: 201, Training Loss: 0.3782, Test Loss: 0.9776
Epoch: 202, Training Loss: 0.3764, Test Loss: 0.9796
Epoch: 203, Training Loss: 0.3747, Test Loss: 0.9867
Epoch: 204, Training Loss: 0.3718, Test Loss: 0.9761
Epoch: 205, Training Loss: 0.3720, Test Loss: 0.9957
Epoch: 206, Training Loss: 0.3694, Test Loss: 0.9902
Epoch: 207, Training Loss: 0.3675, Test Loss: 0.9852
Epoch: 208, Training Loss: 0.3661, Test Loss: 0.9863
Epoch: 209, Training Loss: 0.3634, Test Loss: 0.9702
Epoch: 210, Training Loss: 0.3623, Test Loss: 0.9939
Epoch: 211, Training Loss: 0.3592, Test Loss: 0.9954
Epoch: 212, Training Loss: 0.3585, Test Loss: 0.9758

 86%|███████████████████████████████████████████████████████████████████████████████████              | 214/250 [00:17<00:03, 11.66it/s, loss_test=0.986]
Epoch: 214, Training Loss: 0.3543, Test Loss: 0.9864
Epoch: 215, Training Loss: 0.3533, Test Loss: 0.9959
Epoch: 216, Training Loss: 0.3504, Test Loss: 0.9997
Epoch: 217, Training Loss: 0.3503, Test Loss: 0.9900
Epoch: 218, Training Loss: 0.3486, Test Loss: 0.9807
Epoch: 219, Training Loss: 0.3455, Test Loss: 1.0043
Epoch: 220, Training Loss: 0.3450, Test Loss: 1.0032
Epoch: 221, Training Loss: 0.3436, Test Loss: 1.0052
Epoch: 222, Training Loss: 0.3411, Test Loss: 0.9853
Epoch: 223, Training Loss: 0.3389, Test Loss: 0.9808
Epoch: 224, Training Loss: 0.3386, Test Loss: 0.9951
Epoch: 225, Training Loss: 0.3369, Test Loss: 1.0103
Epoch: 226, Training Loss: 0.3349, Test Loss: 0.9945
Epoch: 227, Training Loss: 0.3331, Test Loss: 0.9965
Epoch: 228, Training Loss: 0.3324, Test Loss: 1.0043
Epoch: 229, Training Loss: 0.3310, Test Loss: 1.0077
Epoch: 230, Training Loss: 0.3293, Test Loss: 0.9979
Epoch: 231, Training Loss: 0.3280, Test Loss: 1.0103
Epoch: 232, Training Loss: 0.3257, Test Loss: 1.0021
Epoch: 233, Training Loss: 0.3247, Test Loss: 0.9842
Epoch: 234, Training Loss: 0.3222, Test Loss: 0.9897
Epoch: 235, Training Loss: 0.3211, Test Loss: 0.9877
Epoch: 236, Training Loss: 0.3210, Test Loss: 1.0060


100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.06it/s, loss_test=0.995]
Epoch: 238, Training Loss: 0.3177, Test Loss: 1.0068
Epoch: 239, Training Loss: 0.3170, Test Loss: 0.9790
Epoch: 240, Training Loss: 0.3150, Test Loss: 1.0152
Epoch: 241, Training Loss: 0.3128, Test Loss: 1.0075
Epoch: 242, Training Loss: 0.3111, Test Loss: 0.9841
Epoch: 243, Training Loss: 0.3111, Test Loss: 0.9998
Epoch: 244, Training Loss: 0.3087, Test Loss: 0.9821
Epoch: 245, Training Loss: 0.3071, Test Loss: 0.9854
Epoch: 246, Training Loss: 0.3061, Test Loss: 1.0073
Epoch: 247, Training Loss: 0.3043, Test Loss: 1.0008
Epoch: 248, Training Loss: 0.3039, Test Loss: 1.0014
Epoch: 249, Training Loss: 0.3023, Test Loss: 0.9953
Model saved as model_8771755np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-123000-8771755np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9973670244216919, 0.9943727493286133, 0.9906524181365967, 0.9874734282493591, 0.987548542022705, 0.9936949372291565, 0.9866316437721252, 0.9905923724174499, 0.9900316715240478, 0.992108964920044, 0.9884406328201294, 0.9856817126274109, 0.9794435262680053, 0.9908035516738891, 0.9907058238983154, 0.985470449924469, 0.9877124428749084, 0.986982798576355, 0.9879393100738525, 0.9821402907371521, 0.9850517988204956, 0.9834071278572083, 0.9852540373802186, 0.9852564930915833, 0.9828654170036316, 0.9837478280067444, 0.9806792259216308, 0.9804444670677185, 0.9776355981826782, 0.9762167930603027, 0.9718057155609131, 0.9730571269989013, 0.9678393363952636, 0.9599617719650269, 0.9519420385360717, 0.9440352916717529, 0.9379812002182006, 0.933760142326355, 0.9249594926834106, 0.9200317025184631, 0.9159167170524597, 0.9100908994674682, 0.9053646445274353, 0.9018597602844238, 0.9008314490318299, 0.8935439944267273, 0.8907471179962159, 0.8876480102539063, 0.8823292851448059, 0.8802846431732178, 0.8765119671821594, 0.8724828362464905, 0.8668614506721497, 0.8650022983551026, 0.8614639282226563, 0.8524099588394165, 0.8487945914268493, 0.8433805108070374, 0.8389016032218933, 0.8358620285987854, 0.8308075904846192, 0.8256986498832702, 0.8199115633964539, 0.819792115688324, 0.8111624240875244, 0.8085233807563782, 0.7980201244354248, 0.7973667621612549, 0.7915263891220092, 0.7852424144744873, 0.7840131282806396, 0.7776102542877197, 0.7711366772651672, 0.7664031744003296, 0.7591503620147705, 0.756565535068512, 0.7535855531692505, 0.7479651927947998, 0.7435999989509583, 0.7409750580787658, 0.73512943983078, 0.7331927180290222, 0.731476628780365, 0.7280551075935364, 0.7223602414131165, 0.7165096759796142, 0.7122180104255676, 0.7107594251632691, 0.707836925983429, 0.7011628985404968, 0.7027042508125305, 0.6957352876663208, 0.6927745699882507, 0.6891993165016175, 0.6866320729255676, 0.6821421623229981, 0.6802204847335815, 0.6750417709350586, 0.6717000007629395, 0.6704590559005738, 0.6677427053451538, 0.6617432236671448, 0.6587843060493469, 0.655502462387085, 0.6509642243385315, 0.6466993689537048, 0.6446751594543457, 0.6390464663505554, 0.6368184804916381, 0.6333161950111389, 0.6315978527069092, 0.6286891222000122, 0.6226466298103333, 0.6187671422958374, 0.6168865203857422, 0.6132259964942932, 0.60768963098526, 0.6045572876930236, 0.6011593461036682, 0.5979690194129944, 0.5942615151405335, 0.5912524819374084, 0.5867255806922913, 0.5834313750267028, 0.5806051015853881, 0.5783538699150086, 0.5746274828910828, 0.5706547617912292, 0.5668224453926086, 0.5646388292312622, 0.5587397217750549, 0.5574891924858093, 0.5525765895843506, 0.5485341429710389, 0.5489529967308044, 0.5441981196403504, 0.5395431041717529, 0.5376804590225219, 0.5324532508850097, 0.5307736277580262, 0.526620352268219, 0.5267468929290772, 0.5209171652793885, 0.519021737575531, 0.5167729496955872, 0.5126622557640076, 0.5115157127380371, 0.5075764656066895, 0.5032006025314331, 0.502392852306366, 0.4989739179611206, 0.49643433690071104, 0.49353418946266175, 0.4899617373943329, 0.48669988512992857, 0.483827543258667, 0.4822682738304138, 0.47980096340179446, 0.4766418397426605, 0.47300332188606264, 0.4717926323413849, 0.4697137475013733, 0.46467923521995547, 0.4644564986228943, 0.46092440485954284, 0.45883920788764954, 0.4575585603713989, 0.4541759669780731, 0.45147657990455625, 0.4493087410926819, 0.4448546230792999, 0.44403588175773623, 0.44028252363204956, 0.4407437801361084, 0.43869757652282715, 0.43435108065605166, 0.4289880692958832, 0.427397346496582, 0.42649751901626587, 0.424136221408844, 0.4221593737602234, 0.4206554114818573, 0.4178808808326721, 0.415466719865799, 0.4125525176525116, 0.412842845916748, 0.4089155673980713, 0.4079599499702454, 0.4052417457103729, 0.4036255180835724, 0.40106210112571716, 0.39987061619758607, 0.3961674153804779, 0.394127631187439, 0.39207919836044314, 0.3905171811580658, 0.38798596858978274, 0.38520264625549316, 0.38524729013442993, 0.38247297406196595, 0.38128775358200073, 0.37816320061683656, 0.3764220416545868, 0.37469030618667604, 0.37175155282020567, 0.3719567060470581, 0.3694183170795441, 0.36753990054130553, 0.36607806086540223, 0.3634046494960785, 0.3623081624507904, 0.3591867983341217, 0.3585237920284271, 0.3576739847660065, 0.35431207418441774, 0.35325794816017153, 0.35044686794281005, 0.35026265382766725, 0.3485682845115662, 0.3454832971096039, 0.34503319263458254, 0.3435776770114899, 0.34109923243522644, 0.33887633085250857, 0.3386071026325226, 0.3369456470012665, 0.3348884999752045, 0.33309510350227356, 0.33236650824546815, 0.33100478649139403, 0.3292552590370178, 0.3280106008052826, 0.32570136785507203, 0.324696284532547, 0.3222441792488098, 0.3211185097694397, 0.3210325241088867, 0.31834699511528014, 0.3176526308059692, 0.31698832511901853, 0.31495568752288816, 0.312801206111908, 0.31110079288482667, 0.3111064672470093, 0.3086544990539551, 0.3070649266242981, 0.3060889959335327, 0.30428182482719424, 0.30389794111251833, 0.3022751986980438], 'loss_test': [1.0816459655761719, 1.0807970762252808, 1.0873022079467773, 1.0977858304977417, 1.0687772035598755, 1.1015888452529907, 1.0557103157043457, 1.056551456451416, 1.0983870029449463, 1.0824466943740845, 1.085893154144287, 1.0778305530548096, 1.0824624300003052, 1.0829914808273315, 1.0691560506820679, 1.07183837890625, 1.0832055807113647, 1.093741774559021, 1.0685206651687622, 1.0759838819503784, 1.06899893283844, 1.0761386156082153, 1.075200080871582, 1.073986291885376, 1.0757684707641602, 1.08512282371521, 1.0702269077301025, 1.0761678218841553, 1.0768181085586548, 1.0761359930038452, 1.0634255409240723, 1.077001929283142, 1.058929443359375, 1.0561118125915527, 1.044993281364441, 1.0464214086532593, 1.0095244646072388, 1.0220913887023926, 1.007975459098816, 0.994866132736206, 0.9894149899482727, 0.9921126961708069, 0.9755252003669739, 0.9814472198486328, 0.969928503036499, 0.986696720123291, 0.9674525260925293, 0.9765570759773254, 0.9565969109535217, 0.9547129273414612, 0.9503255486488342, 0.9506556391716003, 0.9374504089355469, 0.9582404494285583, 0.9567203521728516, 0.9502201080322266, 0.9567209482192993, 0.9357258677482605, 0.9297668933868408, 0.9447485208511353, 0.9489669799804688, 0.9384281635284424, 0.9397275447845459, 0.9329593181610107, 0.9307505488395691, 0.9362632036209106, 0.9296324253082275, 0.937207043170929, 0.9199520945549011, 0.9232720732688904, 0.9193028807640076, 0.9228923320770264, 0.9230701923370361, 0.9296778440475464, 0.9126705527305603, 0.9206995964050293, 0.9040829539299011, 0.9248812794685364, 0.9122130870819092, 0.9060913324356079, 0.9203115701675415, 0.9130257964134216, 0.9049957394599915, 0.9172528982162476, 0.9175344109535217, 0.9040743112564087, 0.9030576348304749, 0.8984434604644775, 0.9107796549797058, 0.9032501578330994, 0.8949866890907288, 0.8918526768684387, 0.920203685760498, 0.9030214548110962, 0.9009952545166016, 0.9027621150016785, 0.9085910320281982, 0.9074435830116272, 0.9022148251533508, 0.9080721139907837, 0.8987833857536316, 0.9030399322509766, 0.908398449420929, 0.9090613722801208, 0.9093176126480103, 0.9065340161323547, 0.9064019322395325, 0.8967501521110535, 0.9139156341552734, 0.9017911553382874, 0.9083238840103149, 0.9021226763725281, 0.9109867811203003, 0.9203886389732361, 0.9127806425094604, 0.9132972359657288, 0.9260315299034119, 0.9082865715026855, 0.9147549271583557, 0.9191002249717712, 0.9177905917167664, 0.9320017099380493, 0.9262363314628601, 0.9254418611526489, 0.9174361824989319, 0.9262288808822632, 0.9165323972702026, 0.9112743735313416, 0.9176955819129944, 0.9269324541091919, 0.9193789958953857, 0.928321361541748, 0.9188025593757629, 0.9189277291297913, 0.9323211312294006, 0.9231060743331909, 0.9285557270050049, 0.9311618804931641, 0.9131585359573364, 0.925273060798645, 0.9380617141723633, 0.9242725372314453, 0.9197211861610413, 0.9291391372680664, 0.9374681711196899, 0.9340976476669312, 0.9297300577163696, 0.9395886063575745, 0.9364591836929321, 0.9425839781761169, 0.9301185011863708, 0.9479553699493408, 0.9521031975746155, 0.9411022663116455, 0.9516761898994446, 0.9429436922073364, 0.9436061382293701, 0.9470922350883484, 0.9421090483665466, 0.9366302490234375, 0.9541658163070679, 0.9442183971405029, 0.951880156993866, 0.9500471353530884, 0.9469921588897705, 0.9479880928993225, 0.95838463306427, 0.9596530795097351, 0.9666554927825928, 0.9559325575828552, 0.9543641805648804, 0.9590440988540649, 0.9515613317489624, 0.954136312007904, 0.9612745642662048, 0.957806408405304, 0.9504889845848083, 0.9641535878181458, 0.9749866127967834, 0.9727059602737427, 0.9713268876075745, 0.96901535987854, 0.9774088859558105, 0.974469780921936, 0.9800930619239807, 0.9705524444580078, 0.9909284114837646, 0.9778116941452026, 0.9707061052322388, 0.9845857620239258, 0.9793461561203003, 0.9753221273422241, 0.9703169465065002, 0.9783474802970886, 0.9699639678001404, 0.9682096242904663, 0.9845386147499084, 0.9953175187110901, 0.9750789403915405, 0.9904685616493225, 1.0053563117980957, 0.9776285290718079, 0.979621410369873, 0.9867031574249268, 0.9760859608650208, 0.995668888092041, 0.9901520609855652, 0.9852127432823181, 0.9863247871398926, 0.9701947569847107, 0.993866503238678, 0.9954051971435547, 0.9758404493331909, 0.9816854000091553, 0.9863696694374084, 0.9959462881088257, 0.9996899366378784, 0.990037739276886, 0.9806967973709106, 1.0043213367462158, 1.0031933784484863, 1.0051848888397217, 0.985309362411499, 0.9807881116867065, 0.9951239228248596, 1.0103398561477661, 0.9944761991500854, 0.9965275526046753, 1.004286527633667, 1.0077108144760132, 0.9978950023651123, 1.010254144668579, 1.0020772218704224, 0.9841927886009216, 0.9897237420082092, 0.987730860710144, 1.0059802532196045, 0.9975306391716003, 1.006778597831726, 0.9789921641349792, 1.0151787996292114, 1.0075290203094482, 0.9841210246086121, 0.9998483061790466, 0.9820907711982727, 0.9854351878166199, 1.007310390472412, 1.0007922649383545, 1.0014084577560425, 0.9952652454376221], 'identifier': '8771755np'}