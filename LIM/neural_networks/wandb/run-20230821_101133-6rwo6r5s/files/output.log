
 19%|██████████████████▌                                                                               | 19/100 [00:01<00:06, 12.57it/s, loss_test=1.094]
Epoch: 00, Training Loss: 0.9950, Test Loss: 1.0855
Epoch: 01, Training Loss: 0.9913, Test Loss: 1.0711
Epoch: 02, Training Loss: 0.9880, Test Loss: 1.0800
Epoch: 03, Training Loss: 0.9924, Test Loss: 1.0901
Epoch: 04, Training Loss: 0.9865, Test Loss: 1.0869
Epoch: 05, Training Loss: 0.9878, Test Loss: 1.0883
Epoch: 06, Training Loss: 0.9941, Test Loss: 1.0918
Epoch: 07, Training Loss: 0.9877, Test Loss: 1.0916
Epoch: 08, Training Loss: 0.9908, Test Loss: 1.0877
Epoch: 09, Training Loss: 0.9869, Test Loss: 1.0873
Epoch: 10, Training Loss: 0.9864, Test Loss: 1.0915
Epoch: 11, Training Loss: 0.9879, Test Loss: 1.0924
Epoch: 12, Training Loss: 0.9899, Test Loss: 1.0808
Epoch: 13, Training Loss: 0.9883, Test Loss: 1.0827
Epoch: 14, Training Loss: 0.9834, Test Loss: 1.1015
Epoch: 15, Training Loss: 0.9918, Test Loss: 1.0935
Epoch: 16, Training Loss: 0.9877, Test Loss: 1.0660
Epoch: 17, Training Loss: 0.9900, Test Loss: 1.0582
Epoch: 18, Training Loss: 0.9905, Test Loss: 1.0793

 45%|████████████████████████████████████████████                                                      | 45/100 [00:03<00:04, 12.13it/s, loss_test=0.985]
Epoch: 20, Training Loss: 0.9847, Test Loss: 1.1078
Epoch: 21, Training Loss: 0.9861, Test Loss: 1.0852
Epoch: 22, Training Loss: 0.9858, Test Loss: 1.0673
Epoch: 23, Training Loss: 0.9861, Test Loss: 1.0732
Epoch: 24, Training Loss: 0.9773, Test Loss: 1.0823
Epoch: 25, Training Loss: 0.9794, Test Loss: 1.0716
Epoch: 26, Training Loss: 0.9808, Test Loss: 1.0834
Epoch: 27, Training Loss: 0.9836, Test Loss: 1.0624
Epoch: 28, Training Loss: 0.9746, Test Loss: 1.0739
Epoch: 29, Training Loss: 0.9737, Test Loss: 1.0698
Epoch: 30, Training Loss: 0.9734, Test Loss: 1.0734
Epoch: 31, Training Loss: 0.9649, Test Loss: 1.0539
Epoch: 32, Training Loss: 0.9588, Test Loss: 1.0650
Epoch: 33, Training Loss: 0.9531, Test Loss: 1.0508
Epoch: 34, Training Loss: 0.9427, Test Loss: 1.0350
Epoch: 35, Training Loss: 0.9319, Test Loss: 1.0335
Epoch: 36, Training Loss: 0.9257, Test Loss: 1.0097
Epoch: 37, Training Loss: 0.9176, Test Loss: 1.0286
Epoch: 38, Training Loss: 0.9117, Test Loss: 0.9998
Epoch: 39, Training Loss: 0.9064, Test Loss: 0.9893
Epoch: 40, Training Loss: 0.9035, Test Loss: 0.9876
Epoch: 41, Training Loss: 0.8944, Test Loss: 0.9862
Epoch: 42, Training Loss: 0.8965, Test Loss: 0.9822
Epoch: 43, Training Loss: 0.8925, Test Loss: 0.9900

 69%|███████████████████████████████████████████████████████████████████▌                              | 69/100 [00:05<00:02, 12.14it/s, loss_test=0.935]
Epoch: 45, Training Loss: 0.8893, Test Loss: 0.9825
Epoch: 46, Training Loss: 0.8847, Test Loss: 0.9741
Epoch: 47, Training Loss: 0.8793, Test Loss: 0.9786
Epoch: 48, Training Loss: 0.8769, Test Loss: 0.9741
Epoch: 49, Training Loss: 0.8741, Test Loss: 0.9739
Epoch: 50, Training Loss: 0.8703, Test Loss: 0.9494
Epoch: 51, Training Loss: 0.8659, Test Loss: 0.9727
Epoch: 52, Training Loss: 0.8665, Test Loss: 0.9527
Epoch: 53, Training Loss: 0.8612, Test Loss: 0.9486
Epoch: 54, Training Loss: 0.8564, Test Loss: 0.9574
Epoch: 55, Training Loss: 0.8489, Test Loss: 0.9449
Epoch: 56, Training Loss: 0.8503, Test Loss: 0.9650
Epoch: 57, Training Loss: 0.8451, Test Loss: 0.9407
Epoch: 58, Training Loss: 0.8409, Test Loss: 0.9332
Epoch: 59, Training Loss: 0.8449, Test Loss: 0.9401
Epoch: 60, Training Loss: 0.8398, Test Loss: 0.9200
Epoch: 61, Training Loss: 0.8344, Test Loss: 0.9452
Epoch: 62, Training Loss: 0.8337, Test Loss: 0.9235
Epoch: 63, Training Loss: 0.8273, Test Loss: 0.9412
Epoch: 64, Training Loss: 0.8255, Test Loss: 0.9349
Epoch: 65, Training Loss: 0.8232, Test Loss: 0.9288
Epoch: 66, Training Loss: 0.8238, Test Loss: 0.9361
Epoch: 67, Training Loss: 0.8183, Test Loss: 0.9198

 95%|█████████████████████████████████████████████████████████████████████████████████████████████     | 95/100 [00:07<00:00, 12.67it/s, loss_test=0.897]
Epoch: 69, Training Loss: 0.8099, Test Loss: 0.9347
Epoch: 70, Training Loss: 0.8093, Test Loss: 0.9170
Epoch: 71, Training Loss: 0.8028, Test Loss: 0.9238
Epoch: 72, Training Loss: 0.8006, Test Loss: 0.9100
Epoch: 73, Training Loss: 0.7965, Test Loss: 0.8985
Epoch: 74, Training Loss: 0.7917, Test Loss: 0.9048
Epoch: 75, Training Loss: 0.7887, Test Loss: 0.9172
Epoch: 76, Training Loss: 0.7832, Test Loss: 0.9099
Epoch: 77, Training Loss: 0.7798, Test Loss: 0.9100
Epoch: 78, Training Loss: 0.7753, Test Loss: 0.9250
Epoch: 79, Training Loss: 0.7751, Test Loss: 0.9124
Epoch: 80, Training Loss: 0.7698, Test Loss: 0.9034
Epoch: 81, Training Loss: 0.7674, Test Loss: 0.9227
Epoch: 82, Training Loss: 0.7595, Test Loss: 0.8896
Epoch: 83, Training Loss: 0.7543, Test Loss: 0.8974
Epoch: 84, Training Loss: 0.7492, Test Loss: 0.8973
Epoch: 85, Training Loss: 0.7452, Test Loss: 0.8992
Epoch: 86, Training Loss: 0.7432, Test Loss: 0.9037
Epoch: 87, Training Loss: 0.7366, Test Loss: 0.8999
Epoch: 88, Training Loss: 0.7336, Test Loss: 0.9024
Epoch: 89, Training Loss: 0.7281, Test Loss: 0.9108
Epoch: 90, Training Loss: 0.7259, Test Loss: 0.8918
Epoch: 91, Training Loss: 0.7220, Test Loss: 0.8890
Epoch: 92, Training Loss: 0.7173, Test Loss: 0.8891
Epoch: 93, Training Loss: 0.7115, Test Loss: 0.8916

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.32it/s, loss_test=0.900]
Epoch: 95, Training Loss: 0.7048, Test Loss: 0.8887
Epoch: 96, Training Loss: 0.7015, Test Loss: 0.8863
Epoch: 97, Training Loss: 0.6985, Test Loss: 0.8989
Epoch: 98, Training Loss: 0.6964, Test Loss: 0.8971
Epoch: 99, Training Loss: 0.6908, Test Loss: 0.8999
Model saved as model_8063970np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-12180000-8063970np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 100, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9949603796005249, 0.9913156986236572, 0.9880403399467468, 0.9924420952796936, 0.9865433692932128, 0.9877827525138855, 0.9941305637359619, 0.9876877069473267, 0.9907572031021118, 0.9868640899658203, 0.9864234089851379, 0.9878831386566163, 0.989932382106781, 0.9883294820785522, 0.9834311246871948, 0.9918257594108582, 0.9877058386802673, 0.990009891986847, 0.9904939532279968, 0.9871940612792969, 0.9847440004348755, 0.986066460609436, 0.9857552766799926, 0.9860944986343384, 0.977328085899353, 0.9794047951698304, 0.9807706475257874, 0.9836495041847229, 0.9745754599571228, 0.9737257719039917, 0.9733510136604309, 0.9649317741394043, 0.9587749719619751, 0.9531340599060059, 0.9427451729774475, 0.931941282749176, 0.9257084131240845, 0.9176499724388123, 0.911703372001648, 0.9063943028450012, 0.9034652590751648, 0.8943621635437011, 0.8964998722076416, 0.8924945831298828, 0.8901464581489563, 0.8893113493919372, 0.8847484230995178, 0.879317843914032, 0.8769254803657531, 0.8741399645805359, 0.8702907204627991, 0.8659467577934266, 0.8665109395980835, 0.8612334251403808, 0.856419312953949, 0.8489018678665161, 0.8502516746520996, 0.8450999736785889, 0.840886116027832, 0.8449040055274963, 0.8397592186927796, 0.8343782424926758, 0.8337173104286194, 0.8273234605789185, 0.8255364894866943, 0.823225462436676, 0.8238381147384644, 0.8183159112930298, 0.8105747699737549, 0.8099080085754394, 0.8093166708946228, 0.8027565836906433, 0.8005804181098938, 0.796541178226471, 0.7916609764099121, 0.7887246608734131, 0.7831968545913697, 0.7797629117965699, 0.7753407478332519, 0.775145161151886, 0.7698142528533936, 0.7674020528793335, 0.7595132946968078, 0.754323399066925, 0.7492423892021179, 0.7451869010925293, 0.7431564927101135, 0.7365846276283264, 0.7336392283439637, 0.7281174659729004, 0.7259072661399841, 0.7220396876335144, 0.7173086762428283, 0.7115087270736694, 0.7080188512802124, 0.7048363208770752, 0.7015133380889893, 0.6984720945358276, 0.6963611364364624, 0.6908185839653015], 'loss_test': [1.0854606628417969, 1.071135401725769, 1.079994797706604, 1.0901134014129639, 1.0869311094284058, 1.088268518447876, 1.091809630393982, 1.0915555953979492, 1.0877312421798706, 1.0872693061828613, 1.0915172100067139, 1.0924211740493774, 1.0807846784591675, 1.0827058553695679, 1.1015344858169556, 1.0935472249984741, 1.0660405158996582, 1.0581907033920288, 1.0793321132659912, 1.0939756631851196, 1.107840895652771, 1.0852454900741577, 1.0673469305038452, 1.0731903314590454, 1.0823436975479126, 1.0716185569763184, 1.0834109783172607, 1.062397837638855, 1.0739156007766724, 1.0697637796401978, 1.073431134223938, 1.0538983345031738, 1.0649930238723755, 1.0508300065994263, 1.0349534749984741, 1.0334956645965576, 1.0096540451049805, 1.0285813808441162, 0.9998082518577576, 0.9892501831054688, 0.9875802397727966, 0.986192524433136, 0.9822355508804321, 0.9899606704711914, 0.9845959544181824, 0.9824572205543518, 0.9740586280822754, 0.9786497354507446, 0.974129855632782, 0.9738893508911133, 0.9494242668151855, 0.9726754426956177, 0.9526914954185486, 0.9485862851142883, 0.9573919773101807, 0.9448655843734741, 0.9650486707687378, 0.9407404661178589, 0.9332085847854614, 0.9401123523712158, 0.9199696183204651, 0.9452104568481445, 0.923518717288971, 0.9412472248077393, 0.9348673820495605, 0.9288134574890137, 0.9360601305961609, 0.9197654724121094, 0.9214967489242554, 0.9346632361412048, 0.9170019030570984, 0.9238077998161316, 0.9099596738815308, 0.8984768390655518, 0.904827892780304, 0.9171658158302307, 0.9099498391151428, 0.909970223903656, 0.9249827265739441, 0.9124398231506348, 0.9034411907196045, 0.9227206707000732, 0.8896369338035583, 0.897357702255249, 0.8972969055175781, 0.8991740345954895, 0.9036809802055359, 0.8999334573745728, 0.9024113416671753, 0.9107702970504761, 0.891801118850708, 0.8889878988265991, 0.8891255259513855, 0.891583263874054, 0.8968058228492737, 0.888672411441803, 0.8862621784210205, 0.898899495601654, 0.8971107602119446, 0.8999471068382263], 'identifier': '8063970np'}