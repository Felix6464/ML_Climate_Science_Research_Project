
  8%|███████▊                                                                                          | 20/250 [00:01<00:19, 11.65it/s, loss_test=1.095]
Epoch: 00, Training Loss: 0.9960, Test Loss: 1.0776
Epoch: 01, Training Loss: 0.9944, Test Loss: 1.0796
Epoch: 02, Training Loss: 1.0005, Test Loss: 1.0773
Epoch: 03, Training Loss: 0.9965, Test Loss: 1.0940
Epoch: 04, Training Loss: 0.9987, Test Loss: 1.0879
Epoch: 05, Training Loss: 0.9940, Test Loss: 1.0802
Epoch: 06, Training Loss: 0.9948, Test Loss: 1.0845
Epoch: 07, Training Loss: 0.9915, Test Loss: 1.0846
Epoch: 08, Training Loss: 0.9951, Test Loss: 1.0640
Epoch: 09, Training Loss: 0.9888, Test Loss: 1.0825
Epoch: 10, Training Loss: 0.9948, Test Loss: 1.0655
Epoch: 11, Training Loss: 0.9938, Test Loss: 1.0821
Epoch: 12, Training Loss: 0.9890, Test Loss: 1.0702
Epoch: 13, Training Loss: 0.9881, Test Loss: 1.0754
Epoch: 14, Training Loss: 0.9898, Test Loss: 1.0850
Epoch: 15, Training Loss: 0.9905, Test Loss: 1.0944
Epoch: 16, Training Loss: 0.9865, Test Loss: 1.0794
Epoch: 17, Training Loss: 0.9927, Test Loss: 1.0894
Epoch: 18, Training Loss: 0.9883, Test Loss: 1.0823
Epoch: 19, Training Loss: 0.9832, Test Loss: 1.0798

 18%|██████████████████                                                                                | 46/250 [00:03<00:16, 12.43it/s, loss_test=1.007]
Epoch: 21, Training Loss: 0.9878, Test Loss: 1.0857
Epoch: 22, Training Loss: 0.9794, Test Loss: 1.0774
Epoch: 23, Training Loss: 0.9807, Test Loss: 1.0679
Epoch: 24, Training Loss: 0.9873, Test Loss: 1.0972
Epoch: 25, Training Loss: 0.9809, Test Loss: 1.0878
Epoch: 26, Training Loss: 0.9796, Test Loss: 1.0908
Epoch: 27, Training Loss: 0.9829, Test Loss: 1.0952
Epoch: 28, Training Loss: 0.9828, Test Loss: 1.0684
Epoch: 29, Training Loss: 0.9820, Test Loss: 1.0752
Epoch: 30, Training Loss: 0.9769, Test Loss: 1.0855
Epoch: 31, Training Loss: 0.9777, Test Loss: 1.0666
Epoch: 32, Training Loss: 0.9703, Test Loss: 1.0782
Epoch: 33, Training Loss: 0.9731, Test Loss: 1.0631
Epoch: 34, Training Loss: 0.9634, Test Loss: 1.0779
Epoch: 35, Training Loss: 0.9552, Test Loss: 1.0645
Epoch: 36, Training Loss: 0.9476, Test Loss: 1.0605
Epoch: 37, Training Loss: 0.9440, Test Loss: 1.0588
Epoch: 38, Training Loss: 0.9326, Test Loss: 1.0521
Epoch: 39, Training Loss: 0.9227, Test Loss: 1.0341
Epoch: 40, Training Loss: 0.9195, Test Loss: 1.0391
Epoch: 41, Training Loss: 0.9146, Test Loss: 1.0374
Epoch: 42, Training Loss: 0.9097, Test Loss: 1.0120
Epoch: 43, Training Loss: 0.9020, Test Loss: 1.0212
Epoch: 44, Training Loss: 0.8997, Test Loss: 1.0075

 28%|███████████████████████████▍                                                                      | 70/250 [00:05<00:14, 12.26it/s, loss_test=0.915]
Epoch: 46, Training Loss: 0.8876, Test Loss: 1.0119
Epoch: 47, Training Loss: 0.8807, Test Loss: 1.0020
Epoch: 48, Training Loss: 0.8754, Test Loss: 0.9995
Epoch: 49, Training Loss: 0.8709, Test Loss: 0.9861
Epoch: 50, Training Loss: 0.8654, Test Loss: 0.9692
Epoch: 51, Training Loss: 0.8587, Test Loss: 0.9838
Epoch: 52, Training Loss: 0.8562, Test Loss: 0.9803
Epoch: 53, Training Loss: 0.8474, Test Loss: 0.9757
Epoch: 54, Training Loss: 0.8418, Test Loss: 0.9585
Epoch: 55, Training Loss: 0.8383, Test Loss: 0.9687
Epoch: 56, Training Loss: 0.8317, Test Loss: 0.9722
Epoch: 57, Training Loss: 0.8285, Test Loss: 0.9678
Epoch: 58, Training Loss: 0.8187, Test Loss: 0.9463
Epoch: 59, Training Loss: 0.8176, Test Loss: 0.9339
Epoch: 60, Training Loss: 0.8048, Test Loss: 0.9376
Epoch: 61, Training Loss: 0.8011, Test Loss: 0.9322
Epoch: 62, Training Loss: 0.7967, Test Loss: 0.9282
Epoch: 63, Training Loss: 0.7910, Test Loss: 0.9485
Epoch: 64, Training Loss: 0.7862, Test Loss: 0.9336
Epoch: 65, Training Loss: 0.7839, Test Loss: 0.9214
Epoch: 66, Training Loss: 0.7784, Test Loss: 0.9159
Epoch: 67, Training Loss: 0.7750, Test Loss: 0.9211
Epoch: 68, Training Loss: 0.7665, Test Loss: 0.9293
Epoch: 69, Training Loss: 0.7635, Test Loss: 0.9169

 38%|█████████████████████████████████████▋                                                            | 96/250 [00:07<00:12, 12.54it/s, loss_test=0.908]
Epoch: 71, Training Loss: 0.7530, Test Loss: 0.9155
Epoch: 72, Training Loss: 0.7474, Test Loss: 0.9132
Epoch: 73, Training Loss: 0.7428, Test Loss: 0.9063
Epoch: 74, Training Loss: 0.7372, Test Loss: 0.9198
Epoch: 75, Training Loss: 0.7351, Test Loss: 0.9287
Epoch: 76, Training Loss: 0.7282, Test Loss: 0.9053
Epoch: 77, Training Loss: 0.7238, Test Loss: 0.9285
Epoch: 78, Training Loss: 0.7190, Test Loss: 0.9067
Epoch: 79, Training Loss: 0.7180, Test Loss: 0.8970
Epoch: 80, Training Loss: 0.7105, Test Loss: 0.9078
Epoch: 81, Training Loss: 0.7038, Test Loss: 0.9085
Epoch: 82, Training Loss: 0.7039, Test Loss: 0.9135
Epoch: 83, Training Loss: 0.7011, Test Loss: 0.9100
Epoch: 84, Training Loss: 0.6967, Test Loss: 0.9024
Epoch: 85, Training Loss: 0.6916, Test Loss: 0.9003
Epoch: 86, Training Loss: 0.6870, Test Loss: 0.9077
Epoch: 87, Training Loss: 0.6849, Test Loss: 0.9127
Epoch: 88, Training Loss: 0.6789, Test Loss: 0.9076
Epoch: 89, Training Loss: 0.6769, Test Loss: 0.9051
Epoch: 90, Training Loss: 0.6749, Test Loss: 0.9073
Epoch: 91, Training Loss: 0.6712, Test Loss: 0.9097
Epoch: 92, Training Loss: 0.6648, Test Loss: 0.9180
Epoch: 93, Training Loss: 0.6623, Test Loss: 0.9105
Epoch: 94, Training Loss: 0.6574, Test Loss: 0.8933

 48%|██████████████████████████████████████████████▌                                                  | 120/250 [00:09<00:10, 11.85it/s, loss_test=0.895]
Epoch: 96, Training Loss: 0.6530, Test Loss: 0.9070
Epoch: 97, Training Loss: 0.6472, Test Loss: 0.9086
Epoch: 98, Training Loss: 0.6456, Test Loss: 0.9092
Epoch: 99, Training Loss: 0.6404, Test Loss: 0.9014
Epoch: 100, Training Loss: 0.6379, Test Loss: 0.9034
Epoch: 101, Training Loss: 0.6343, Test Loss: 0.8971
Epoch: 102, Training Loss: 0.6333, Test Loss: 0.9073
Epoch: 103, Training Loss: 0.6270, Test Loss: 0.9040
Epoch: 104, Training Loss: 0.6271, Test Loss: 0.9103
Epoch: 105, Training Loss: 0.6214, Test Loss: 0.9195
Epoch: 106, Training Loss: 0.6201, Test Loss: 0.9039
Epoch: 107, Training Loss: 0.6172, Test Loss: 0.9093
Epoch: 108, Training Loss: 0.6112, Test Loss: 0.9108
Epoch: 109, Training Loss: 0.6113, Test Loss: 0.9007
Epoch: 110, Training Loss: 0.6058, Test Loss: 0.8946
Epoch: 111, Training Loss: 0.6037, Test Loss: 0.9010
Epoch: 112, Training Loss: 0.6004, Test Loss: 0.9023
Epoch: 113, Training Loss: 0.5967, Test Loss: 0.9060
Epoch: 114, Training Loss: 0.5965, Test Loss: 0.9045
Epoch: 115, Training Loss: 0.5904, Test Loss: 0.9068
Epoch: 116, Training Loss: 0.5894, Test Loss: 0.9053
Epoch: 117, Training Loss: 0.5863, Test Loss: 0.9106
Epoch: 118, Training Loss: 0.5849, Test Loss: 0.8942
Epoch: 119, Training Loss: 0.5811, Test Loss: 0.9086

 58%|████████████████████████████████████████████████████████▋                                        | 146/250 [00:11<00:08, 12.13it/s, loss_test=0.929]
Epoch: 121, Training Loss: 0.5740, Test Loss: 0.9038
Epoch: 122, Training Loss: 0.5717, Test Loss: 0.9179
Epoch: 123, Training Loss: 0.5696, Test Loss: 0.9062
Epoch: 124, Training Loss: 0.5652, Test Loss: 0.9153
Epoch: 125, Training Loss: 0.5619, Test Loss: 0.9028
Epoch: 126, Training Loss: 0.5589, Test Loss: 0.9144
Epoch: 127, Training Loss: 0.5557, Test Loss: 0.9115
Epoch: 128, Training Loss: 0.5560, Test Loss: 0.9171
Epoch: 129, Training Loss: 0.5542, Test Loss: 0.9168
Epoch: 130, Training Loss: 0.5495, Test Loss: 0.9108
Epoch: 131, Training Loss: 0.5452, Test Loss: 0.9172
Epoch: 132, Training Loss: 0.5440, Test Loss: 0.9124
Epoch: 133, Training Loss: 0.5393, Test Loss: 0.9168
Epoch: 134, Training Loss: 0.5403, Test Loss: 0.9158
Epoch: 135, Training Loss: 0.5347, Test Loss: 0.9206
Epoch: 136, Training Loss: 0.5330, Test Loss: 0.9174
Epoch: 137, Training Loss: 0.5298, Test Loss: 0.9162
Epoch: 138, Training Loss: 0.5263, Test Loss: 0.9098
Epoch: 139, Training Loss: 0.5244, Test Loss: 0.9247
Epoch: 140, Training Loss: 0.5235, Test Loss: 0.8965
Epoch: 141, Training Loss: 0.5177, Test Loss: 0.9148
Epoch: 142, Training Loss: 0.5168, Test Loss: 0.9197
Epoch: 143, Training Loss: 0.5160, Test Loss: 0.9271

 68%|█████████████████████████████████████████████████████████████████▉                               | 170/250 [00:13<00:06, 12.13it/s, loss_test=0.934]
Epoch: 145, Training Loss: 0.5089, Test Loss: 0.9291
Epoch: 146, Training Loss: 0.5048, Test Loss: 0.9188
Epoch: 147, Training Loss: 0.5066, Test Loss: 0.9265
Epoch: 148, Training Loss: 0.5042, Test Loss: 0.9300
Epoch: 149, Training Loss: 0.4988, Test Loss: 0.9169
Epoch: 150, Training Loss: 0.4948, Test Loss: 0.9308
Epoch: 151, Training Loss: 0.4935, Test Loss: 0.9181
Epoch: 152, Training Loss: 0.4924, Test Loss: 0.9386
Epoch: 153, Training Loss: 0.4896, Test Loss: 0.9239
Epoch: 154, Training Loss: 0.4899, Test Loss: 0.9129
Epoch: 155, Training Loss: 0.4857, Test Loss: 0.9296
Epoch: 156, Training Loss: 0.4815, Test Loss: 0.9237
Epoch: 157, Training Loss: 0.4773, Test Loss: 0.9303
Epoch: 158, Training Loss: 0.4765, Test Loss: 0.9208
Epoch: 159, Training Loss: 0.4740, Test Loss: 0.9262
Epoch: 160, Training Loss: 0.4725, Test Loss: 0.9283
Epoch: 161, Training Loss: 0.4695, Test Loss: 0.9184
Epoch: 162, Training Loss: 0.4671, Test Loss: 0.9445
Epoch: 163, Training Loss: 0.4663, Test Loss: 0.9338
Epoch: 164, Training Loss: 0.4623, Test Loss: 0.9452
Epoch: 165, Training Loss: 0.4589, Test Loss: 0.9271
Epoch: 166, Training Loss: 0.4578, Test Loss: 0.9355
Epoch: 167, Training Loss: 0.4576, Test Loss: 0.9245
Epoch: 168, Training Loss: 0.4540, Test Loss: 0.9272

 78%|███████████████████████████████████████████████████████████████████████████▎                     | 194/250 [00:15<00:04, 11.94it/s, loss_test=0.943]
Epoch: 170, Training Loss: 0.4485, Test Loss: 0.9286
Epoch: 171, Training Loss: 0.4455, Test Loss: 0.9327
Epoch: 172, Training Loss: 0.4435, Test Loss: 0.9332
Epoch: 173, Training Loss: 0.4443, Test Loss: 0.9431
Epoch: 174, Training Loss: 0.4388, Test Loss: 0.9287
Epoch: 175, Training Loss: 0.4380, Test Loss: 0.9418
Epoch: 176, Training Loss: 0.4354, Test Loss: 0.9363
Epoch: 177, Training Loss: 0.4333, Test Loss: 0.9373
Epoch: 178, Training Loss: 0.4309, Test Loss: 0.9418
Epoch: 179, Training Loss: 0.4296, Test Loss: 0.9383
Epoch: 180, Training Loss: 0.4253, Test Loss: 0.9384
Epoch: 181, Training Loss: 0.4244, Test Loss: 0.9559
Epoch: 182, Training Loss: 0.4231, Test Loss: 0.9477
Epoch: 183, Training Loss: 0.4172, Test Loss: 0.9390
Epoch: 184, Training Loss: 0.4166, Test Loss: 0.9491
Epoch: 185, Training Loss: 0.4155, Test Loss: 0.9624
Epoch: 186, Training Loss: 0.4139, Test Loss: 0.9466
Epoch: 187, Training Loss: 0.4119, Test Loss: 0.9512
Epoch: 188, Training Loss: 0.4101, Test Loss: 0.9483
Epoch: 189, Training Loss: 0.4069, Test Loss: 0.9542
Epoch: 190, Training Loss: 0.4058, Test Loss: 0.9534
Epoch: 191, Training Loss: 0.4021, Test Loss: 0.9499
Epoch: 192, Training Loss: 0.3998, Test Loss: 0.9566

 87%|████████████████████████████████████████████████████████████████████████████████████▌            | 218/250 [00:17<00:02, 11.73it/s, loss_test=0.982]
Epoch: 194, Training Loss: 0.3968, Test Loss: 0.9425
Epoch: 195, Training Loss: 0.3925, Test Loss: 0.9520
Epoch: 196, Training Loss: 0.3936, Test Loss: 0.9542
Epoch: 197, Training Loss: 0.3908, Test Loss: 0.9487
Epoch: 198, Training Loss: 0.3882, Test Loss: 0.9544
Epoch: 199, Training Loss: 0.3867, Test Loss: 0.9572
Epoch: 200, Training Loss: 0.3840, Test Loss: 0.9650
Epoch: 201, Training Loss: 0.3814, Test Loss: 0.9574
Epoch: 202, Training Loss: 0.3802, Test Loss: 0.9628
Epoch: 203, Training Loss: 0.3797, Test Loss: 0.9645
Epoch: 204, Training Loss: 0.3767, Test Loss: 0.9584
Epoch: 205, Training Loss: 0.3759, Test Loss: 0.9578
Epoch: 206, Training Loss: 0.3736, Test Loss: 0.9621
Epoch: 207, Training Loss: 0.3696, Test Loss: 0.9637
Epoch: 208, Training Loss: 0.3689, Test Loss: 0.9687
Epoch: 209, Training Loss: 0.3689, Test Loss: 0.9739
Epoch: 210, Training Loss: 0.3656, Test Loss: 0.9744
Epoch: 211, Training Loss: 0.3641, Test Loss: 0.9684
Epoch: 212, Training Loss: 0.3611, Test Loss: 0.9687
Epoch: 213, Training Loss: 0.3603, Test Loss: 0.9693
Epoch: 214, Training Loss: 0.3570, Test Loss: 0.9669
Epoch: 215, Training Loss: 0.3565, Test Loss: 0.9679

 97%|█████████████████████████████████████████████████████████████████████████████████████████████▉   | 242/250 [00:19<00:00, 12.29it/s, loss_test=0.994]
Epoch: 217, Training Loss: 0.3530, Test Loss: 0.9821
Epoch: 218, Training Loss: 0.3512, Test Loss: 0.9704
Epoch: 219, Training Loss: 0.3496, Test Loss: 0.9720
Epoch: 220, Training Loss: 0.3483, Test Loss: 0.9683
Epoch: 221, Training Loss: 0.3454, Test Loss: 0.9715
Epoch: 222, Training Loss: 0.3429, Test Loss: 0.9805
Epoch: 223, Training Loss: 0.3427, Test Loss: 0.9844
Epoch: 224, Training Loss: 0.3410, Test Loss: 0.9840
Epoch: 225, Training Loss: 0.3400, Test Loss: 0.9802
Epoch: 226, Training Loss: 0.3368, Test Loss: 0.9752
Epoch: 227, Training Loss: 0.3363, Test Loss: 0.9724
Epoch: 228, Training Loss: 0.3346, Test Loss: 0.9745
Epoch: 229, Training Loss: 0.3327, Test Loss: 0.9923
Epoch: 230, Training Loss: 0.3313, Test Loss: 0.9791
Epoch: 231, Training Loss: 0.3303, Test Loss: 0.9865
Epoch: 232, Training Loss: 0.3268, Test Loss: 0.9874
Epoch: 233, Training Loss: 0.3274, Test Loss: 1.0049
Epoch: 234, Training Loss: 0.3252, Test Loss: 0.9915
Epoch: 235, Training Loss: 0.3236, Test Loss: 0.9900
Epoch: 236, Training Loss: 0.3220, Test Loss: 0.9799
Epoch: 237, Training Loss: 0.3212, Test Loss: 0.9767
Epoch: 238, Training Loss: 0.3198, Test Loss: 1.0014
Epoch: 239, Training Loss: 0.3165, Test Loss: 0.9719
Epoch: 240, Training Loss: 0.3156, Test Loss: 0.9950

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.20it/s, loss_test=1.007]
Epoch: 242, Training Loss: 0.3131, Test Loss: 0.9937
Epoch: 243, Training Loss: 0.3116, Test Loss: 0.9963
Epoch: 244, Training Loss: 0.3106, Test Loss: 0.9986
Epoch: 245, Training Loss: 0.3078, Test Loss: 0.9985
Epoch: 246, Training Loss: 0.3074, Test Loss: 0.9837
Epoch: 247, Training Loss: 0.3054, Test Loss: 0.9971
Epoch: 248, Training Loss: 0.3047, Test Loss: 0.9947
Epoch: 249, Training Loss: 0.3027, Test Loss: 1.0071
Model saved as model_9915537np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-12150000-9915537np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9960259914398193, 0.9944348573684693, 1.0004939198493958, 0.9965215921401978, 0.9987288117408752, 0.9940284848213196, 0.9947763085365295, 0.9915221571922302, 0.9950747847557068, 0.9888338446617126, 0.9948432445526123, 0.9938349962234497, 0.9889812707901001, 0.9880651950836181, 0.9897783517837524, 0.9905117154121399, 0.986456549167633, 0.9927093982696533, 0.988344156742096, 0.9831959009170532, 0.9867026090621949, 0.9878168940544129, 0.9793723940849304, 0.9806541323661804, 0.9873417854309082, 0.9808577537536621, 0.9795897841453552, 0.9828972816467285, 0.982838237285614, 0.981963038444519, 0.9768810391426086, 0.9777268409729004, 0.9702635407447815, 0.9730623722076416, 0.9634217381477356, 0.955219829082489, 0.9476402521133422, 0.9439630746841431, 0.9325667500495911, 0.9226658582687378, 0.9195422291755676, 0.9145691990852356, 0.9096829533576966, 0.9019643545150757, 0.8996636271476746, 0.8899408221244812, 0.8875728964805603, 0.8807480573654175, 0.8753552198410034, 0.8708580255508422, 0.8654064655303955, 0.8587005257606506, 0.856184184551239, 0.8474377274513245, 0.841794216632843, 0.8383153319358826, 0.8316621899604797, 0.8285135507583619, 0.818654453754425, 0.8175896883010865, 0.8047751784324646, 0.8010818243026734, 0.7967081308364868, 0.7910111427307129, 0.7862073898315429, 0.7838800191879273, 0.7783624410629273, 0.7750287055969238, 0.766451358795166, 0.7635125398635865, 0.7559040188789368, 0.7529712796211243, 0.7474464535713196, 0.7428465843200683, 0.7372054100036621, 0.7350576519966125, 0.7282379031181335, 0.7237988352775574, 0.7189582228660584, 0.7180339932441712, 0.7105404257774353, 0.7038387537002564, 0.7038897275924683, 0.7011421084403991, 0.6966763496398926, 0.6915927767753601, 0.6869710326194763, 0.684926974773407, 0.6788592100143432, 0.6769049286842346, 0.6748663783073425, 0.6712343454360962, 0.6648282766342163, 0.662315309047699, 0.6574427843093872, 0.6560060501098632, 0.6529772877693176, 0.6472453355789185, 0.6456077933311463, 0.6403561592102051, 0.6379200696945191, 0.6343415141105652, 0.6332801103591919, 0.6270321130752563, 0.6270612716674805, 0.6213528275489807, 0.6200673699378967, 0.6171609878540039, 0.6112071514129639, 0.6112717032432556, 0.6058036804199218, 0.6036644577980042, 0.6003680109977723, 0.5967015504837037, 0.5965248823165894, 0.59041348695755, 0.5894198417663574, 0.586343252658844, 0.5848661422729492, 0.5810515642166137, 0.5768975377082824, 0.5739758133888244, 0.5716666579246521, 0.5695624113082886, 0.5652192592620849, 0.5619455099105835, 0.558905303478241, 0.5556690931320191, 0.5560003757476807, 0.5541937112808227, 0.5495002388954162, 0.5452278256416321, 0.5440211772918702, 0.5393016934394836, 0.5403427243232727, 0.5347177743911743, 0.5329779982566833, 0.5297583222389222, 0.5262986660003662, 0.5244085669517518, 0.523500633239746, 0.5177149593830108, 0.5168056726455689, 0.515965735912323, 0.5114379942417144, 0.5089086711406707, 0.5047808587551117, 0.5065868616104126, 0.5041544079780579, 0.4988203406333923, 0.4948065459728241, 0.4934963881969452, 0.49241477251052856, 0.4896386981010437, 0.4898818492889404, 0.48574466705322267, 0.48151096105575564, 0.47727895975112916, 0.4765020549297333, 0.4739744305610657, 0.47246657609939574, 0.46950157880783083, 0.46713910698890687, 0.46629010438919066, 0.4622900664806366, 0.458865761756897, 0.4578199326992035, 0.45755885243415834, 0.45402511954307556, 0.45053504705429076, 0.4484586477279663, 0.44552398324012754, 0.4434686303138733, 0.4443315386772156, 0.43881420493125917, 0.43802924156188966, 0.43544335961341857, 0.43334659934043884, 0.43090746402740476, 0.42963454127311707, 0.4253138303756714, 0.4243709444999695, 0.4230805814266205, 0.41724247932434083, 0.41655716896057127, 0.4155306935310364, 0.4139146268367767, 0.41187761425971986, 0.4100872874259949, 0.40690137147903443, 0.4058425545692444, 0.4021353363990784, 0.3997623920440674, 0.3988735437393188, 0.39675356149673463, 0.39254833459854127, 0.3935519874095917, 0.39078755378723146, 0.3881528556346893, 0.38671048283576964, 0.38401579260826113, 0.38136855959892274, 0.380171936750412, 0.37974876165390015, 0.37674257159233093, 0.3758553147315979, 0.37362487316131593, 0.36962506771087644, 0.3689405083656311, 0.3689492106437683, 0.36561280488967896, 0.36412985920906066, 0.36109310388565063, 0.36029332280159, 0.3570014417171478, 0.35652275681495665, 0.3551247715950012, 0.35302047729492186, 0.3512348771095276, 0.3495537340641022, 0.34826700687408446, 0.34537922739982607, 0.3429412543773651, 0.34270307421684265, 0.34099392890930175, 0.3400468468666077, 0.3368227005004883, 0.3363401174545288, 0.3345874845981598, 0.3326768517494202, 0.33127317428588865, 0.33028407096862794, 0.3268173038959503, 0.3274277627468109, 0.3252062737941742, 0.3235690832138062, 0.32203882932662964, 0.3212256669998169, 0.31980851888656614, 0.3164539933204651, 0.3156252920627594, 0.3140572547912598, 0.3130648910999298, 0.3116461753845215, 0.3105572760105133, 0.307759952545166, 0.30739972591400144, 0.30541210174560546, 0.3047187030315399, 0.3026599586009979], 'loss_test': [1.0776488780975342, 1.0795716047286987, 1.077274203300476, 1.0940046310424805, 1.087935447692871, 1.0802381038665771, 1.084473729133606, 1.0846110582351685, 1.0640407800674438, 1.0824939012527466, 1.0654566287994385, 1.0820542573928833, 1.0702476501464844, 1.0753810405731201, 1.0849921703338623, 1.0944017171859741, 1.079424262046814, 1.0894336700439453, 1.0823049545288086, 1.079819917678833, 1.0952215194702148, 1.0856552124023438, 1.0774118900299072, 1.0679391622543335, 1.0972437858581543, 1.0877814292907715, 1.0907775163650513, 1.0952214002609253, 1.0684207677841187, 1.0751618146896362, 1.0855107307434082, 1.0665919780731201, 1.0782052278518677, 1.0631035566329956, 1.0778597593307495, 1.0644841194152832, 1.0605125427246094, 1.0587587356567383, 1.0520809888839722, 1.0340816974639893, 1.0391478538513184, 1.0373919010162354, 1.0119924545288086, 1.0212117433547974, 1.0074677467346191, 1.00705885887146, 1.0119115114212036, 1.0019621849060059, 0.9995333552360535, 0.9860533475875854, 0.96923828125, 0.9837585687637329, 0.9802505970001221, 0.97566157579422, 0.9584912657737732, 0.9687188267707825, 0.9722317457199097, 0.9677886962890625, 0.9463024735450745, 0.9339010715484619, 0.9376136064529419, 0.9322013854980469, 0.9282287955284119, 0.9485007524490356, 0.933626651763916, 0.9213664531707764, 0.9158750772476196, 0.921109139919281, 0.9293398857116699, 0.9168523550033569, 0.9153088331222534, 0.9154913425445557, 0.9131556153297424, 0.9063356518745422, 0.9198485016822815, 0.9286988377571106, 0.9053245186805725, 0.9284996390342712, 0.9066833853721619, 0.8970362544059753, 0.9077615737915039, 0.9085213541984558, 0.9135243892669678, 0.9099891185760498, 0.9024149179458618, 0.9002512693405151, 0.9076575040817261, 0.912731945514679, 0.9075533151626587, 0.9050577282905579, 0.9072698354721069, 0.909699022769928, 0.9180030226707458, 0.9104649424552917, 0.8933145403862, 0.9076605439186096, 0.907048225402832, 0.9086092114448547, 0.9092028737068176, 0.9013882875442505, 0.903414785861969, 0.8970534801483154, 0.9072557687759399, 0.903961718082428, 0.9102688431739807, 0.9195443391799927, 0.9038645625114441, 0.9093185663223267, 0.9108490347862244, 0.90073162317276, 0.894608736038208, 0.9010037183761597, 0.9022907614707947, 0.9060290455818176, 0.9045112729072571, 0.9068271517753601, 0.9053022861480713, 0.9106025099754333, 0.8942245841026306, 0.9085832834243774, 0.8951653838157654, 0.9038075804710388, 0.9178532958030701, 0.9062178134918213, 0.9153102040290833, 0.9027530550956726, 0.9144107699394226, 0.9115028381347656, 0.9170874953269958, 0.9167905449867249, 0.9107798337936401, 0.9172336459159851, 0.9123803377151489, 0.9167895913124084, 0.9157724380493164, 0.9205666780471802, 0.917354166507721, 0.9162214994430542, 0.9097944498062134, 0.9247342944145203, 0.8964654803276062, 0.9147567749023438, 0.9197070598602295, 0.9270545244216919, 0.9285047054290771, 0.9290828704833984, 0.9188034534454346, 0.9265155792236328, 0.9299848079681396, 0.9169210195541382, 0.9307506084442139, 0.9181423783302307, 0.9385716915130615, 0.9239228367805481, 0.9128865003585815, 0.9295693635940552, 0.9236882925033569, 0.9302520751953125, 0.9207854866981506, 0.9261640906333923, 0.9283370971679688, 0.9184263944625854, 0.9444685578346252, 0.9337729811668396, 0.9452060461044312, 0.9270864725112915, 0.9354845285415649, 0.9245149493217468, 0.9272422194480896, 0.9338405132293701, 0.9286064505577087, 0.9326721429824829, 0.9331679940223694, 0.9431365132331848, 0.928716242313385, 0.9418084025382996, 0.9362808465957642, 0.9373072385787964, 0.9418204426765442, 0.9383383989334106, 0.938446581363678, 0.9559444785118103, 0.9477391839027405, 0.9389560222625732, 0.9491018652915955, 0.962376594543457, 0.9465842843055725, 0.9511926770210266, 0.9483217000961304, 0.9542136788368225, 0.9533522725105286, 0.9498973488807678, 0.9565826654434204, 0.9579129815101624, 0.9425174593925476, 0.9519675374031067, 0.9542227983474731, 0.9486920237541199, 0.9544497132301331, 0.9572152495384216, 0.9650378227233887, 0.9574440121650696, 0.9627857804298401, 0.9644539952278137, 0.958357572555542, 0.9577969312667847, 0.9621466398239136, 0.9637199640274048, 0.9686883687973022, 0.9739329814910889, 0.9744430184364319, 0.9684357047080994, 0.9687010049819946, 0.9693354368209839, 0.9669130444526672, 0.9678833484649658, 0.9686977863311768, 0.982090950012207, 0.9703732132911682, 0.9720308184623718, 0.9683417677879333, 0.9714643359184265, 0.9804750680923462, 0.9844062328338623, 0.983974039554596, 0.9801544547080994, 0.9751915335655212, 0.9724012613296509, 0.9745190143585205, 0.992343008518219, 0.9790958166122437, 0.9865025281906128, 0.9874027967453003, 1.0048874616622925, 0.9915269613265991, 0.9899717569351196, 0.9798779487609863, 0.9766883850097656, 1.0014370679855347, 0.9718607664108276, 0.995032787322998, 0.9825869202613831, 0.9936695098876953, 0.9963312745094299, 0.9986392855644226, 0.9984562397003174, 0.9837034940719604, 0.9970560669898987, 0.9946771860122681, 1.0071076154708862], 'identifier': '9915537np'}