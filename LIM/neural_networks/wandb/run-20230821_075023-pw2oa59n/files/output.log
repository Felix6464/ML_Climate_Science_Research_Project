
  8%|███████▍                                                                                          | 19/250 [00:01<00:19, 12.10it/s, loss_test=1.083]
Epoch: 00, Training Loss: 0.9972, Test Loss: 1.0993
Epoch: 01, Training Loss: 0.9952, Test Loss: 1.0832
Epoch: 02, Training Loss: 0.9930, Test Loss: 1.0916
Epoch: 03, Training Loss: 0.9942, Test Loss: 1.0834
Epoch: 04, Training Loss: 0.9961, Test Loss: 1.0563
Epoch: 05, Training Loss: 0.9906, Test Loss: 1.0992
Epoch: 06, Training Loss: 0.9898, Test Loss: 1.0731
Epoch: 07, Training Loss: 0.9933, Test Loss: 1.0799
Epoch: 08, Training Loss: 0.9901, Test Loss: 1.0734
Epoch: 09, Training Loss: 0.9886, Test Loss: 1.0837
Epoch: 10, Training Loss: 0.9873, Test Loss: 1.0826
Epoch: 11, Training Loss: 0.9889, Test Loss: 1.0794
Epoch: 12, Training Loss: 0.9854, Test Loss: 1.0605
Epoch: 13, Training Loss: 0.9878, Test Loss: 1.0800
Epoch: 14, Training Loss: 0.9857, Test Loss: 1.0783
Epoch: 15, Training Loss: 0.9870, Test Loss: 1.0686
Epoch: 16, Training Loss: 0.9923, Test Loss: 1.0937
Epoch: 17, Training Loss: 0.9867, Test Loss: 1.0739
Epoch: 18, Training Loss: 0.9908, Test Loss: 1.0655

 17%|████████████████▊                                                                                 | 43/250 [00:03<00:17, 11.70it/s, loss_test=1.003]
Epoch: 20, Training Loss: 0.9823, Test Loss: 1.0697
Epoch: 21, Training Loss: 0.9863, Test Loss: 1.0895
Epoch: 22, Training Loss: 0.9882, Test Loss: 1.0818
Epoch: 23, Training Loss: 0.9826, Test Loss: 1.0710
Epoch: 24, Training Loss: 0.9803, Test Loss: 1.0805
Epoch: 25, Training Loss: 0.9790, Test Loss: 1.0768
Epoch: 26, Training Loss: 0.9825, Test Loss: 1.0721
Epoch: 27, Training Loss: 0.9855, Test Loss: 1.0797
Epoch: 28, Training Loss: 0.9797, Test Loss: 1.0885
Epoch: 29, Training Loss: 0.9740, Test Loss: 1.0760
Epoch: 30, Training Loss: 0.9711, Test Loss: 1.0751
Epoch: 31, Training Loss: 0.9625, Test Loss: 1.0711
Epoch: 32, Training Loss: 0.9591, Test Loss: 1.0671
Epoch: 33, Training Loss: 0.9524, Test Loss: 1.0662
Epoch: 34, Training Loss: 0.9420, Test Loss: 1.0594
Epoch: 35, Training Loss: 0.9368, Test Loss: 1.0455
Epoch: 36, Training Loss: 0.9322, Test Loss: 1.0488
Epoch: 37, Training Loss: 0.9246, Test Loss: 1.0486
Epoch: 38, Training Loss: 0.9220, Test Loss: 1.0148
Epoch: 39, Training Loss: 0.9135, Test Loss: 1.0051
Epoch: 40, Training Loss: 0.9110, Test Loss: 1.0281
Epoch: 41, Training Loss: 0.9054, Test Loss: 1.0031
Epoch: 42, Training Loss: 0.9001, Test Loss: 0.9892

 27%|██████████████████████████▎                                                                       | 67/250 [00:05<00:15, 11.81it/s, loss_test=0.952]
Epoch: 44, Training Loss: 0.8930, Test Loss: 0.9855
Epoch: 45, Training Loss: 0.8875, Test Loss: 0.9728
Epoch: 46, Training Loss: 0.8870, Test Loss: 0.9921
Epoch: 47, Training Loss: 0.8794, Test Loss: 0.9789
Epoch: 48, Training Loss: 0.8818, Test Loss: 0.9784
Epoch: 49, Training Loss: 0.8753, Test Loss: 0.9739
Epoch: 50, Training Loss: 0.8708, Test Loss: 0.9801
Epoch: 51, Training Loss: 0.8681, Test Loss: 0.9714
Epoch: 52, Training Loss: 0.8657, Test Loss: 0.9554
Epoch: 53, Training Loss: 0.8617, Test Loss: 0.9699
Epoch: 54, Training Loss: 0.8581, Test Loss: 0.9597
Epoch: 55, Training Loss: 0.8567, Test Loss: 0.9474
Epoch: 56, Training Loss: 0.8533, Test Loss: 0.9595
Epoch: 57, Training Loss: 0.8486, Test Loss: 0.9675
Epoch: 58, Training Loss: 0.8459, Test Loss: 0.9527
Epoch: 59, Training Loss: 0.8490, Test Loss: 0.9643
Epoch: 60, Training Loss: 0.8425, Test Loss: 0.9471
Epoch: 61, Training Loss: 0.8342, Test Loss: 0.9446
Epoch: 62, Training Loss: 0.8332, Test Loss: 0.9507
Epoch: 63, Training Loss: 0.8288, Test Loss: 0.9485
Epoch: 64, Training Loss: 0.8263, Test Loss: 0.9544
Epoch: 65, Training Loss: 0.8253, Test Loss: 0.9643
Epoch: 66, Training Loss: 0.8219, Test Loss: 0.9482

 37%|████████████████████████████████████▍                                                             | 93/250 [00:07<00:13, 11.98it/s, loss_test=0.904]
Epoch: 68, Training Loss: 0.8084, Test Loss: 0.9352
Epoch: 69, Training Loss: 0.8040, Test Loss: 0.9335
Epoch: 70, Training Loss: 0.8019, Test Loss: 0.9386
Epoch: 71, Training Loss: 0.7917, Test Loss: 0.9238
Epoch: 72, Training Loss: 0.7882, Test Loss: 0.9310
Epoch: 73, Training Loss: 0.7853, Test Loss: 0.9189
Epoch: 74, Training Loss: 0.7809, Test Loss: 0.9337
Epoch: 75, Training Loss: 0.7769, Test Loss: 0.9138
Epoch: 76, Training Loss: 0.7703, Test Loss: 0.9358
Epoch: 77, Training Loss: 0.7668, Test Loss: 0.9197
Epoch: 78, Training Loss: 0.7616, Test Loss: 0.9099
Epoch: 79, Training Loss: 0.7578, Test Loss: 0.9157
Epoch: 80, Training Loss: 0.7564, Test Loss: 0.9184
Epoch: 81, Training Loss: 0.7467, Test Loss: 0.8987
Epoch: 82, Training Loss: 0.7451, Test Loss: 0.9130
Epoch: 83, Training Loss: 0.7397, Test Loss: 0.9013
Epoch: 84, Training Loss: 0.7360, Test Loss: 0.9083
Epoch: 85, Training Loss: 0.7310, Test Loss: 0.9022
Epoch: 86, Training Loss: 0.7269, Test Loss: 0.9134
Epoch: 87, Training Loss: 0.7229, Test Loss: 0.9068
Epoch: 88, Training Loss: 0.7155, Test Loss: 0.9060
Epoch: 89, Training Loss: 0.7150, Test Loss: 0.8960
Epoch: 90, Training Loss: 0.7123, Test Loss: 0.8993

 47%|█████████████████████████████████████████████▍                                                   | 117/250 [00:09<00:10, 12.17it/s, loss_test=0.891]
Epoch: 92, Training Loss: 0.7005, Test Loss: 0.9044
Epoch: 93, Training Loss: 0.6968, Test Loss: 0.9047
Epoch: 94, Training Loss: 0.6934, Test Loss: 0.8948
Epoch: 95, Training Loss: 0.6876, Test Loss: 0.9020
Epoch: 96, Training Loss: 0.6825, Test Loss: 0.8922
Epoch: 97, Training Loss: 0.6796, Test Loss: 0.9040
Epoch: 98, Training Loss: 0.6763, Test Loss: 0.9018
Epoch: 99, Training Loss: 0.6739, Test Loss: 0.8948
Epoch: 100, Training Loss: 0.6718, Test Loss: 0.8923
Epoch: 101, Training Loss: 0.6682, Test Loss: 0.8883
Epoch: 102, Training Loss: 0.6630, Test Loss: 0.9066
Epoch: 103, Training Loss: 0.6598, Test Loss: 0.9103
Epoch: 104, Training Loss: 0.6582, Test Loss: 0.8982
Epoch: 105, Training Loss: 0.6515, Test Loss: 0.8882
Epoch: 106, Training Loss: 0.6484, Test Loss: 0.8916
Epoch: 107, Training Loss: 0.6457, Test Loss: 0.9043
Epoch: 108, Training Loss: 0.6425, Test Loss: 0.8839
Epoch: 109, Training Loss: 0.6410, Test Loss: 0.9023
Epoch: 110, Training Loss: 0.6349, Test Loss: 0.8983
Epoch: 111, Training Loss: 0.6344, Test Loss: 0.9027
Epoch: 112, Training Loss: 0.6295, Test Loss: 0.8984
Epoch: 113, Training Loss: 0.6273, Test Loss: 0.8943
Epoch: 114, Training Loss: 0.6221, Test Loss: 0.9069

 56%|██████████████████████████████████████████████████████▋                                          | 141/250 [00:11<00:08, 12.58it/s, loss_test=0.928]
Epoch: 116, Training Loss: 0.6179, Test Loss: 0.8914
Epoch: 117, Training Loss: 0.6119, Test Loss: 0.9096
Epoch: 118, Training Loss: 0.6103, Test Loss: 0.8940
Epoch: 119, Training Loss: 0.6097, Test Loss: 0.9064
Epoch: 120, Training Loss: 0.6055, Test Loss: 0.8965
Epoch: 121, Training Loss: 0.6028, Test Loss: 0.9008
Epoch: 122, Training Loss: 0.5976, Test Loss: 0.8918
Epoch: 123, Training Loss: 0.5963, Test Loss: 0.8895
Epoch: 124, Training Loss: 0.5926, Test Loss: 0.9049
Epoch: 125, Training Loss: 0.5896, Test Loss: 0.9006
Epoch: 126, Training Loss: 0.5904, Test Loss: 0.9030
Epoch: 127, Training Loss: 0.5844, Test Loss: 0.9049
Epoch: 128, Training Loss: 0.5820, Test Loss: 0.9004
Epoch: 129, Training Loss: 0.5789, Test Loss: 0.9108
Epoch: 130, Training Loss: 0.5760, Test Loss: 0.9082
Epoch: 131, Training Loss: 0.5729, Test Loss: 0.8998
Epoch: 132, Training Loss: 0.5697, Test Loss: 0.9038
Epoch: 133, Training Loss: 0.5675, Test Loss: 0.8946
Epoch: 134, Training Loss: 0.5646, Test Loss: 0.9078
Epoch: 135, Training Loss: 0.5608, Test Loss: 0.9129
Epoch: 136, Training Loss: 0.5583, Test Loss: 0.9166
Epoch: 137, Training Loss: 0.5538, Test Loss: 0.9070
Epoch: 138, Training Loss: 0.5528, Test Loss: 0.9108
Epoch: 139, Training Loss: 0.5493, Test Loss: 0.9113

 66%|████████████████████████████████████████████████████████████████                                 | 165/250 [00:13<00:07, 11.77it/s, loss_test=0.954]
Epoch: 141, Training Loss: 0.5439, Test Loss: 0.9283
Epoch: 142, Training Loss: 0.5411, Test Loss: 0.9257
Epoch: 143, Training Loss: 0.5385, Test Loss: 0.9163
Epoch: 144, Training Loss: 0.5373, Test Loss: 0.9107
Epoch: 145, Training Loss: 0.5319, Test Loss: 0.9135
Epoch: 146, Training Loss: 0.5310, Test Loss: 0.9176
Epoch: 147, Training Loss: 0.5273, Test Loss: 0.9150
Epoch: 148, Training Loss: 0.5249, Test Loss: 0.9210
Epoch: 149, Training Loss: 0.5222, Test Loss: 0.9168
Epoch: 150, Training Loss: 0.5205, Test Loss: 0.9149
Epoch: 151, Training Loss: 0.5172, Test Loss: 0.9149
Epoch: 152, Training Loss: 0.5144, Test Loss: 0.9294
Epoch: 153, Training Loss: 0.5140, Test Loss: 0.9265
Epoch: 154, Training Loss: 0.5078, Test Loss: 0.9276
Epoch: 155, Training Loss: 0.5068, Test Loss: 0.9207
Epoch: 156, Training Loss: 0.5042, Test Loss: 0.9313
Epoch: 157, Training Loss: 0.4997, Test Loss: 0.9420
Epoch: 158, Training Loss: 0.4973, Test Loss: 0.9364
Epoch: 159, Training Loss: 0.4973, Test Loss: 0.9364
Epoch: 160, Training Loss: 0.4931, Test Loss: 0.9325
Epoch: 161, Training Loss: 0.4905, Test Loss: 0.9322
Epoch: 162, Training Loss: 0.4874, Test Loss: 0.9437
Epoch: 163, Training Loss: 0.4861, Test Loss: 0.9319

 76%|█████████████████████████████████████████████████████████████████████████▎                       | 189/250 [00:15<00:05, 11.79it/s, loss_test=0.974]
Epoch: 165, Training Loss: 0.4820, Test Loss: 0.9540
Epoch: 166, Training Loss: 0.4778, Test Loss: 0.9450
Epoch: 167, Training Loss: 0.4749, Test Loss: 0.9614
Epoch: 168, Training Loss: 0.4709, Test Loss: 0.9429
Epoch: 169, Training Loss: 0.4703, Test Loss: 0.9573
Epoch: 170, Training Loss: 0.4680, Test Loss: 0.9370
Epoch: 171, Training Loss: 0.4647, Test Loss: 0.9517
Epoch: 172, Training Loss: 0.4603, Test Loss: 0.9527
Epoch: 173, Training Loss: 0.4599, Test Loss: 0.9517
Epoch: 174, Training Loss: 0.4584, Test Loss: 0.9486
Epoch: 175, Training Loss: 0.4560, Test Loss: 0.9606
Epoch: 176, Training Loss: 0.4536, Test Loss: 0.9561
Epoch: 177, Training Loss: 0.4500, Test Loss: 0.9585
Epoch: 178, Training Loss: 0.4496, Test Loss: 0.9591
Epoch: 179, Training Loss: 0.4446, Test Loss: 0.9712
Epoch: 180, Training Loss: 0.4437, Test Loss: 0.9559
Epoch: 181, Training Loss: 0.4418, Test Loss: 0.9643
Epoch: 182, Training Loss: 0.4395, Test Loss: 0.9507
Epoch: 183, Training Loss: 0.4360, Test Loss: 0.9604
Epoch: 184, Training Loss: 0.4323, Test Loss: 0.9545
Epoch: 185, Training Loss: 0.4321, Test Loss: 0.9652
Epoch: 186, Training Loss: 0.4298, Test Loss: 0.9543
Epoch: 187, Training Loss: 0.4268, Test Loss: 0.9844

 86%|███████████████████████████████████████████████████████████████████████████████████▍             | 215/250 [00:17<00:02, 12.56it/s, loss_test=0.993]
Epoch: 189, Training Loss: 0.4227, Test Loss: 0.9717
Epoch: 190, Training Loss: 0.4189, Test Loss: 0.9736
Epoch: 191, Training Loss: 0.4181, Test Loss: 0.9605
Epoch: 192, Training Loss: 0.4156, Test Loss: 0.9803
Epoch: 193, Training Loss: 0.4143, Test Loss: 0.9823
Epoch: 194, Training Loss: 0.4110, Test Loss: 0.9968
Epoch: 195, Training Loss: 0.4092, Test Loss: 0.9820
Epoch: 196, Training Loss: 0.4081, Test Loss: 0.9734
Epoch: 197, Training Loss: 0.4073, Test Loss: 0.9707
Epoch: 198, Training Loss: 0.4029, Test Loss: 0.9941
Epoch: 199, Training Loss: 0.4020, Test Loss: 0.9817
Epoch: 200, Training Loss: 0.4005, Test Loss: 0.9749
Epoch: 201, Training Loss: 0.3968, Test Loss: 0.9777
Epoch: 202, Training Loss: 0.3958, Test Loss: 0.9792
Epoch: 203, Training Loss: 0.3927, Test Loss: 0.9887
Epoch: 204, Training Loss: 0.3913, Test Loss: 0.9836
Epoch: 205, Training Loss: 0.3904, Test Loss: 0.9848
Epoch: 206, Training Loss: 0.3872, Test Loss: 0.9735
Epoch: 207, Training Loss: 0.3849, Test Loss: 0.9816
Epoch: 208, Training Loss: 0.3851, Test Loss: 0.9864
Epoch: 209, Training Loss: 0.3817, Test Loss: 0.9921
Epoch: 210, Training Loss: 0.3800, Test Loss: 0.9887
Epoch: 211, Training Loss: 0.3772, Test Loss: 0.9922

 96%|████████████████████████████████████████████████████████████████████████████████████████████▋    | 239/250 [00:19<00:00, 12.02it/s, loss_test=1.020]
Epoch: 213, Training Loss: 0.3748, Test Loss: 1.0034
Epoch: 214, Training Loss: 0.3726, Test Loss: 0.9931
Epoch: 215, Training Loss: 0.3712, Test Loss: 1.0058
Epoch: 216, Training Loss: 0.3684, Test Loss: 1.0003
Epoch: 217, Training Loss: 0.3663, Test Loss: 0.9683
Epoch: 218, Training Loss: 0.3655, Test Loss: 0.9915
Epoch: 219, Training Loss: 0.3639, Test Loss: 1.0013
Epoch: 220, Training Loss: 0.3618, Test Loss: 0.9895
Epoch: 221, Training Loss: 0.3597, Test Loss: 1.0065
Epoch: 222, Training Loss: 0.3579, Test Loss: 0.9761
Epoch: 223, Training Loss: 0.3564, Test Loss: 1.0069
Epoch: 224, Training Loss: 0.3554, Test Loss: 1.0200
Epoch: 225, Training Loss: 0.3526, Test Loss: 1.0038
Epoch: 226, Training Loss: 0.3504, Test Loss: 0.9895
Epoch: 227, Training Loss: 0.3496, Test Loss: 1.0348
Epoch: 228, Training Loss: 0.3480, Test Loss: 0.9880
Epoch: 229, Training Loss: 0.3457, Test Loss: 1.0013
Epoch: 230, Training Loss: 0.3448, Test Loss: 1.0009
Epoch: 231, Training Loss: 0.3415, Test Loss: 0.9889
Epoch: 232, Training Loss: 0.3406, Test Loss: 0.9979
Epoch: 233, Training Loss: 0.3389, Test Loss: 1.0242
Epoch: 234, Training Loss: 0.3386, Test Loss: 1.0037
Epoch: 235, Training Loss: 0.3370, Test Loss: 1.0059
Epoch: 236, Training Loss: 0.3346, Test Loss: 1.0001

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.07it/s, loss_test=1.030]
Epoch: 238, Training Loss: 0.3318, Test Loss: 0.9928
Epoch: 239, Training Loss: 0.3292, Test Loss: 1.0199
Epoch: 240, Training Loss: 0.3281, Test Loss: 1.0185
Epoch: 241, Training Loss: 0.3269, Test Loss: 1.0038
Epoch: 242, Training Loss: 0.3262, Test Loss: 1.0043
Epoch: 243, Training Loss: 0.3237, Test Loss: 1.0038
Epoch: 244, Training Loss: 0.3224, Test Loss: 1.0111
Epoch: 245, Training Loss: 0.3220, Test Loss: 1.0284
Epoch: 246, Training Loss: 0.3194, Test Loss: 0.9972
Epoch: 247, Training Loss: 0.3185, Test Loss: 1.0161
Epoch: 248, Training Loss: 0.3157, Test Loss: 1.0313
Epoch: 249, Training Loss: 0.3147, Test Loss: 1.0295
Model saved as model_5537978np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-124000-5537978np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9972010731697083, 0.9951913237571717, 0.9930343627929688, 0.994218933582306, 0.9960892915725708, 0.9906095862388611, 0.989818811416626, 0.993258535861969, 0.9900989055633544, 0.9886439919471741, 0.9872654438018799, 0.988910460472107, 0.9853651404380799, 0.9878113031387329, 0.9857176899909973, 0.9870339155197143, 0.9922669529914856, 0.98666011095047, 0.9908131241798401, 0.9887675523757935, 0.9822761058807373, 0.9862877368927002, 0.9882173895835876, 0.9826205968856812, 0.9803133487701416, 0.9789615392684936, 0.9825131893157959, 0.985507357120514, 0.9796989440917969, 0.9740451574325562, 0.9710896730422973, 0.962511396408081, 0.9590816020965576, 0.952396821975708, 0.9419518947601319, 0.9368105053901672, 0.9321603298187255, 0.9246462345123291, 0.9219552278518677, 0.9135060548782349, 0.9109522700309753, 0.9054000377655029, 0.9001216888427734, 0.897502326965332, 0.8930032849311829, 0.8874597191810608, 0.8870481371879577, 0.8793755292892456, 0.88175048828125, 0.8753159165382385, 0.8708114862442017, 0.8680744051933289, 0.8656993746757508, 0.8616824746131897, 0.8580938816070557, 0.8567078351974488, 0.8532512307167053, 0.8485715866088868, 0.8459012985229493, 0.849025559425354, 0.8424856543540955, 0.834196925163269, 0.8331874370574951, 0.8288089394569397, 0.8263315796852112, 0.8252876996994019, 0.8218624114990234, 0.8112732410430908, 0.8084348440170288, 0.8040408015251159, 0.8018594145774841, 0.7917125582695007, 0.78821861743927, 0.7852519035339356, 0.7809215188026428, 0.7768854618072509, 0.7702748537063598, 0.7667737364768982, 0.7615511655807495, 0.7577612519264221, 0.7564383983612061, 0.7467489242553711, 0.7451473832130432, 0.7396679162979126, 0.7360255718231201, 0.731003749370575, 0.7268803000450135, 0.7228598475456238, 0.7155239939689636, 0.7149671912193298, 0.7122847676277161, 0.7050659537315369, 0.7004539132118225, 0.6968336582183838, 0.6934133529663086, 0.6875988721847535, 0.682463014125824, 0.6796302437782288, 0.6762597560882568, 0.6738740086555481, 0.6717991828918457, 0.6681586146354676, 0.6629856348037719, 0.6597610592842102, 0.6582330465316772, 0.6514741778373718, 0.6483973264694214, 0.6456938743591308, 0.6425037145614624, 0.6410383939743042, 0.6348597526550293, 0.6343794941902161, 0.6294539570808411, 0.6273178577423095, 0.6221096515655518, 0.6201088309288025, 0.6178786635398865, 0.6119250059127808, 0.610338819026947, 0.6097112774848938, 0.6055434465408325, 0.602798867225647, 0.597620964050293, 0.596268355846405, 0.5925652384757996, 0.5896063685417176, 0.5903788089752198, 0.5844116926193237, 0.5820488333702087, 0.5788800358772278, 0.576034414768219, 0.5729333639144898, 0.5697386026382446, 0.5675450444221497, 0.5646106958389282, 0.5608433604240417, 0.5582834243774414, 0.5537606477737427, 0.5528084874153137, 0.5492879152297974, 0.5477026343345642, 0.5439048409461975, 0.5411310076713562, 0.5384626269340516, 0.5372553586959838, 0.531870675086975, 0.530998420715332, 0.5272912859916687, 0.52489755153656, 0.5222422242164612, 0.5205005884170533, 0.5172486305236816, 0.5143667459487915, 0.5139749646186829, 0.5078421115875245, 0.5068110406398774, 0.5041847586631775, 0.4997182250022888, 0.4973369002342224, 0.4972609281539917, 0.49311570525169374, 0.49051690101623535, 0.4873897671699524, 0.48610028624534607, 0.4834997534751892, 0.48202304244041444, 0.47776795625686647, 0.4748884320259094, 0.47094114422798156, 0.4703156590461731, 0.46804465651512145, 0.4646633505821228, 0.4603274047374725, 0.45988547801971436, 0.4584004878997803, 0.4559793472290039, 0.45357662439346313, 0.4500205874443054, 0.44956812262535095, 0.44458713531494143, 0.4436793804168701, 0.4417624831199646, 0.43953293561935425, 0.43596455454826355, 0.43226503729820254, 0.4321120083332062, 0.4297508239746094, 0.42681777477264404, 0.4253758192062378, 0.4227202653884888, 0.41890864372253417, 0.41813079714775087, 0.41562625765800476, 0.4142614483833313, 0.41101263761520385, 0.4092370569705963, 0.40813117623329165, 0.4073279440402985, 0.4028577744960785, 0.40203797817230225, 0.4005484163761139, 0.39680169224739076, 0.3958448112010956, 0.3927326321601868, 0.3913330078125, 0.39041818380355836, 0.3872473895549774, 0.38490429520606995, 0.38510788083076475, 0.38167264461517336, 0.3800427198410034, 0.37715821862220766, 0.37493919730186465, 0.37480026483535767, 0.37258137464523317, 0.3712084054946899, 0.36841089129447935, 0.36628177762031555, 0.3654505372047424, 0.363925164937973, 0.36183546781539916, 0.3596892714500427, 0.3579219222068787, 0.35640241503715514, 0.35538113713264463, 0.35261787176132203, 0.3504304468631744, 0.34958978891372683, 0.3479970693588257, 0.3456859827041626, 0.34484475255012514, 0.34154508709907533, 0.3405881762504578, 0.3388812065124512, 0.3385535299777985, 0.3369875729084015, 0.3345503568649292, 0.3323610186576843, 0.33181466460227965, 0.3291600406169891, 0.32808294892311096, 0.326924079656601, 0.3262129187583923, 0.32365331053733826, 0.3223810434341431, 0.3220228374004364, 0.3193719208240509, 0.31845073103904725, 0.31572756767272947, 0.31472386717796325], 'loss_test': [1.0992759466171265, 1.0832209587097168, 1.0916306972503662, 1.0833709239959717, 1.0562688112258911, 1.099196434020996, 1.073093056678772, 1.079910159111023, 1.073441982269287, 1.083660364151001, 1.0826303958892822, 1.0793509483337402, 1.0605448484420776, 1.0799707174301147, 1.0783146619796753, 1.0686116218566895, 1.0936839580535889, 1.073940634727478, 1.0654869079589844, 1.0834650993347168, 1.0697354078292847, 1.089512825012207, 1.0818225145339966, 1.070962905883789, 1.0805047750473022, 1.076797604560852, 1.072108268737793, 1.0796926021575928, 1.0884863138198853, 1.0759638547897339, 1.0751429796218872, 1.0710982084274292, 1.0670788288116455, 1.0662310123443604, 1.059376835823059, 1.0455434322357178, 1.0488436222076416, 1.0486161708831787, 1.0147508382797241, 1.0050965547561646, 1.028071641921997, 1.0030514001846313, 0.9891955852508545, 1.0027159452438354, 0.985511302947998, 0.9728051424026489, 0.9920934438705444, 0.9789235591888428, 0.9783995151519775, 0.9739069938659668, 0.9800823926925659, 0.9714294672012329, 0.955398440361023, 0.969903826713562, 0.9597405791282654, 0.9474471211433411, 0.9595461487770081, 0.9674642086029053, 0.9527152180671692, 0.9643224477767944, 0.9471219778060913, 0.9445614218711853, 0.9507074952125549, 0.9484602212905884, 0.9544243812561035, 0.9642861485481262, 0.9481762647628784, 0.9523979425430298, 0.9352133274078369, 0.9335179924964905, 0.9386329054832458, 0.9238373637199402, 0.9309980869293213, 0.9189404249191284, 0.9336788654327393, 0.9137729406356812, 0.9358391165733337, 0.9196794629096985, 0.90985506772995, 0.9157296419143677, 0.9183729887008667, 0.8987257480621338, 0.9129984974861145, 0.9013159871101379, 0.9082622528076172, 0.9021879434585571, 0.9133779406547546, 0.9068117141723633, 0.9059590697288513, 0.8959903717041016, 0.8993222117424011, 0.8859217762947083, 0.9044144153594971, 0.9046724438667297, 0.894834578037262, 0.9019858837127686, 0.8922178745269775, 0.903975248336792, 0.9017584919929504, 0.8948487639427185, 0.8923460245132446, 0.8883499503135681, 0.906552255153656, 0.9103024005889893, 0.8982282876968384, 0.8882061839103699, 0.8916088938713074, 0.9043301939964294, 0.8839033842086792, 0.9023470878601074, 0.8983272314071655, 0.9027414917945862, 0.8983637690544128, 0.8942559361457825, 0.9068679809570312, 0.8982127904891968, 0.8914484977722168, 0.9096101522445679, 0.8940104246139526, 0.906403124332428, 0.8965303301811218, 0.9007843136787415, 0.8917908072471619, 0.8895207643508911, 0.9049047827720642, 0.9006044268608093, 0.9029781818389893, 0.9049418568611145, 0.9003869891166687, 0.9107759594917297, 0.9082357287406921, 0.8997570872306824, 0.9038184285163879, 0.8946492075920105, 0.9077625870704651, 0.9128786325454712, 0.9165773987770081, 0.9070056676864624, 0.910771369934082, 0.9112939834594727, 0.9047882556915283, 0.9283043146133423, 0.9256635904312134, 0.9163230061531067, 0.9107471108436584, 0.9134830832481384, 0.9176051616668701, 0.9149568676948547, 0.9210479259490967, 0.9168160557746887, 0.9148871898651123, 0.9149264097213745, 0.9293630123138428, 0.9265428781509399, 0.927611231803894, 0.9206992387771606, 0.9313404560089111, 0.9419611692428589, 0.9363844990730286, 0.936446487903595, 0.9324520826339722, 0.932232677936554, 0.9437283277511597, 0.9318947792053223, 0.9416226148605347, 0.9540248513221741, 0.9450042843818665, 0.9614173173904419, 0.9429153800010681, 0.9573327302932739, 0.9369511008262634, 0.9516713619232178, 0.9526609778404236, 0.9516729116439819, 0.9486066699028015, 0.9606258273124695, 0.9560514092445374, 0.9584769010543823, 0.9590511322021484, 0.9711855053901672, 0.9558973908424377, 0.9642946124076843, 0.9506549835205078, 0.960402250289917, 0.9544721245765686, 0.9651874303817749, 0.954270601272583, 0.9843826293945312, 0.9669308662414551, 0.9717236161231995, 0.9735510945320129, 0.9604899287223816, 0.9802595376968384, 0.9822993278503418, 0.9967546463012695, 0.9819807410240173, 0.9734410643577576, 0.9707138538360596, 0.9940604567527771, 0.9817265868186951, 0.9748984575271606, 0.9776538610458374, 0.9792153239250183, 0.9886671304702759, 0.9835588335990906, 0.9848430156707764, 0.9735428094863892, 0.9815936088562012, 0.986350953578949, 0.9920669794082642, 0.9887305498123169, 0.9921759963035583, 1.0009981393814087, 1.0034180879592896, 0.9931192994117737, 1.0058022737503052, 1.0003275871276855, 0.968331515789032, 0.9915013909339905, 1.0013139247894287, 0.9894561767578125, 1.0065488815307617, 0.9760633111000061, 1.0068907737731934, 1.0200220346450806, 1.003800868988037, 0.9895080924034119, 1.0347646474838257, 0.9879730343818665, 1.0013154745101929, 1.0009377002716064, 0.9889380931854248, 0.9979445934295654, 1.0242422819137573, 1.0037238597869873, 1.005857229232788, 1.000076174736023, 0.9995590448379517, 0.9928085803985596, 1.0199499130249023, 1.0184553861618042, 1.003794550895691, 1.004304051399231, 1.0038307905197144, 1.0111104249954224, 1.0284026861190796, 0.9971618056297302, 1.0161173343658447, 1.031323790550232, 1.0295134782791138], 'identifier': '5537978np'}