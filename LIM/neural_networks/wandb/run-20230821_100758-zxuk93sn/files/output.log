
 20%|███████████████████▌                                                                              | 20/100 [00:01<00:06, 11.85it/s, loss_test=1.081]
Epoch: 00, Training Loss: 0.9977, Test Loss: 1.0986
Epoch: 01, Training Loss: 0.9952, Test Loss: 1.0882
Epoch: 02, Training Loss: 0.9949, Test Loss: 1.0929
Epoch: 03, Training Loss: 0.9915, Test Loss: 1.1030
Epoch: 04, Training Loss: 0.9934, Test Loss: 1.0834
Epoch: 05, Training Loss: 0.9920, Test Loss: 1.0900
Epoch: 06, Training Loss: 0.9879, Test Loss: 1.0770
Epoch: 07, Training Loss: 0.9897, Test Loss: 1.0855
Epoch: 08, Training Loss: 0.9922, Test Loss: 1.0871
Epoch: 09, Training Loss: 0.9941, Test Loss: 1.0958
Epoch: 10, Training Loss: 0.9886, Test Loss: 1.1183
Epoch: 11, Training Loss: 0.9908, Test Loss: 1.0875
Epoch: 12, Training Loss: 0.9844, Test Loss: 1.0800
Epoch: 13, Training Loss: 0.9855, Test Loss: 1.0829
Epoch: 14, Training Loss: 0.9877, Test Loss: 1.0759
Epoch: 15, Training Loss: 0.9858, Test Loss: 1.0956
Epoch: 16, Training Loss: 0.9860, Test Loss: 1.0801
Epoch: 17, Training Loss: 0.9856, Test Loss: 1.0905
Epoch: 18, Training Loss: 0.9818, Test Loss: 1.0857

 44%|███████████████████████████████████████████                                                       | 44/100 [00:03<00:04, 12.08it/s, loss_test=1.034]
Epoch: 20, Training Loss: 0.9838, Test Loss: 1.0868
Epoch: 21, Training Loss: 0.9837, Test Loss: 1.0735
Epoch: 22, Training Loss: 0.9828, Test Loss: 1.0935
Epoch: 23, Training Loss: 0.9869, Test Loss: 1.0826
Epoch: 24, Training Loss: 0.9834, Test Loss: 1.0721
Epoch: 25, Training Loss: 0.9786, Test Loss: 1.0670
Epoch: 26, Training Loss: 0.9781, Test Loss: 1.0950
Epoch: 27, Training Loss: 0.9799, Test Loss: 1.0736
Epoch: 28, Training Loss: 0.9744, Test Loss: 1.0700
Epoch: 29, Training Loss: 0.9673, Test Loss: 1.0653
Epoch: 30, Training Loss: 0.9670, Test Loss: 1.0671
Epoch: 31, Training Loss: 0.9656, Test Loss: 1.0734
Epoch: 32, Training Loss: 0.9597, Test Loss: 1.0907
Epoch: 33, Training Loss: 0.9561, Test Loss: 1.0687
Epoch: 34, Training Loss: 0.9545, Test Loss: 1.0675
Epoch: 35, Training Loss: 0.9458, Test Loss: 1.0628
Epoch: 36, Training Loss: 0.9455, Test Loss: 1.0579
Epoch: 37, Training Loss: 0.9474, Test Loss: 1.0538
Epoch: 38, Training Loss: 0.9446, Test Loss: 1.0654
Epoch: 39, Training Loss: 0.9396, Test Loss: 1.0483
Epoch: 40, Training Loss: 0.9339, Test Loss: 1.0483
Epoch: 41, Training Loss: 0.9308, Test Loss: 1.0367
Epoch: 42, Training Loss: 0.9252, Test Loss: 1.0455

 70%|████████████████████████████████████████████████████████████████████▌                             | 70/100 [00:05<00:02, 12.67it/s, loss_test=0.942]
Epoch: 44, Training Loss: 0.9117, Test Loss: 1.0342
Epoch: 45, Training Loss: 0.9083, Test Loss: 1.0167
Epoch: 46, Training Loss: 0.9074, Test Loss: 1.0298
Epoch: 47, Training Loss: 0.9026, Test Loss: 1.0171
Epoch: 48, Training Loss: 0.8955, Test Loss: 1.0065
Epoch: 49, Training Loss: 0.8861, Test Loss: 1.0137
Epoch: 50, Training Loss: 0.8860, Test Loss: 1.0174
Epoch: 51, Training Loss: 0.8829, Test Loss: 0.9984
Epoch: 52, Training Loss: 0.8790, Test Loss: 0.9993
Epoch: 53, Training Loss: 0.8769, Test Loss: 0.9925
Epoch: 54, Training Loss: 0.8670, Test Loss: 0.9982
Epoch: 55, Training Loss: 0.8600, Test Loss: 0.9586
Epoch: 56, Training Loss: 0.8580, Test Loss: 0.9713
Epoch: 57, Training Loss: 0.8519, Test Loss: 0.9846
Epoch: 58, Training Loss: 0.8501, Test Loss: 0.9672
Epoch: 59, Training Loss: 0.8408, Test Loss: 0.9719
Epoch: 60, Training Loss: 0.8423, Test Loss: 0.9684
Epoch: 61, Training Loss: 0.8322, Test Loss: 0.9845
Epoch: 62, Training Loss: 0.8299, Test Loss: 0.9582
Epoch: 63, Training Loss: 0.8219, Test Loss: 0.9493
Epoch: 64, Training Loss: 0.8180, Test Loss: 0.9562
Epoch: 65, Training Loss: 0.8162, Test Loss: 0.9390
Epoch: 66, Training Loss: 0.8088, Test Loss: 0.9514
Epoch: 67, Training Loss: 0.8055, Test Loss: 0.9484
Epoch: 68, Training Loss: 0.8000, Test Loss: 0.9496
Epoch: 69, Training Loss: 0.7961, Test Loss: 0.9425
Epoch: 70, Training Loss: 0.7945, Test Loss: 0.9370
Epoch: 71, Training Loss: 0.7881, Test Loss: 0.9280
Epoch: 72, Training Loss: 0.7851, Test Loss: 0.9390
Epoch: 73, Training Loss: 0.7799, Test Loss: 0.9204
Epoch: 74, Training Loss: 0.7767, Test Loss: 0.9394
Epoch: 75, Training Loss: 0.7722, Test Loss: 0.9240
Epoch: 76, Training Loss: 0.7662, Test Loss: 0.9271
Epoch: 77, Training Loss: 0.7634, Test Loss: 0.9317
Epoch: 78, Training Loss: 0.7521, Test Loss: 0.9375
Epoch: 79, Training Loss: 0.7555, Test Loss: 0.9177
Epoch: 80, Training Loss: 0.7516, Test Loss: 0.9095
Epoch: 81, Training Loss: 0.7480, Test Loss: 0.9101
Epoch: 82, Training Loss: 0.7423, Test Loss: 0.9087
Epoch: 83, Training Loss: 0.7403, Test Loss: 0.9121
Epoch: 84, Training Loss: 0.7390, Test Loss: 0.9045
Epoch: 85, Training Loss: 0.7296, Test Loss: 0.9129
Epoch: 86, Training Loss: 0.7261, Test Loss: 0.9021
Epoch: 87, Training Loss: 0.7268, Test Loss: 0.9118
Epoch: 88, Training Loss: 0.7233, Test Loss: 0.9046
Epoch: 89, Training Loss: 0.7182, Test Loss: 0.9060
Epoch: 90, Training Loss: 0.7154, Test Loss: 0.9047
Epoch: 91, Training Loss: 0.7132, Test Loss: 0.9036
Epoch: 92, Training Loss: 0.7067, Test Loss: 0.9031


100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.14it/s, loss_test=0.892]
Epoch: 94, Training Loss: 0.7014, Test Loss: 0.9124
Epoch: 95, Training Loss: 0.6993, Test Loss: 0.9027
Epoch: 96, Training Loss: 0.6953, Test Loss: 0.9037
Epoch: 97, Training Loss: 0.6915, Test Loss: 0.8933
Epoch: 98, Training Loss: 0.6871, Test Loss: 0.9105
Epoch: 99, Training Loss: 0.6819, Test Loss: 0.8924
Model saved as model_6590495np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-1270000-6590495np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 100, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9976658940315246, 0.9952269911766052, 0.9949249267578125, 0.9914874196052551, 0.9934391140937805, 0.9920439958572388, 0.9879016637802124, 0.9896732330322265, 0.9921581149101257, 0.9940789222717286, 0.9885610461235046, 0.9908430099487304, 0.9844229459762573, 0.9855360746383667, 0.9877324581146241, 0.9858220458030701, 0.9860067486763, 0.9856223940849305, 0.9817726373672485, 0.9836653232574463, 0.9837721824645996, 0.9836701035499573, 0.9828189849853516, 0.9868661761283875, 0.9834131956100464, 0.9785773754119873, 0.9781211376190185, 0.979886507987976, 0.9743951916694641, 0.967317521572113, 0.9669996500015259, 0.9655902862548829, 0.9596895575523376, 0.9561258912086487, 0.9545386910438538, 0.9457512259483337, 0.9454514384269714, 0.9474283695220947, 0.944588589668274, 0.9396405577659607, 0.9339324712753296, 0.930838429927826, 0.9251843333244324, 0.9168061733245849, 0.9116525173187255, 0.9083218574523926, 0.9074258685112, 0.9025749683380127, 0.8955108880996704, 0.886085319519043, 0.8859921097755432, 0.8829257011413574, 0.8790014982223511, 0.8769460439682006, 0.8669731259346009, 0.8600208997726441, 0.8580275416374207, 0.851880156993866, 0.8500744223594665, 0.8408014178276062, 0.8423327088356019, 0.8321520447731018, 0.8299086332321167, 0.8219186186790466, 0.8179890036582946, 0.8161792635917664, 0.8087516307830811, 0.8055107593536377, 0.8000388860702514, 0.7960847735404968, 0.7944605708122253, 0.7881258368492127, 0.785059928894043, 0.7798537492752076, 0.7767255663871765, 0.7721534967422485, 0.7662122845649719, 0.7634095549583435, 0.7521437883377076, 0.755505895614624, 0.7516232132911682, 0.7479710459709168, 0.7422861456871033, 0.7403197407722473, 0.7389751553535462, 0.7296138644218445, 0.7261168241500855, 0.7267883658409119, 0.7233012914657593, 0.7182098388671875, 0.7154314041137695, 0.7131978392601013, 0.7067086815834045, 0.7020825862884521, 0.7013623356819153, 0.6992898344993591, 0.6952531337738037, 0.6915031671524048, 0.6871096730232239, 0.6819041013717652], 'loss_test': [1.098633885383606, 1.0882190465927124, 1.0929365158081055, 1.1030224561691284, 1.0834349393844604, 1.0900170803070068, 1.077004313468933, 1.0855158567428589, 1.087095022201538, 1.095751166343689, 1.1183007955551147, 1.0875290632247925, 1.0799996852874756, 1.0828852653503418, 1.0759261846542358, 1.0956308841705322, 1.0800617933273315, 1.090516448020935, 1.0856608152389526, 1.0807973146438599, 1.0868405103683472, 1.0734519958496094, 1.0935428142547607, 1.0825685262680054, 1.0721015930175781, 1.0669643878936768, 1.0949581861495972, 1.073574185371399, 1.0699946880340576, 1.0653198957443237, 1.0671107769012451, 1.0734243392944336, 1.0907363891601562, 1.0686988830566406, 1.0674872398376465, 1.0628079175949097, 1.057944893836975, 1.053768515586853, 1.0653765201568604, 1.0483441352844238, 1.0482844114303589, 1.0367388725280762, 1.0454758405685425, 1.0474416017532349, 1.0342371463775635, 1.0166605710983276, 1.0297634601593018, 1.0170605182647705, 1.0064995288848877, 1.0137122869491577, 1.0173805952072144, 0.9984197020530701, 0.9993334412574768, 0.9925086498260498, 0.9981764554977417, 0.9585775136947632, 0.9713311195373535, 0.984574556350708, 0.9671927690505981, 0.9718725085258484, 0.9684252738952637, 0.9845297932624817, 0.958217203617096, 0.9493379592895508, 0.9562121033668518, 0.9390248656272888, 0.9513673782348633, 0.9484025835990906, 0.9495809078216553, 0.9424953460693359, 0.9370152354240417, 0.9279767870903015, 0.9389955401420593, 0.9204283356666565, 0.9393978118896484, 0.9239761829376221, 0.9270985126495361, 0.9317026734352112, 0.937532901763916, 0.9177095293998718, 0.9095105528831482, 0.9101426005363464, 0.9086929559707642, 0.9120744466781616, 0.90445476770401, 0.9129453897476196, 0.9020810723304749, 0.9117954969406128, 0.9046229124069214, 0.906022310256958, 0.9047176837921143, 0.9035883545875549, 0.9031022787094116, 0.8939792513847351, 0.9124443531036377, 0.9026919007301331, 0.9037153124809265, 0.8932586908340454, 0.9104561805725098, 0.8923999667167664], 'identifier': '6590495np'}