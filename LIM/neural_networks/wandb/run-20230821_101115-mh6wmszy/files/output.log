
 20%|███████████████████▌                                                                              | 20/100 [00:01<00:06, 11.87it/s, loss_test=1.076]
Epoch: 00, Training Loss: 0.9928, Test Loss: 1.0578
Epoch: 01, Training Loss: 0.9928, Test Loss: 1.0919
Epoch: 02, Training Loss: 0.9941, Test Loss: 1.0914
Epoch: 03, Training Loss: 0.9938, Test Loss: 1.0968
Epoch: 04, Training Loss: 0.9936, Test Loss: 1.0865
Epoch: 05, Training Loss: 0.9955, Test Loss: 1.0899
Epoch: 06, Training Loss: 0.9946, Test Loss: 1.0830
Epoch: 07, Training Loss: 0.9916, Test Loss: 1.0819
Epoch: 08, Training Loss: 0.9884, Test Loss: 1.0873
Epoch: 09, Training Loss: 0.9889, Test Loss: 1.0674
Epoch: 10, Training Loss: 0.9920, Test Loss: 1.0815
Epoch: 11, Training Loss: 0.9876, Test Loss: 1.0927
Epoch: 12, Training Loss: 0.9872, Test Loss: 1.0635
Epoch: 13, Training Loss: 0.9908, Test Loss: 1.0920
Epoch: 14, Training Loss: 0.9905, Test Loss: 1.0922
Epoch: 15, Training Loss: 0.9861, Test Loss: 1.0862
Epoch: 16, Training Loss: 0.9859, Test Loss: 1.0768
Epoch: 17, Training Loss: 0.9824, Test Loss: 1.0944
Epoch: 18, Training Loss: 0.9858, Test Loss: 1.0557

 44%|███████████████████████████████████████████                                                       | 44/100 [00:03<00:04, 12.16it/s, loss_test=1.002]
Epoch: 20, Training Loss: 0.9843, Test Loss: 1.0983
Epoch: 21, Training Loss: 0.9886, Test Loss: 1.0891
Epoch: 22, Training Loss: 0.9855, Test Loss: 1.0886
Epoch: 23, Training Loss: 0.9864, Test Loss: 1.0706
Epoch: 24, Training Loss: 0.9849, Test Loss: 1.0876
Epoch: 25, Training Loss: 0.9794, Test Loss: 1.0817
Epoch: 26, Training Loss: 0.9788, Test Loss: 1.0933
Epoch: 27, Training Loss: 0.9828, Test Loss: 1.0831
Epoch: 28, Training Loss: 0.9799, Test Loss: 1.0576
Epoch: 29, Training Loss: 0.9784, Test Loss: 1.0748
Epoch: 30, Training Loss: 0.9780, Test Loss: 1.0866
Epoch: 31, Training Loss: 0.9740, Test Loss: 1.0774
Epoch: 32, Training Loss: 0.9708, Test Loss: 1.0748
Epoch: 33, Training Loss: 0.9587, Test Loss: 1.0805
Epoch: 34, Training Loss: 0.9563, Test Loss: 1.0712
Epoch: 35, Training Loss: 0.9562, Test Loss: 1.0588
Epoch: 36, Training Loss: 0.9465, Test Loss: 1.0395
Epoch: 37, Training Loss: 0.9414, Test Loss: 1.0450
Epoch: 38, Training Loss: 0.9353, Test Loss: 1.0744
Epoch: 39, Training Loss: 0.9307, Test Loss: 1.0356
Epoch: 40, Training Loss: 0.9189, Test Loss: 1.0173
Epoch: 41, Training Loss: 0.9144, Test Loss: 1.0117
Epoch: 42, Training Loss: 0.9099, Test Loss: 1.0259
Epoch: 43, Training Loss: 0.9058, Test Loss: 1.0125

 68%|██████████████████████████████████████████████████████████████████▋                               | 68/100 [00:05<00:02, 11.74it/s, loss_test=0.924]
Epoch: 45, Training Loss: 0.8930, Test Loss: 1.0029
Epoch: 46, Training Loss: 0.8852, Test Loss: 0.9921
Epoch: 47, Training Loss: 0.8847, Test Loss: 0.9870
Epoch: 48, Training Loss: 0.8780, Test Loss: 0.9692
Epoch: 49, Training Loss: 0.8706, Test Loss: 0.9540
Epoch: 50, Training Loss: 0.8634, Test Loss: 0.9700
Epoch: 51, Training Loss: 0.8592, Test Loss: 0.9466
Epoch: 52, Training Loss: 0.8534, Test Loss: 0.9494
Epoch: 53, Training Loss: 0.8502, Test Loss: 0.9469
Epoch: 54, Training Loss: 0.8409, Test Loss: 0.9385
Epoch: 55, Training Loss: 0.8349, Test Loss: 0.9543
Epoch: 56, Training Loss: 0.8234, Test Loss: 0.9503
Epoch: 57, Training Loss: 0.8256, Test Loss: 0.9611
Epoch: 58, Training Loss: 0.8212, Test Loss: 0.9361
Epoch: 59, Training Loss: 0.8132, Test Loss: 0.9460
Epoch: 60, Training Loss: 0.8034, Test Loss: 0.9359
Epoch: 61, Training Loss: 0.8018, Test Loss: 0.9377
Epoch: 62, Training Loss: 0.7978, Test Loss: 0.9443
Epoch: 63, Training Loss: 0.7908, Test Loss: 0.9369
Epoch: 64, Training Loss: 0.7886, Test Loss: 0.9282
Epoch: 65, Training Loss: 0.7812, Test Loss: 0.9140
Epoch: 66, Training Loss: 0.7762, Test Loss: 0.9226
Epoch: 67, Training Loss: 0.7784, Test Loss: 0.9371

 94%|████████████████████████████████████████████████████████████████████████████████████████████      | 94/100 [00:07<00:00, 12.04it/s, loss_test=0.916]
Epoch: 69, Training Loss: 0.7648, Test Loss: 0.9240
Epoch: 70, Training Loss: 0.7581, Test Loss: 0.9276
Epoch: 71, Training Loss: 0.7537, Test Loss: 0.9108
Epoch: 72, Training Loss: 0.7501, Test Loss: 0.9255
Epoch: 73, Training Loss: 0.7436, Test Loss: 0.9167
Epoch: 74, Training Loss: 0.7412, Test Loss: 0.9129
Epoch: 75, Training Loss: 0.7388, Test Loss: 0.9158
Epoch: 76, Training Loss: 0.7333, Test Loss: 0.9160
Epoch: 77, Training Loss: 0.7307, Test Loss: 0.9051
Epoch: 78, Training Loss: 0.7251, Test Loss: 0.9081
Epoch: 79, Training Loss: 0.7204, Test Loss: 0.9218
Epoch: 80, Training Loss: 0.7190, Test Loss: 0.9037
Epoch: 81, Training Loss: 0.7135, Test Loss: 0.9030
Epoch: 82, Training Loss: 0.7093, Test Loss: 0.9036
Epoch: 83, Training Loss: 0.7032, Test Loss: 0.9191
Epoch: 84, Training Loss: 0.7018, Test Loss: 0.9018
Epoch: 85, Training Loss: 0.6969, Test Loss: 0.9124
Epoch: 86, Training Loss: 0.6921, Test Loss: 0.9274
Epoch: 87, Training Loss: 0.6891, Test Loss: 0.9160
Epoch: 88, Training Loss: 0.6854, Test Loss: 0.9238
Epoch: 89, Training Loss: 0.6834, Test Loss: 0.9281
Epoch: 90, Training Loss: 0.6778, Test Loss: 0.9121
Epoch: 91, Training Loss: 0.6764, Test Loss: 0.9121
Epoch: 92, Training Loss: 0.6732, Test Loss: 0.9177

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.09it/s, loss_test=0.918]
Epoch: 94, Training Loss: 0.6646, Test Loss: 0.9325
Epoch: 95, Training Loss: 0.6607, Test Loss: 0.9226
Epoch: 96, Training Loss: 0.6583, Test Loss: 0.9271
Epoch: 97, Training Loss: 0.6548, Test Loss: 0.9298
Epoch: 98, Training Loss: 0.6513, Test Loss: 0.9192
Epoch: 99, Training Loss: 0.6465, Test Loss: 0.9181
Model saved as model_5065343np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-12170000-5065343np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 100, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9928461313247681, 0.992776358127594, 0.9941147089004516, 0.9937878966331481, 0.9935676217079162, 0.9954782009124756, 0.9945948004722596, 0.9916224360466004, 0.9884472250938415, 0.9889373183250427, 0.9920477390289306, 0.987646734714508, 0.9871608018875122, 0.9907604098320008, 0.9905076384544372, 0.9861468434333801, 0.985862421989441, 0.9824097633361817, 0.9857573866844177, 0.9871511459350586, 0.9843290686607361, 0.9885900020599365, 0.9854787588119507, 0.9863657474517822, 0.9848978877067566, 0.9793536543846131, 0.9788442730903626, 0.9827835440635682, 0.9798545122146607, 0.9784009099006653, 0.9780221223831177, 0.9739505767822265, 0.9707607746124267, 0.9586511611938476, 0.9563105225563049, 0.9562161207199097, 0.9465126037597656, 0.9414418578147888, 0.9352886199951171, 0.9307136535644531, 0.9188909411430359, 0.9144183158874511, 0.9098950982093811, 0.9057618975639343, 0.8986961841583252, 0.8929701566696167, 0.885221517086029, 0.8846899032592773, 0.8780181765556335, 0.8706367135047912, 0.863415002822876, 0.8591538310050965, 0.8534448862075805, 0.8501694798469543, 0.8409084677696228, 0.8348969221115112, 0.8234385013580322, 0.8255760669708252, 0.8212092041969299, 0.8132171511650086, 0.8034448862075806, 0.8018227815628052, 0.7977744460105896, 0.7908253312110901, 0.788642168045044, 0.7812309026718139, 0.7761845111846923, 0.7784207344055176, 0.7704260349273682, 0.76481614112854, 0.7581496238708496, 0.753665280342102, 0.7500676512718201, 0.7436013340950012, 0.7411696791648865, 0.7387865543365478, 0.733340609073639, 0.73068265914917, 0.7251448631286621, 0.7204469442367554, 0.719044315814972, 0.7134934663772583, 0.7093221306800842, 0.7031934857368469, 0.7018149852752685, 0.6969264864921569, 0.6921244621276855, 0.6890652418136597, 0.685381007194519, 0.6833632111549377, 0.6777906775474548, 0.676448667049408, 0.6731608152389527, 0.6658391118049621, 0.6646272659301757, 0.6607415556907654, 0.6583172678947449, 0.6548297166824341, 0.6513368248939514, 0.6465203523635864], 'loss_test': [1.0578298568725586, 1.0918738842010498, 1.0913875102996826, 1.0968265533447266, 1.0865083932876587, 1.0899087190628052, 1.0830274820327759, 1.0819106101989746, 1.08732271194458, 1.0673915147781372, 1.0815335512161255, 1.0927150249481201, 1.0635151863098145, 1.092015027999878, 1.0921502113342285, 1.086230993270874, 1.0767889022827148, 1.0944336652755737, 1.0557153224945068, 1.0755960941314697, 1.0982586145401, 1.089058518409729, 1.0886417627334595, 1.0705938339233398, 1.08756685256958, 1.081744909286499, 1.0932942628860474, 1.0830624103546143, 1.057610034942627, 1.074784517288208, 1.0865848064422607, 1.077440619468689, 1.0747532844543457, 1.0804941654205322, 1.0711634159088135, 1.05882728099823, 1.0395002365112305, 1.045002818107605, 1.0744071006774902, 1.0356192588806152, 1.0172561407089233, 1.0116841793060303, 1.0258941650390625, 1.0124750137329102, 1.002147912979126, 1.0029454231262207, 0.9920615553855896, 0.987013578414917, 0.9691697955131531, 0.9540033340454102, 0.9699620604515076, 0.9466217160224915, 0.949404776096344, 0.946928083896637, 0.9384857416152954, 0.9542962312698364, 0.9502753615379333, 0.9610902667045593, 0.9361141920089722, 0.9459695219993591, 0.9359363317489624, 0.9376639127731323, 0.9443391561508179, 0.9369484782218933, 0.9282089471817017, 0.9139665365219116, 0.9225579500198364, 0.9371097087860107, 0.9237401485443115, 0.923972487449646, 0.9275909066200256, 0.910812258720398, 0.9255021810531616, 0.9167272448539734, 0.912933886051178, 0.9157785177230835, 0.9160352349281311, 0.9050525426864624, 0.9081069231033325, 0.9217755794525146, 0.9036514759063721, 0.903029203414917, 0.9035580158233643, 0.9190647006034851, 0.9018468260765076, 0.9123960137367249, 0.9274165034294128, 0.9160095453262329, 0.9237632155418396, 0.9281322956085205, 0.9121041297912598, 0.9120647311210632, 0.9177391529083252, 0.9159696102142334, 0.9325382709503174, 0.9226111769676208, 0.9270502924919128, 0.9298323392868042, 0.9191784262657166, 0.9180629253387451], 'identifier': '5065343np'}