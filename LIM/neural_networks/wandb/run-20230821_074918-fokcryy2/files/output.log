
  8%|███████▊                                                                                          | 20/250 [00:01<00:19, 11.95it/s, loss_test=1.089]
Epoch: 00, Training Loss: 0.9943, Test Loss: 1.0768
Epoch: 01, Training Loss: 0.9913, Test Loss: 1.0606
Epoch: 02, Training Loss: 0.9950, Test Loss: 1.0746
Epoch: 03, Training Loss: 0.9953, Test Loss: 1.0832
Epoch: 04, Training Loss: 0.9940, Test Loss: 1.0805
Epoch: 05, Training Loss: 0.9904, Test Loss: 1.0963
Epoch: 06, Training Loss: 0.9890, Test Loss: 1.0688
Epoch: 07, Training Loss: 0.9954, Test Loss: 1.0801
Epoch: 08, Training Loss: 0.9920, Test Loss: 1.0853
Epoch: 09, Training Loss: 0.9928, Test Loss: 1.0717
Epoch: 10, Training Loss: 0.9860, Test Loss: 1.0828
Epoch: 11, Training Loss: 0.9878, Test Loss: 1.0928
Epoch: 12, Training Loss: 0.9894, Test Loss: 1.0837
Epoch: 13, Training Loss: 0.9841, Test Loss: 1.0754
Epoch: 14, Training Loss: 0.9825, Test Loss: 1.0709
Epoch: 15, Training Loss: 0.9879, Test Loss: 1.1054
Epoch: 16, Training Loss: 0.9870, Test Loss: 1.0688
Epoch: 17, Training Loss: 0.9888, Test Loss: 1.0804
Epoch: 18, Training Loss: 0.9866, Test Loss: 1.0875

 18%|█████████████████▏                                                                                | 44/250 [00:03<00:17, 11.81it/s, loss_test=1.038]
Epoch: 20, Training Loss: 0.9800, Test Loss: 1.0844
Epoch: 21, Training Loss: 0.9823, Test Loss: 1.0930
Epoch: 22, Training Loss: 0.9820, Test Loss: 1.0902
Epoch: 23, Training Loss: 0.9857, Test Loss: 1.0939
Epoch: 24, Training Loss: 0.9834, Test Loss: 1.0964
Epoch: 25, Training Loss: 0.9849, Test Loss: 1.0962
Epoch: 26, Training Loss: 0.9792, Test Loss: 1.0729
Epoch: 27, Training Loss: 0.9782, Test Loss: 1.0844
Epoch: 28, Training Loss: 0.9701, Test Loss: 1.0852
Epoch: 29, Training Loss: 0.9724, Test Loss: 1.0702
Epoch: 30, Training Loss: 0.9620, Test Loss: 1.0770
Epoch: 31, Training Loss: 0.9602, Test Loss: 1.0661
Epoch: 32, Training Loss: 0.9567, Test Loss: 1.0640
Epoch: 33, Training Loss: 0.9527, Test Loss: 1.0735
Epoch: 34, Training Loss: 0.9457, Test Loss: 1.0614
Epoch: 35, Training Loss: 0.9463, Test Loss: 1.0702
Epoch: 36, Training Loss: 0.9471, Test Loss: 1.0631
Epoch: 37, Training Loss: 0.9359, Test Loss: 1.0571
Epoch: 38, Training Loss: 0.9318, Test Loss: 1.0443
Epoch: 39, Training Loss: 0.9302, Test Loss: 1.0395
Epoch: 40, Training Loss: 0.9285, Test Loss: 1.0521
Epoch: 41, Training Loss: 0.9242, Test Loss: 1.0538
Epoch: 42, Training Loss: 0.9159, Test Loss: 1.0268
Epoch: 43, Training Loss: 0.9131, Test Loss: 1.0378
Epoch: 44, Training Loss: 0.9109, Test Loss: 1.0376
Epoch: 45, Training Loss: 0.9074, Test Loss: 1.0099
Epoch: 46, Training Loss: 0.9036, Test Loss: 1.0136
Epoch: 47, Training Loss: 0.8989, Test Loss: 1.0078
Epoch: 48, Training Loss: 0.8972, Test Loss: 1.0139
Epoch: 49, Training Loss: 0.8956, Test Loss: 1.0002
Epoch: 50, Training Loss: 0.8877, Test Loss: 0.9994
Epoch: 51, Training Loss: 0.8872, Test Loss: 0.9879
Epoch: 52, Training Loss: 0.8860, Test Loss: 0.9979
Epoch: 53, Training Loss: 0.8812, Test Loss: 1.0040
Epoch: 54, Training Loss: 0.8782, Test Loss: 0.9939
Epoch: 55, Training Loss: 0.8718, Test Loss: 1.0035
Epoch: 56, Training Loss: 0.8732, Test Loss: 0.9843
Epoch: 57, Training Loss: 0.8661, Test Loss: 0.9918
Epoch: 58, Training Loss: 0.8643, Test Loss: 0.9716
Epoch: 59, Training Loss: 0.8592, Test Loss: 0.9776
Epoch: 60, Training Loss: 0.8540, Test Loss: 0.9698
Epoch: 61, Training Loss: 0.8517, Test Loss: 0.9694
Epoch: 62, Training Loss: 0.8500, Test Loss: 0.9613
Epoch: 63, Training Loss: 0.8445, Test Loss: 0.9661
Epoch: 64, Training Loss: 0.8389, Test Loss: 0.9684
Epoch: 65, Training Loss: 0.8337, Test Loss: 0.9554
Epoch: 66, Training Loss: 0.8334, Test Loss: 0.9597
Epoch: 67, Training Loss: 0.8287, Test Loss: 0.9297
Epoch: 68, Training Loss: 0.8237, Test Loss: 0.9390

 28%|███████████████████████████▍                                                                      | 70/250 [00:05<00:14, 12.73it/s, loss_test=0.941]
Epoch: 70, Training Loss: 0.8148, Test Loss: 0.9289
Epoch: 71, Training Loss: 0.8073, Test Loss: 0.9448
Epoch: 72, Training Loss: 0.8055, Test Loss: 0.9265
Epoch: 73, Training Loss: 0.7975, Test Loss: 0.9316
Epoch: 74, Training Loss: 0.7965, Test Loss: 0.9390
Epoch: 75, Training Loss: 0.7942, Test Loss: 0.9302
Epoch: 76, Training Loss: 0.7851, Test Loss: 0.9324
Epoch: 77, Training Loss: 0.7792, Test Loss: 0.9262
Epoch: 78, Training Loss: 0.7770, Test Loss: 0.9204
Epoch: 79, Training Loss: 0.7727, Test Loss: 0.9395
Epoch: 80, Training Loss: 0.7665, Test Loss: 0.9223
Epoch: 81, Training Loss: 0.7628, Test Loss: 0.9085
Epoch: 82, Training Loss: 0.7549, Test Loss: 0.9130
Epoch: 83, Training Loss: 0.7530, Test Loss: 0.9187
Epoch: 84, Training Loss: 0.7444, Test Loss: 0.9229
Epoch: 85, Training Loss: 0.7390, Test Loss: 0.9178
Epoch: 86, Training Loss: 0.7369, Test Loss: 0.9234
Epoch: 87, Training Loss: 0.7289, Test Loss: 0.9142
Epoch: 88, Training Loss: 0.7300, Test Loss: 0.9061
Epoch: 89, Training Loss: 0.7223, Test Loss: 0.9035
Epoch: 90, Training Loss: 0.7173, Test Loss: 0.9194
Epoch: 91, Training Loss: 0.7129, Test Loss: 0.9234
Epoch: 92, Training Loss: 0.7089, Test Loss: 0.9021

 38%|████████████████████████████████████▊                                                             | 94/250 [00:07<00:12, 12.15it/s, loss_test=0.917]
Epoch: 94, Training Loss: 0.6997, Test Loss: 0.9141
Epoch: 95, Training Loss: 0.6940, Test Loss: 0.9018
Epoch: 96, Training Loss: 0.6878, Test Loss: 0.9039
Epoch: 97, Training Loss: 0.6839, Test Loss: 0.8923
Epoch: 98, Training Loss: 0.6801, Test Loss: 0.8974
Epoch: 99, Training Loss: 0.6760, Test Loss: 0.9063
Epoch: 100, Training Loss: 0.6722, Test Loss: 0.8945
Epoch: 101, Training Loss: 0.6688, Test Loss: 0.9065
Epoch: 102, Training Loss: 0.6620, Test Loss: 0.9082
Epoch: 103, Training Loss: 0.6593, Test Loss: 0.9104
Epoch: 104, Training Loss: 0.6556, Test Loss: 0.9033
Epoch: 105, Training Loss: 0.6530, Test Loss: 0.9064
Epoch: 106, Training Loss: 0.6459, Test Loss: 0.9132
Epoch: 107, Training Loss: 0.6415, Test Loss: 0.9033
Epoch: 108, Training Loss: 0.6387, Test Loss: 0.8982
Epoch: 109, Training Loss: 0.6357, Test Loss: 0.9176
Epoch: 110, Training Loss: 0.6324, Test Loss: 0.9078
Epoch: 111, Training Loss: 0.6259, Test Loss: 0.9143
Epoch: 112, Training Loss: 0.6221, Test Loss: 0.8971
Epoch: 113, Training Loss: 0.6231, Test Loss: 0.9047
Epoch: 114, Training Loss: 0.6155, Test Loss: 0.9153
Epoch: 115, Training Loss: 0.6132, Test Loss: 0.9045
Epoch: 116, Training Loss: 0.6072, Test Loss: 0.9063


 57%|███████████████████████████████████████████████████████                                          | 142/250 [00:11<00:09, 11.84it/s, loss_test=0.918]
Epoch: 118, Training Loss: 0.6023, Test Loss: 0.9150
Epoch: 119, Training Loss: 0.5978, Test Loss: 0.9224
Epoch: 120, Training Loss: 0.5939, Test Loss: 0.9210
Epoch: 121, Training Loss: 0.5913, Test Loss: 0.9087
Epoch: 122, Training Loss: 0.5879, Test Loss: 0.9218
Epoch: 123, Training Loss: 0.5840, Test Loss: 0.9146
Epoch: 124, Training Loss: 0.5816, Test Loss: 0.9119
Epoch: 125, Training Loss: 0.5771, Test Loss: 0.9111
Epoch: 126, Training Loss: 0.5755, Test Loss: 0.9127
Epoch: 127, Training Loss: 0.5715, Test Loss: 0.9153
Epoch: 128, Training Loss: 0.5675, Test Loss: 0.9257
Epoch: 129, Training Loss: 0.5666, Test Loss: 0.9195
Epoch: 130, Training Loss: 0.5635, Test Loss: 0.9093
Epoch: 131, Training Loss: 0.5610, Test Loss: 0.9159
Epoch: 132, Training Loss: 0.5560, Test Loss: 0.9260
Epoch: 133, Training Loss: 0.5549, Test Loss: 0.9301
Epoch: 134, Training Loss: 0.5512, Test Loss: 0.9188
Epoch: 135, Training Loss: 0.5483, Test Loss: 0.9093
Epoch: 136, Training Loss: 0.5465, Test Loss: 0.9225
Epoch: 137, Training Loss: 0.5418, Test Loss: 0.9154
Epoch: 138, Training Loss: 0.5384, Test Loss: 0.9233
Epoch: 139, Training Loss: 0.5389, Test Loss: 0.9344
Epoch: 140, Training Loss: 0.5324, Test Loss: 0.9315
Epoch: 141, Training Loss: 0.5294, Test Loss: 0.9185
Epoch: 142, Training Loss: 0.5264, Test Loss: 0.9177
Epoch: 143, Training Loss: 0.5235, Test Loss: 0.9352
Epoch: 144, Training Loss: 0.5189, Test Loss: 0.9207
Epoch: 145, Training Loss: 0.5193, Test Loss: 0.9418
Epoch: 146, Training Loss: 0.5170, Test Loss: 0.9299
Epoch: 147, Training Loss: 0.5117, Test Loss: 0.9329
Epoch: 148, Training Loss: 0.5089, Test Loss: 0.9334
Epoch: 149, Training Loss: 0.5061, Test Loss: 0.9166
Epoch: 150, Training Loss: 0.5002, Test Loss: 0.9244
Epoch: 151, Training Loss: 0.4996, Test Loss: 0.9357
Epoch: 152, Training Loss: 0.4981, Test Loss: 0.9377
Epoch: 153, Training Loss: 0.4950, Test Loss: 0.9236
Epoch: 154, Training Loss: 0.4915, Test Loss: 0.9342
Epoch: 155, Training Loss: 0.4895, Test Loss: 0.9373
Epoch: 156, Training Loss: 0.4877, Test Loss: 0.9410
Epoch: 157, Training Loss: 0.4833, Test Loss: 0.9269
Epoch: 158, Training Loss: 0.4814, Test Loss: 0.9536
Epoch: 159, Training Loss: 0.4793, Test Loss: 0.9508
Epoch: 160, Training Loss: 0.4757, Test Loss: 0.9467
Epoch: 161, Training Loss: 0.4734, Test Loss: 0.9507
Epoch: 162, Training Loss: 0.4694, Test Loss: 0.9552
Epoch: 163, Training Loss: 0.4669, Test Loss: 0.9550
Epoch: 164, Training Loss: 0.4649, Test Loss: 0.9422
Epoch: 165, Training Loss: 0.4615, Test Loss: 0.9297

 66%|████████████████████████████████████████████████████████████████▍                                | 166/250 [00:13<00:06, 12.57it/s, loss_test=0.956]
Epoch: 167, Training Loss: 0.4554, Test Loss: 0.9658
Epoch: 168, Training Loss: 0.4556, Test Loss: 0.9439
Epoch: 169, Training Loss: 0.4534, Test Loss: 0.9414
Epoch: 170, Training Loss: 0.4505, Test Loss: 0.9535
Epoch: 171, Training Loss: 0.4473, Test Loss: 0.9536
Epoch: 172, Training Loss: 0.4448, Test Loss: 0.9494
Epoch: 173, Training Loss: 0.4423, Test Loss: 0.9532
Epoch: 174, Training Loss: 0.4378, Test Loss: 0.9608
Epoch: 175, Training Loss: 0.4377, Test Loss: 0.9659
Epoch: 176, Training Loss: 0.4364, Test Loss: 0.9535
Epoch: 177, Training Loss: 0.4322, Test Loss: 0.9600
Epoch: 178, Training Loss: 0.4285, Test Loss: 0.9762
Epoch: 179, Training Loss: 0.4269, Test Loss: 0.9620
Epoch: 180, Training Loss: 0.4247, Test Loss: 0.9606
Epoch: 181, Training Loss: 0.4224, Test Loss: 0.9639
Epoch: 182, Training Loss: 0.4196, Test Loss: 0.9667
Epoch: 183, Training Loss: 0.4170, Test Loss: 0.9740
Epoch: 184, Training Loss: 0.4154, Test Loss: 0.9722
Epoch: 185, Training Loss: 0.4127, Test Loss: 0.9725
Epoch: 186, Training Loss: 0.4114, Test Loss: 0.9658
Epoch: 187, Training Loss: 0.4080, Test Loss: 0.9694
Epoch: 188, Training Loss: 0.4058, Test Loss: 0.9721
Epoch: 189, Training Loss: 0.4047, Test Loss: 0.9669
Epoch: 190, Training Loss: 0.4022, Test Loss: 0.9784

 77%|██████████████████████████████████████████████████████████████████████████▍                      | 192/250 [00:15<00:04, 12.19it/s, loss_test=0.985]
Epoch: 192, Training Loss: 0.3985, Test Loss: 0.9737
Epoch: 193, Training Loss: 0.3947, Test Loss: 0.9862
Epoch: 194, Training Loss: 0.3936, Test Loss: 0.9897
Epoch: 195, Training Loss: 0.3898, Test Loss: 0.9821
Epoch: 196, Training Loss: 0.3898, Test Loss: 0.9882
Epoch: 197, Training Loss: 0.3872, Test Loss: 0.9823
Epoch: 198, Training Loss: 0.3839, Test Loss: 0.9847
Epoch: 199, Training Loss: 0.3815, Test Loss: 0.9893
Epoch: 200, Training Loss: 0.3808, Test Loss: 0.9826
Epoch: 201, Training Loss: 0.3769, Test Loss: 0.9979
Epoch: 202, Training Loss: 0.3775, Test Loss: 0.9957
Epoch: 203, Training Loss: 0.3742, Test Loss: 0.9940
Epoch: 204, Training Loss: 0.3719, Test Loss: 1.0095
Epoch: 205, Training Loss: 0.3701, Test Loss: 0.9907
Epoch: 206, Training Loss: 0.3675, Test Loss: 0.9928
Epoch: 207, Training Loss: 0.3664, Test Loss: 1.0050
Epoch: 208, Training Loss: 0.3645, Test Loss: 1.0027
Epoch: 209, Training Loss: 0.3623, Test Loss: 1.0028
Epoch: 210, Training Loss: 0.3598, Test Loss: 1.0009
Epoch: 211, Training Loss: 0.3576, Test Loss: 1.0079
Epoch: 212, Training Loss: 0.3568, Test Loss: 0.9961
Epoch: 213, Training Loss: 0.3550, Test Loss: 0.9892
Epoch: 214, Training Loss: 0.3526, Test Loss: 1.0180


 96%|█████████████████████████████████████████████████████████████████████████████████████████████    | 240/250 [00:19<00:00, 12.47it/s, loss_test=1.024]
Epoch: 216, Training Loss: 0.3494, Test Loss: 0.9861
Epoch: 217, Training Loss: 0.3471, Test Loss: 1.0141
Epoch: 218, Training Loss: 0.3445, Test Loss: 1.0072
Epoch: 219, Training Loss: 0.3430, Test Loss: 1.0035
Epoch: 220, Training Loss: 0.3414, Test Loss: 1.0068
Epoch: 221, Training Loss: 0.3400, Test Loss: 1.0037
Epoch: 222, Training Loss: 0.3386, Test Loss: 1.0084
Epoch: 223, Training Loss: 0.3360, Test Loss: 1.0046
Epoch: 224, Training Loss: 0.3348, Test Loss: 1.0190
Epoch: 225, Training Loss: 0.3328, Test Loss: 1.0243
Epoch: 226, Training Loss: 0.3319, Test Loss: 1.0187
Epoch: 227, Training Loss: 0.3291, Test Loss: 1.0079
Epoch: 228, Training Loss: 0.3272, Test Loss: 1.0199
Epoch: 229, Training Loss: 0.3256, Test Loss: 1.0037
Epoch: 230, Training Loss: 0.3254, Test Loss: 1.0323
Epoch: 231, Training Loss: 0.3227, Test Loss: 1.0219
Epoch: 232, Training Loss: 0.3221, Test Loss: 0.9971
Epoch: 233, Training Loss: 0.3195, Test Loss: 1.0095
Epoch: 234, Training Loss: 0.3183, Test Loss: 1.0137
Epoch: 235, Training Loss: 0.3176, Test Loss: 1.0169
Epoch: 236, Training Loss: 0.3147, Test Loss: 1.0151
Epoch: 237, Training Loss: 0.3126, Test Loss: 1.0254
Epoch: 238, Training Loss: 0.3115, Test Loss: 1.0077
Epoch: 239, Training Loss: 0.3104, Test Loss: 1.0229

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.16it/s, loss_test=1.047]
Epoch: 241, Training Loss: 0.3088, Test Loss: 1.0403
Epoch: 242, Training Loss: 0.3055, Test Loss: 1.0145
Epoch: 243, Training Loss: 0.3044, Test Loss: 1.0394
Epoch: 244, Training Loss: 0.3043, Test Loss: 1.0520
Epoch: 245, Training Loss: 0.3021, Test Loss: 1.0219
Epoch: 246, Training Loss: 0.3004, Test Loss: 1.0446
Epoch: 247, Training Loss: 0.2993, Test Loss: 1.0432
Epoch: 248, Training Loss: 0.2986, Test Loss: 1.0151
Epoch: 249, Training Loss: 0.2964, Test Loss: 1.0470
Model saved as model_8370277np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-122000-8370277np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.994264543056488, 0.9912615299224854, 0.9949886560440063, 0.9953147768974304, 0.9939923286437988, 0.9903805494308472, 0.9889691114425659, 0.9953885674476624, 0.9920176506042481, 0.9927747249603271, 0.9859736561775208, 0.9877660036087036, 0.9893720984458924, 0.9840776681900024, 0.9825405955314637, 0.9879192352294922, 0.9870085716247559, 0.9888332605361938, 0.9866442203521728, 0.9848129153251648, 0.9799584865570068, 0.9823236227035522, 0.9820073485374451, 0.9857027769088745, 0.9833972096443176, 0.9849167823791504, 0.9792138934135437, 0.9782171249389648, 0.9700949549674988, 0.972384262084961, 0.9620136737823486, 0.9602021813392639, 0.9566610336303711, 0.9527276158332825, 0.9457216858863831, 0.9462793946266175, 0.9470887184143066, 0.9358756422996521, 0.931801700592041, 0.9301629185676574, 0.9285122036933899, 0.9241812825202942, 0.9158557653427124, 0.9131394147872924, 0.910870087146759, 0.9074434757232666, 0.9036411285400391, 0.8988684058189392, 0.8971585154533386, 0.8956284284591675, 0.8876714468002319, 0.8871647119522095, 0.8859697818756104, 0.8812417268753052, 0.8781841039657593, 0.871783185005188, 0.8732381701469422, 0.8660812854766846, 0.8642505645751953, 0.8592315912246704, 0.8540483355522156, 0.8516613245010376, 0.8500140190124512, 0.8445106506347656, 0.8388899326324463, 0.8336926102638245, 0.8333663702011108, 0.8286595702171325, 0.8236836552619934, 0.815087080001831, 0.8148093342781066, 0.8072680950164794, 0.8055171370506287, 0.7975096464157104, 0.7964907288551331, 0.7941936254501343, 0.7851021766662598, 0.7791754961013794, 0.7770440578460693, 0.7727048397064209, 0.7665376305580139, 0.7627524852752685, 0.754872179031372, 0.7529589295387268, 0.7443862318992615, 0.7389914870262146, 0.7369042754173278, 0.7288986325263977, 0.7299568891525269, 0.72234947681427, 0.7172520160675049, 0.7129384040832519, 0.708879578113556, 0.7021152257919312, 0.6996540665626526, 0.693988811969757, 0.6877655625343323, 0.6838849425315857, 0.6801427364349365, 0.6760034799575806, 0.6721504926681519, 0.668790340423584, 0.6620184779167175, 0.6592573404312134, 0.6556112051010132, 0.6530122518539428, 0.6458614110946655, 0.6415354490280152, 0.6386566996574402, 0.6357099890708924, 0.6324345707893372, 0.6259475231170655, 0.6221498370170593, 0.6230947375297546, 0.6154903054237366, 0.6131939053535461, 0.6071529388427734, 0.6019195675849914, 0.6022977590560913, 0.5978283047676086, 0.5938817143440247, 0.5912747383117676, 0.5878806114196777, 0.5839954495429993, 0.5816462278366089, 0.5771059036254883, 0.5755155086517334, 0.5715437412261963, 0.5674760341644287, 0.5666178584098815, 0.5635283708572387, 0.560971474647522, 0.5560482859611511, 0.5548600673675537, 0.5511550545692444, 0.548272430896759, 0.5464649200439453, 0.5417506694793701, 0.5384261131286621, 0.538851523399353, 0.5324471712112426, 0.5294228553771972, 0.5264230966567993, 0.5234509110450745, 0.5188907742500305, 0.5193187475204468, 0.5170131802558899, 0.511686134338379, 0.508867758512497, 0.5060669720172882, 0.5002168416976929, 0.49963987469673155, 0.49809253215789795, 0.4950322091579437, 0.4915371000766754, 0.4895448863506317, 0.4876820623874664, 0.48328413963317873, 0.4813913881778717, 0.4792949318885803, 0.4757197678089142, 0.4733757615089417, 0.4693800151348114, 0.46688631772994993, 0.4649398744106293, 0.46145225763320924, 0.4592058062553406, 0.45539031028747556, 0.455595600605011, 0.45336257219314574, 0.45053631067276, 0.4473099887371063, 0.4448145627975464, 0.4422562062740326, 0.4378207206726074, 0.43770880699157716, 0.43643476963043215, 0.43223472237586974, 0.42848639488220214, 0.4269037365913391, 0.4246639132499695, 0.42236620783805845, 0.41955122351646423, 0.4170214653015137, 0.41541616916656493, 0.41274333000183105, 0.41135528683662415, 0.4079811453819275, 0.4058132112026215, 0.40470634698867797, 0.4022161543369293, 0.39955548048019407, 0.3984704315662384, 0.39468175172805786, 0.3936082422733307, 0.38984320163726804, 0.3897625982761383, 0.3871521413326263, 0.38385350108146665, 0.38150262236595156, 0.3808342099189758, 0.37693983912467954, 0.3774923741817474, 0.37422640323638917, 0.3718695640563965, 0.3701366126537323, 0.3675179421901703, 0.3664096236228943, 0.3644509017467499, 0.3622886180877686, 0.3598188102245331, 0.357565438747406, 0.35678589940071104, 0.35495569109916686, 0.35259494185447693, 0.35205988883972167, 0.34944167137146, 0.34707674384117126, 0.34454654455184935, 0.34301918745040894, 0.34142930507659913, 0.3400186121463776, 0.33856196999549865, 0.335965621471405, 0.3347643971443176, 0.33281390070915223, 0.33186936378479004, 0.32913382053375245, 0.327221417427063, 0.3256413459777832, 0.3253907561302185, 0.32272247076034544, 0.3220746636390686, 0.3195038974285126, 0.31827648878097536, 0.3175654947757721, 0.3146977603435516, 0.31259424686431886, 0.3114963948726654, 0.31038661003112794, 0.3083912551403046, 0.30877443552017214, 0.3055174112319946, 0.30439200401306155, 0.3042882442474365, 0.3021416664123535, 0.3004004180431366, 0.29932190775871276, 0.2985527336597443, 0.2963569343090057], 'loss_test': [1.0768048763275146, 1.0605909824371338, 1.0746341943740845, 1.08316969871521, 1.0804675817489624, 1.0962611436843872, 1.068794846534729, 1.0801020860671997, 1.0853042602539062, 1.0717127323150635, 1.0827745199203491, 1.0927871465682983, 1.0837081670761108, 1.0754207372665405, 1.0708822011947632, 1.1053979396820068, 1.068767786026001, 1.0803709030151367, 1.0875452756881714, 1.089442491531372, 1.0844277143478394, 1.0929991006851196, 1.0902460813522339, 1.0939468145370483, 1.0963801145553589, 1.0962039232254028, 1.0728756189346313, 1.0843838453292847, 1.0851510763168335, 1.0702029466629028, 1.0769764184951782, 1.0660974979400635, 1.064039707183838, 1.0734893083572388, 1.0613598823547363, 1.0702018737792969, 1.0631409883499146, 1.0570918321609497, 1.0442668199539185, 1.0395143032073975, 1.0520834922790527, 1.0537669658660889, 1.0267908573150635, 1.0378272533416748, 1.0376187562942505, 1.009896993637085, 1.0135663747787476, 1.0077893733978271, 1.0138810873031616, 1.0001864433288574, 0.9994082450866699, 0.9879232048988342, 0.9978771209716797, 1.00395929813385, 0.9938755631446838, 1.0034644603729248, 0.9843053221702576, 0.9917842745780945, 0.9716485738754272, 0.9775552153587341, 0.9698449373245239, 0.9694306254386902, 0.9612817168235779, 0.9661136269569397, 0.9684178233146667, 0.9554430246353149, 0.9596713185310364, 0.9296913146972656, 0.9389563798904419, 0.9412029981613159, 0.9288967847824097, 0.9447692036628723, 0.9264675974845886, 0.9315544962882996, 0.9390250444412231, 0.9302417635917664, 0.932424783706665, 0.9261974692344666, 0.9204235076904297, 0.9395414590835571, 0.9223151803016663, 0.9084810018539429, 0.9130451083183289, 0.9187074899673462, 0.9229177236557007, 0.9177510142326355, 0.9233542680740356, 0.9142495393753052, 0.9061157703399658, 0.903500497341156, 0.9194331169128418, 0.9233958125114441, 0.9021323323249817, 0.9165244698524475, 0.9141137003898621, 0.9017983675003052, 0.9038974642753601, 0.892315685749054, 0.8974034786224365, 0.9062859416007996, 0.8945199251174927, 0.9065322279930115, 0.9082456827163696, 0.9103888869285583, 0.9032735228538513, 0.9063639640808105, 0.9131651520729065, 0.9032766222953796, 0.8982390761375427, 0.9175896644592285, 0.9078442454338074, 0.914339542388916, 0.897101640701294, 0.9046621918678284, 0.915332555770874, 0.9044962525367737, 0.9063324332237244, 0.9209078550338745, 0.9150351881980896, 0.9223697185516357, 0.9209761023521423, 0.9086930155754089, 0.9218077063560486, 0.914569616317749, 0.9118757843971252, 0.9110702872276306, 0.9126688241958618, 0.91529780626297, 0.9256872534751892, 0.9195032715797424, 0.9092778563499451, 0.9158838987350464, 0.9259926676750183, 0.9300826191902161, 0.918797492980957, 0.9092641472816467, 0.9225262403488159, 0.9153763055801392, 0.923272967338562, 0.9343538880348206, 0.931462287902832, 0.9184972643852234, 0.9177329540252686, 0.9351510405540466, 0.9206765294075012, 0.9417710304260254, 0.9298641085624695, 0.9329283237457275, 0.9333889484405518, 0.9166244864463806, 0.924404501914978, 0.9356919527053833, 0.9377123117446899, 0.9236048460006714, 0.9342293739318848, 0.93733149766922, 0.9409961700439453, 0.9268615245819092, 0.9535537362098694, 0.9507607817649841, 0.9467489719390869, 0.950747013092041, 0.9551634192466736, 0.9549520015716553, 0.9422289133071899, 0.9296804070472717, 0.9563088417053223, 0.9657779335975647, 0.9438990354537964, 0.9413610100746155, 0.9535290598869324, 0.9536285400390625, 0.9494017362594604, 0.9531886577606201, 0.9607505202293396, 0.9659101366996765, 0.9534695148468018, 0.9600112438201904, 0.9762079119682312, 0.961989164352417, 0.9606407284736633, 0.963865339756012, 0.9666504859924316, 0.9739536046981812, 0.9722339510917664, 0.9724523425102234, 0.9657632112503052, 0.9694400429725647, 0.9720978140830994, 0.9669362902641296, 0.9783660173416138, 0.9854184985160828, 0.9737288951873779, 0.9862409234046936, 0.989654541015625, 0.9821133613586426, 0.9881753325462341, 0.982322096824646, 0.9846897125244141, 0.9893308877944946, 0.9826117753982544, 0.9978882074356079, 0.9956782460212708, 0.9940065145492554, 1.0094976425170898, 0.9906677603721619, 0.9928321242332458, 1.004998803138733, 1.0026684999465942, 1.0028116703033447, 1.0008668899536133, 1.0079143047332764, 0.9960510730743408, 0.9891812205314636, 1.0179921388626099, 0.9984923005104065, 0.9860909581184387, 1.0141477584838867, 1.007193922996521, 1.0034868717193604, 1.0067551136016846, 1.0037277936935425, 1.0083955526351929, 1.0045993328094482, 1.018988013267517, 1.024253249168396, 1.018683910369873, 1.007872223854065, 1.0198816061019897, 1.0036673545837402, 1.0323283672332764, 1.0218970775604248, 0.9971082210540771, 1.009499430656433, 1.0137066841125488, 1.0169010162353516, 1.0151406526565552, 1.025400996208191, 1.007696509361267, 1.022902011871338, 1.0238711833953857, 1.0402979850769043, 1.0144850015640259, 1.0393660068511963, 1.0520286560058594, 1.0218607187271118, 1.0446046590805054, 1.0432378053665161, 1.0150872468948364, 1.0469589233398438], 'identifier': '8370277np'}