Epoch: 00, Training Loss: 0.9943, Test Loss: 1.0791
Epoch: 01, Training Loss: 0.9953, Test Loss: 1.0692
Epoch: 02, Training Loss: 0.9909, Test Loss: 1.0680
Epoch: 03, Training Loss: 0.9938, Test Loss: 1.0755
Epoch: 04, Training Loss: 0.9872, Test Loss: 1.0994
Epoch: 05, Training Loss: 0.9862, Test Loss: 1.0654
Epoch: 06, Training Loss: 0.9903, Test Loss: 1.0862
Epoch: 07, Training Loss: 0.9921, Test Loss: 1.0816
Epoch: 08, Training Loss: 0.9902, Test Loss: 1.0786
Epoch: 09, Training Loss: 0.9843, Test Loss: 1.0680
Epoch: 10, Training Loss: 0.9893, Test Loss: 1.0823
Epoch: 11, Training Loss: 0.9934, Test Loss: 1.0655
Epoch: 12, Training Loss: 0.9857, Test Loss: 1.0821
Epoch: 13, Training Loss: 0.9864, Test Loss: 1.0733
Epoch: 14, Training Loss: 0.9899, Test Loss: 1.0821
Epoch: 15, Training Loss: 0.9888, Test Loss: 1.0788
Epoch: 16, Training Loss: 0.9842, Test Loss: 1.0766
Epoch: 17, Training Loss: 0.9884, Test Loss: 1.0782
Epoch: 18, Training Loss: 0.9844, Test Loss: 1.0922
Epoch: 19, Training Loss: 0.9801, Test Loss: 1.0876
Epoch: 20, Training Loss: 0.9871, Test Loss: 1.0662
 20%|███████████████████▌                                                                              | 20/100 [00:01<00:06, 11.86it/s, loss_test=1.088]
Epoch: 21, Training Loss: 0.9857, Test Loss: 1.0852
Epoch: 22, Training Loss: 0.9850, Test Loss: 1.0782
Epoch: 23, Training Loss: 0.9810, Test Loss: 1.0867
Epoch: 24, Training Loss: 0.9819, Test Loss: 1.0826
Epoch: 25, Training Loss: 0.9829, Test Loss: 1.0746
Epoch: 26, Training Loss: 0.9840, Test Loss: 1.0919
Epoch: 27, Training Loss: 0.9766, Test Loss: 1.0917
Epoch: 28, Training Loss: 0.9789, Test Loss: 1.0827
Epoch: 29, Training Loss: 0.9803, Test Loss: 1.0777
Epoch: 30, Training Loss: 0.9762, Test Loss: 1.0663
Epoch: 31, Training Loss: 0.9748, Test Loss: 1.0525
Epoch: 32, Training Loss: 0.9688, Test Loss: 1.0493
Epoch: 33, Training Loss: 0.9602, Test Loss: 1.0650
Epoch: 34, Training Loss: 0.9499, Test Loss: 1.0437
Epoch: 35, Training Loss: 0.9402, Test Loss: 1.0345
Epoch: 36, Training Loss: 0.9372, Test Loss: 1.0312
Epoch: 37, Training Loss: 0.9245, Test Loss: 1.0146
Epoch: 38, Training Loss: 0.9220, Test Loss: 1.0039
Epoch: 39, Training Loss: 0.9110, Test Loss: 0.9938
Epoch: 40, Training Loss: 0.9129, Test Loss: 0.9848
Epoch: 41, Training Loss: 0.9081, Test Loss: 0.9952
Epoch: 42, Training Loss: 0.9004, Test Loss: 0.9830
Epoch: 43, Training Loss: 0.8937, Test Loss: 0.9882

 44%|███████████████████████████████████████████                                                       | 44/100 [00:03<00:04, 12.49it/s, loss_test=0.959]
Epoch: 45, Training Loss: 0.8879, Test Loss: 0.9718
Epoch: 46, Training Loss: 0.8826, Test Loss: 0.9814
Epoch: 47, Training Loss: 0.8766, Test Loss: 0.9617
Epoch: 48, Training Loss: 0.8760, Test Loss: 0.9677
Epoch: 49, Training Loss: 0.8718, Test Loss: 0.9607
Epoch: 50, Training Loss: 0.8678, Test Loss: 0.9566
Epoch: 51, Training Loss: 0.8651, Test Loss: 0.9521
Epoch: 52, Training Loss: 0.8629, Test Loss: 0.9411
Epoch: 53, Training Loss: 0.8581, Test Loss: 0.9593
Epoch: 54, Training Loss: 0.8510, Test Loss: 0.9528
Epoch: 55, Training Loss: 0.8451, Test Loss: 0.9360
Epoch: 56, Training Loss: 0.8469, Test Loss: 0.9466
Epoch: 57, Training Loss: 0.8397, Test Loss: 0.9542
Epoch: 58, Training Loss: 0.8358, Test Loss: 0.9448
Epoch: 59, Training Loss: 0.8284, Test Loss: 0.9459
Epoch: 60, Training Loss: 0.8308, Test Loss: 0.9174
Epoch: 61, Training Loss: 0.8280, Test Loss: 0.9242
Epoch: 62, Training Loss: 0.8240, Test Loss: 0.9248
Epoch: 63, Training Loss: 0.8216, Test Loss: 0.9310
Epoch: 64, Training Loss: 0.8179, Test Loss: 0.9317
Epoch: 65, Training Loss: 0.8128, Test Loss: 0.9141
Epoch: 66, Training Loss: 0.8108, Test Loss: 0.9436
Epoch: 67, Training Loss: 0.8094, Test Loss: 0.9229
Epoch: 68, Training Loss: 0.8034, Test Loss: 0.9118


 92%|██████████████████████████████████████████████████████████████████████████████████████████▏       | 92/100 [00:07<00:00, 11.94it/s, loss_test=0.881]
Epoch: 70, Training Loss: 0.7999, Test Loss: 0.9210
Epoch: 71, Training Loss: 0.7932, Test Loss: 0.9351
Epoch: 72, Training Loss: 0.7881, Test Loss: 0.9125
Epoch: 73, Training Loss: 0.7849, Test Loss: 0.9061
Epoch: 74, Training Loss: 0.7805, Test Loss: 0.9078
Epoch: 75, Training Loss: 0.7796, Test Loss: 0.8982
Epoch: 76, Training Loss: 0.7748, Test Loss: 0.9156
Epoch: 77, Training Loss: 0.7685, Test Loss: 0.9194
Epoch: 78, Training Loss: 0.7632, Test Loss: 0.9088
Epoch: 79, Training Loss: 0.7590, Test Loss: 0.9032
Epoch: 80, Training Loss: 0.7547, Test Loss: 0.8986
Epoch: 81, Training Loss: 0.7501, Test Loss: 0.8971
Epoch: 82, Training Loss: 0.7467, Test Loss: 0.9066
Epoch: 83, Training Loss: 0.7398, Test Loss: 0.8981
Epoch: 84, Training Loss: 0.7374, Test Loss: 0.9039
Epoch: 85, Training Loss: 0.7344, Test Loss: 0.9044
Epoch: 86, Training Loss: 0.7298, Test Loss: 0.8955
Epoch: 87, Training Loss: 0.7248, Test Loss: 0.8993
Epoch: 88, Training Loss: 0.7189, Test Loss: 0.8911
Epoch: 89, Training Loss: 0.7147, Test Loss: 0.8979
Epoch: 90, Training Loss: 0.7106, Test Loss: 0.9034
Epoch: 91, Training Loss: 0.7058, Test Loss: 0.9117

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.00it/s, loss_test=0.899]
Epoch: 93, Training Loss: 0.6980, Test Loss: 0.8993
Epoch: 94, Training Loss: 0.6937, Test Loss: 0.9079
Epoch: 95, Training Loss: 0.6890, Test Loss: 0.9018
Epoch: 96, Training Loss: 0.6848, Test Loss: 0.9015
Epoch: 97, Training Loss: 0.6797, Test Loss: 0.8923
Epoch: 98, Training Loss: 0.6769, Test Loss: 0.8993
Epoch: 99, Training Loss: 0.6737, Test Loss: 0.8992
Model saved as model_536230np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-128000-536230np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 100, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9942553043365479, 0.9953358888626098, 0.9908782601356506, 0.9938241362571716, 0.9872282862663269, 0.9861947536468506, 0.99028400182724, 0.9920634508132935, 0.9902355432510376, 0.9842963218688965, 0.9892831921577454, 0.9933699846267701, 0.9857003211975097, 0.9864068269729614, 0.989921760559082, 0.9888001322746277, 0.9842311263084411, 0.9883868217468261, 0.9843984723091126, 0.9801041960716248, 0.9870754480361938, 0.9857113718986511, 0.9850352525711059, 0.9809659242630004, 0.9818622469902039, 0.9828906416893005, 0.9840304374694824, 0.9765619993209839, 0.97885662317276, 0.9803252816200256, 0.9761763930320739, 0.974832558631897, 0.9687645435333252, 0.9601834893226624, 0.9498656988143921, 0.9402209997177124, 0.9371742606163025, 0.9244944810867309, 0.9219867587089539, 0.9109767079353333, 0.9128749966621399, 0.9080807566642761, 0.9003535985946656, 0.8937381267547607, 0.8931358575820922, 0.8878767371177674, 0.8825784921646118, 0.8766149640083313, 0.875968074798584, 0.871760892868042, 0.8677841544151306, 0.8651333212852478, 0.8629290580749511, 0.8581335067749023, 0.8509861350059509, 0.8451341986656189, 0.8469410181045532, 0.8397014617919922, 0.8357773661613465, 0.8283526301383972, 0.8307595133781434, 0.8279962658882141, 0.8239940881729126, 0.821618664264679, 0.8178592562675476, 0.8127755403518677, 0.8107752084732056, 0.8094417095184326, 0.8034222245216369, 0.7988728880882263, 0.7999448657035828, 0.7931766867637634, 0.7881120204925537, 0.7848552227020263, 0.7804580807685852, 0.7796186089515686, 0.7747906923294068, 0.7684897422790528, 0.76324462890625, 0.7590227723121643, 0.7546560525894165, 0.7501051306724549, 0.7466641664505005, 0.7397728323936462, 0.7374096512794495, 0.7343767642974853, 0.7297648072242737, 0.7247530817985535, 0.71894211769104, 0.7147223591804505, 0.7105637311935424, 0.7057803869247437, 0.7001811861991882, 0.6979633688926696, 0.6936914324760437, 0.6889901995658875, 0.6847900032997132, 0.6797402262687683, 0.6768680214881897, 0.6737100481987], 'loss_test': [1.0790973901748657, 1.0691789388656616, 1.0680230855941772, 1.0755130052566528, 1.0994329452514648, 1.0654075145721436, 1.086186408996582, 1.0815849304199219, 1.0785740613937378, 1.067979335784912, 1.0822523832321167, 1.065487265586853, 1.0821120738983154, 1.0733287334442139, 1.0820785760879517, 1.0787930488586426, 1.0766050815582275, 1.07816481590271, 1.092166543006897, 1.0875508785247803, 1.0662211179733276, 1.0852328538894653, 1.0782344341278076, 1.086675763130188, 1.0826395750045776, 1.0745865106582642, 1.09192955493927, 1.0917479991912842, 1.0826566219329834, 1.0776588916778564, 1.066300392150879, 1.052507996559143, 1.0492899417877197, 1.0650012493133545, 1.0436694622039795, 1.0345449447631836, 1.0311983823776245, 1.0145761966705322, 1.003867506980896, 0.9937542676925659, 0.9847708344459534, 0.9952423572540283, 0.9829544425010681, 0.9882418513298035, 0.9585629105567932, 0.9718384146690369, 0.9814199209213257, 0.9617176055908203, 0.9676563739776611, 0.9607352614402771, 0.9565756320953369, 0.9520560503005981, 0.941145658493042, 0.9592705965042114, 0.952826976776123, 0.9360373020172119, 0.9466239213943481, 0.954207718372345, 0.9448321461677551, 0.9458938837051392, 0.9174041748046875, 0.9241804480552673, 0.9248355031013489, 0.9309834241867065, 0.9316540360450745, 0.9141327738761902, 0.9435846209526062, 0.9229326844215393, 0.9117955565452576, 0.9221767783164978, 0.921006441116333, 0.9350864887237549, 0.9125250577926636, 0.9061145782470703, 0.9077891111373901, 0.8981706500053406, 0.9155722856521606, 0.9193500876426697, 0.9088373184204102, 0.903170645236969, 0.8985829949378967, 0.8970513343811035, 0.9065753817558289, 0.8981009125709534, 0.9039067029953003, 0.9043557047843933, 0.895463228225708, 0.8993043899536133, 0.8911164999008179, 0.8979393839836121, 0.9034211039543152, 0.9117211699485779, 0.8809762001037598, 0.8993353247642517, 0.9079344272613525, 0.9018217325210571, 0.9014892578125, 0.8923041820526123, 0.8993450999259949, 0.8992170095443726], 'identifier': '536230np'}