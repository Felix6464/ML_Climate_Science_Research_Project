Epoch: 00, Training Loss: 0.9920, Test Loss: 1.0770
Epoch: 01, Training Loss: 0.9935, Test Loss: 1.0694
Epoch: 02, Training Loss: 0.9959, Test Loss: 1.0671
Epoch: 03, Training Loss: 0.9904, Test Loss: 1.0776
Epoch: 04, Training Loss: 0.9923, Test Loss: 1.0792
Epoch: 05, Training Loss: 0.9902, Test Loss: 1.0973
Epoch: 06, Training Loss: 0.9958, Test Loss: 1.0619
Epoch: 07, Training Loss: 0.9866, Test Loss: 1.0846
Epoch: 08, Training Loss: 0.9904, Test Loss: 1.0800
Epoch: 09, Training Loss: 0.9886, Test Loss: 1.0682
Epoch: 10, Training Loss: 0.9852, Test Loss: 1.0718
Epoch: 11, Training Loss: 0.9877, Test Loss: 1.0758
Epoch: 12, Training Loss: 0.9888, Test Loss: 1.0841
Epoch: 13, Training Loss: 0.9891, Test Loss: 1.0906
Epoch: 14, Training Loss: 0.9875, Test Loss: 1.0686
Epoch: 15, Training Loss: 0.9869, Test Loss: 1.0776
Epoch: 16, Training Loss: 0.9866, Test Loss: 1.0777
Epoch: 17, Training Loss: 0.9878, Test Loss: 1.0732
Epoch: 18, Training Loss: 0.9863, Test Loss: 1.0659
Epoch: 19, Training Loss: 0.9830, Test Loss: 1.0639
Epoch: 20, Training Loss: 0.9839, Test Loss: 1.1018

 46%|█████████████████████████████████████████████                                                     | 46/100 [00:03<00:04, 12.53it/s, loss_test=1.007]
Epoch: 21, Training Loss: 0.9838, Test Loss: 1.0731
Epoch: 22, Training Loss: 0.9875, Test Loss: 1.0627
Epoch: 23, Training Loss: 0.9795, Test Loss: 1.1011
Epoch: 24, Training Loss: 0.9770, Test Loss: 1.0998
Epoch: 25, Training Loss: 0.9791, Test Loss: 1.0625
Epoch: 26, Training Loss: 0.9760, Test Loss: 1.0835
Epoch: 27, Training Loss: 0.9807, Test Loss: 1.0830
Epoch: 28, Training Loss: 0.9807, Test Loss: 1.0749
Epoch: 29, Training Loss: 0.9764, Test Loss: 1.0724
Epoch: 30, Training Loss: 0.9715, Test Loss: 1.0824
Epoch: 31, Training Loss: 0.9728, Test Loss: 1.0722
Epoch: 32, Training Loss: 0.9607, Test Loss: 1.0786
Epoch: 33, Training Loss: 0.9626, Test Loss: 1.0697
Epoch: 34, Training Loss: 0.9538, Test Loss: 1.0667
Epoch: 35, Training Loss: 0.9482, Test Loss: 1.0667
Epoch: 36, Training Loss: 0.9380, Test Loss: 1.0559
Epoch: 37, Training Loss: 0.9311, Test Loss: 1.0442
Epoch: 38, Training Loss: 0.9230, Test Loss: 1.0310
Epoch: 39, Training Loss: 0.9181, Test Loss: 1.0217
Epoch: 40, Training Loss: 0.9092, Test Loss: 1.0136
Epoch: 41, Training Loss: 0.9101, Test Loss: 1.0121
Epoch: 42, Training Loss: 0.9013, Test Loss: 0.9946
Epoch: 43, Training Loss: 0.9000, Test Loss: 1.0013
Epoch: 44, Training Loss: 0.8902, Test Loss: 1.0002
Epoch: 45, Training Loss: 0.8929, Test Loss: 1.0066
Epoch: 46, Training Loss: 0.8843, Test Loss: 0.9809
Epoch: 47, Training Loss: 0.8838, Test Loss: 0.9876
Epoch: 48, Training Loss: 0.8789, Test Loss: 0.9988
Epoch: 49, Training Loss: 0.8727, Test Loss: 0.9688
Epoch: 50, Training Loss: 0.8720, Test Loss: 0.9785
Epoch: 51, Training Loss: 0.8633, Test Loss: 0.9681
Epoch: 52, Training Loss: 0.8613, Test Loss: 0.9823
Epoch: 53, Training Loss: 0.8590, Test Loss: 0.9776
Epoch: 54, Training Loss: 0.8551, Test Loss: 0.9690
Epoch: 55, Training Loss: 0.8543, Test Loss: 0.9610
Epoch: 56, Training Loss: 0.8495, Test Loss: 0.9481
Epoch: 57, Training Loss: 0.8449, Test Loss: 0.9541
Epoch: 58, Training Loss: 0.8399, Test Loss: 0.9630
Epoch: 59, Training Loss: 0.8393, Test Loss: 0.9579
Epoch: 60, Training Loss: 0.8323, Test Loss: 0.9533
Epoch: 61, Training Loss: 0.8280, Test Loss: 0.9463
Epoch: 62, Training Loss: 0.8277, Test Loss: 0.9491
Epoch: 63, Training Loss: 0.8238, Test Loss: 0.9314
Epoch: 64, Training Loss: 0.8189, Test Loss: 0.9282
Epoch: 65, Training Loss: 0.8112, Test Loss: 0.9348
Epoch: 66, Training Loss: 0.8081, Test Loss: 0.9325
Epoch: 67, Training Loss: 0.8098, Test Loss: 0.9432
Epoch: 68, Training Loss: 0.8031, Test Loss: 0.9239
Epoch: 69, Training Loss: 0.7991, Test Loss: 0.9221

 70%|████████████████████████████████████████████████████████████████████▌                             | 70/100 [00:05<00:02, 12.07it/s, loss_test=0.933]
Epoch: 71, Training Loss: 0.7934, Test Loss: 0.9364
Epoch: 72, Training Loss: 0.7908, Test Loss: 0.9193
Epoch: 73, Training Loss: 0.7843, Test Loss: 0.9113
Epoch: 74, Training Loss: 0.7813, Test Loss: 0.9123
Epoch: 75, Training Loss: 0.7780, Test Loss: 0.9105
Epoch: 76, Training Loss: 0.7744, Test Loss: 0.9271
Epoch: 77, Training Loss: 0.7738, Test Loss: 0.9199
Epoch: 78, Training Loss: 0.7682, Test Loss: 0.9218
Epoch: 79, Training Loss: 0.7663, Test Loss: 0.9238
Epoch: 80, Training Loss: 0.7643, Test Loss: 0.9190
Epoch: 81, Training Loss: 0.7573, Test Loss: 0.9102
Epoch: 82, Training Loss: 0.7580, Test Loss: 0.9003
Epoch: 83, Training Loss: 0.7497, Test Loss: 0.9088
Epoch: 84, Training Loss: 0.7461, Test Loss: 0.9185
Epoch: 85, Training Loss: 0.7453, Test Loss: 0.9075
Epoch: 86, Training Loss: 0.7447, Test Loss: 0.9132
Epoch: 87, Training Loss: 0.7427, Test Loss: 0.9063
Epoch: 88, Training Loss: 0.7315, Test Loss: 0.8982
Epoch: 89, Training Loss: 0.7293, Test Loss: 0.9156
Epoch: 90, Training Loss: 0.7262, Test Loss: 0.9099
Epoch: 91, Training Loss: 0.7263, Test Loss: 0.9081
Epoch: 92, Training Loss: 0.7209, Test Loss: 0.9149
Epoch: 93, Training Loss: 0.7177, Test Loss: 0.9032
Epoch: 94, Training Loss: 0.7146, Test Loss: 0.9210


100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.42it/s, loss_test=0.911]
Epoch: 96, Training Loss: 0.7087, Test Loss: 0.9089
Epoch: 97, Training Loss: 0.7028, Test Loss: 0.9167
Epoch: 98, Training Loss: 0.6986, Test Loss: 0.9206
Epoch: 99, Training Loss: 0.6959, Test Loss: 0.9115
Model saved as model_8333596np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-12130000-8333596np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 100, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9919749617576599, 0.993506920337677, 0.9958718180656433, 0.9904075503349304, 0.9922725558280945, 0.9901866555213928, 0.9958095073699951, 0.9865791320800781, 0.9904135704040528, 0.9885810375213623, 0.9852453351020813, 0.9877032160758972, 0.9888421535491944, 0.9891283273696899, 0.9875377416610718, 0.9868913888931274, 0.9865695357322692, 0.9877536654472351, 0.9863104104995728, 0.9830251574516297, 0.9838647842407227, 0.9837716937065124, 0.9875041246414185, 0.9795381307601929, 0.9769945859909057, 0.979116153717041, 0.9760380029678345, 0.9806822896003723, 0.9806619048118591, 0.9764128446578979, 0.9714607834815979, 0.9727917313575745, 0.9607446908950805, 0.9626145958900452, 0.9538263320922852, 0.9481505393981934, 0.9380202412605285, 0.9311475396156311, 0.9230451464653016, 0.9180986046791076, 0.9091924786567688, 0.9101075887680053, 0.9013365864753723, 0.9000211715698242, 0.8902407288551331, 0.8928504705429077, 0.884346091747284, 0.8838167786598206, 0.8788874864578247, 0.8727385401725769, 0.8719955682754517, 0.8632787227630615, 0.8613174676895141, 0.8589937686920166, 0.8550632357597351, 0.8543124318122863, 0.8495034813880921, 0.8449352383613586, 0.8399427175521851, 0.8392575025558472, 0.8323322534561157, 0.8280331492424011, 0.8276973962783813, 0.823796021938324, 0.8188985824584961, 0.8111978888511657, 0.8080901265144348, 0.809828782081604, 0.8031284093856812, 0.7991377234458923, 0.7948265075683594, 0.7933520674705505, 0.7907901406288147, 0.7843037724494935, 0.7812519669532776, 0.777957272529602, 0.7743557095527649, 0.7737539529800415, 0.7681918621063233, 0.7662612557411194, 0.7642675042152405, 0.7572986364364624, 0.7580339670181274, 0.7496770620346069, 0.7460728287696838, 0.7453308820724487, 0.7446800947189331, 0.7427486181259155, 0.7314735770225524, 0.729272186756134, 0.7262486100196839, 0.7262832999229432, 0.720863425731659, 0.7177481532096863, 0.7146221280097962, 0.7101320266723633, 0.7087152600288391, 0.7028303980827332, 0.6985920667648315, 0.6958940625190735], 'loss_test': [1.0770480632781982, 1.069389820098877, 1.067144513130188, 1.0775554180145264, 1.0792436599731445, 1.0972751379013062, 1.0618535280227661, 1.0845540761947632, 1.0800079107284546, 1.0682339668273926, 1.0718153715133667, 1.0758302211761475, 1.0841038227081299, 1.090631127357483, 1.0686373710632324, 1.0776393413543701, 1.0776528120040894, 1.0732239484786987, 1.0658643245697021, 1.0639472007751465, 1.1018239259719849, 1.0731213092803955, 1.0626577138900757, 1.1011286973953247, 1.0998108386993408, 1.0624972581863403, 1.0835347175598145, 1.0829695463180542, 1.074910283088684, 1.0724081993103027, 1.0823765993118286, 1.072175145149231, 1.0785741806030273, 1.0696889162063599, 1.0667226314544678, 1.0666764974594116, 1.0559101104736328, 1.0441627502441406, 1.0310014486312866, 1.0217320919036865, 1.013604760169983, 1.0120652914047241, 0.9946243762969971, 1.0012798309326172, 1.0002208948135376, 1.0065621137619019, 0.9809477925300598, 0.9876495003700256, 0.9987850189208984, 0.9688071012496948, 0.9785439372062683, 0.9681161642074585, 0.9823286533355713, 0.9776201844215393, 0.9689841866493225, 0.9610174894332886, 0.9481339454650879, 0.9541000723838806, 0.9629886150360107, 0.9579100012779236, 0.9532932639122009, 0.9462588429450989, 0.9490946531295776, 0.9314245581626892, 0.9282171726226807, 0.9347662329673767, 0.9324761629104614, 0.9432117342948914, 0.9239192008972168, 0.9221410155296326, 0.9330205321311951, 0.9364147782325745, 0.9193295240402222, 0.9113433957099915, 0.9122717976570129, 0.9104880690574646, 0.9271401762962341, 0.9199147820472717, 0.9218102693557739, 0.9237797260284424, 0.9190018773078918, 0.9102168679237366, 0.9003009796142578, 0.9087979197502136, 0.9184640049934387, 0.9074701070785522, 0.9132224321365356, 0.9062868356704712, 0.8982108235359192, 0.9155997633934021, 0.9098914265632629, 0.9080520868301392, 0.914915919303894, 0.9032084345817566, 0.9209598898887634, 0.90469890832901, 0.9088574051856995, 0.9167057871818542, 0.9206188917160034, 0.9114881753921509], 'identifier': '8333596np'}