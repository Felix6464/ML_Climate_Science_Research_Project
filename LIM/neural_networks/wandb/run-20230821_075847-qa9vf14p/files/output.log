Epoch: 00, Training Loss: 0.9954, Test Loss: 1.0781
Epoch: 01, Training Loss: 0.9967, Test Loss: 1.0891
Epoch: 02, Training Loss: 0.9890, Test Loss: 1.0629
Epoch: 03, Training Loss: 0.9914, Test Loss: 1.0725
Epoch: 04, Training Loss: 0.9950, Test Loss: 1.0777
Epoch: 05, Training Loss: 0.9961, Test Loss: 1.0750
Epoch: 06, Training Loss: 0.9898, Test Loss: 1.0759
Epoch: 07, Training Loss: 0.9923, Test Loss: 1.0579
Epoch: 08, Training Loss: 0.9881, Test Loss: 1.0782
Epoch: 09, Training Loss: 0.9907, Test Loss: 1.0736
Epoch: 10, Training Loss: 0.9907, Test Loss: 1.0743
Epoch: 11, Training Loss: 0.9893, Test Loss: 1.0777
Epoch: 12, Training Loss: 0.9899, Test Loss: 1.0793
Epoch: 13, Training Loss: 0.9860, Test Loss: 1.0601
Epoch: 14, Training Loss: 0.9910, Test Loss: 1.0654
Epoch: 15, Training Loss: 0.9875, Test Loss: 1.0624
Epoch: 16, Training Loss: 0.9858, Test Loss: 1.0781
Epoch: 17, Training Loss: 0.9875, Test Loss: 1.0881
Epoch: 18, Training Loss: 0.9857, Test Loss: 1.0828
Epoch: 19, Training Loss: 0.9828, Test Loss: 1.0674
Epoch: 20, Training Loss: 0.9837, Test Loss: 1.0896
Epoch: 21, Training Loss: 0.9858, Test Loss: 1.0686
Epoch: 22, Training Loss: 0.9918, Test Loss: 1.0713
Epoch: 23, Training Loss: 0.9868, Test Loss: 1.0777
Epoch: 24, Training Loss: 0.9812, Test Loss: 1.0739
Epoch: 25, Training Loss: 0.9807, Test Loss: 1.0805
Epoch: 26, Training Loss: 0.9800, Test Loss: 1.0822
Epoch: 27, Training Loss: 0.9748, Test Loss: 1.0719
Epoch: 28, Training Loss: 0.9771, Test Loss: 1.0802
Epoch: 29, Training Loss: 0.9739, Test Loss: 1.0721
Epoch: 30, Training Loss: 0.9770, Test Loss: 1.0804
Epoch: 31, Training Loss: 0.9748, Test Loss: 1.0704
Epoch: 32, Training Loss: 0.9681, Test Loss: 1.0766
Epoch: 33, Training Loss: 0.9608, Test Loss: 1.0715
Epoch: 34, Training Loss: 0.9496, Test Loss: 1.0782
Epoch: 35, Training Loss: 0.9472, Test Loss: 1.0466
Epoch: 36, Training Loss: 0.9387, Test Loss: 1.0379
Epoch: 37, Training Loss: 0.9316, Test Loss: 1.0528
Epoch: 38, Training Loss: 0.9263, Test Loss: 1.0359
Epoch: 39, Training Loss: 0.9184, Test Loss: 1.0208
Epoch: 40, Training Loss: 0.9143, Test Loss: 1.0389
Epoch: 41, Training Loss: 0.9081, Test Loss: 1.0404
Epoch: 42, Training Loss: 0.9051, Test Loss: 1.0344
Epoch: 43, Training Loss: 0.9008, Test Loss: 1.0303
Epoch: 44, Training Loss: 0.8936, Test Loss: 1.0137
Epoch: 45, Training Loss: 0.8933, Test Loss: 1.0102
Epoch: 46, Training Loss: 0.8864, Test Loss: 0.9961
Epoch: 47, Training Loss: 0.8800, Test Loss: 1.0103
Epoch: 48, Training Loss: 0.8816, Test Loss: 1.0096
Epoch: 49, Training Loss: 0.8764, Test Loss: 0.9984
Epoch: 50, Training Loss: 0.8726, Test Loss: 0.9766
Epoch: 51, Training Loss: 0.8704, Test Loss: 0.9955
Epoch: 52, Training Loss: 0.8638, Test Loss: 0.9566
Epoch: 53, Training Loss: 0.8604, Test Loss: 0.9841
Epoch: 54, Training Loss: 0.8558, Test Loss: 0.9643
Epoch: 55, Training Loss: 0.8564, Test Loss: 0.9786
Epoch: 56, Training Loss: 0.8524, Test Loss: 0.9652
Epoch: 57, Training Loss: 0.8507, Test Loss: 0.9610
Epoch: 58, Training Loss: 0.8478, Test Loss: 0.9638
Epoch: 59, Training Loss: 0.8458, Test Loss: 0.9569
Epoch: 60, Training Loss: 0.8404, Test Loss: 0.9587
Epoch: 61, Training Loss: 0.8376, Test Loss: 0.9653
Epoch: 62, Training Loss: 0.8380, Test Loss: 0.9618
Epoch: 63, Training Loss: 0.8329, Test Loss: 0.9325
Epoch: 64, Training Loss: 0.8296, Test Loss: 0.9465
Epoch: 65, Training Loss: 0.8322, Test Loss: 0.9617
Epoch: 66, Training Loss: 0.8243, Test Loss: 0.9423
Epoch: 67, Training Loss: 0.8179, Test Loss: 0.9454
Epoch: 68, Training Loss: 0.8173, Test Loss: 0.9531
Epoch: 69, Training Loss: 0.8183, Test Loss: 0.9346
Epoch: 70, Training Loss: 0.8127, Test Loss: 0.9398
Epoch: 71, Training Loss: 0.8090, Test Loss: 0.9323
Epoch: 72, Training Loss: 0.8043, Test Loss: 0.9299
Epoch: 73, Training Loss: 0.8027, Test Loss: 0.9360
Epoch: 74, Training Loss: 0.7975, Test Loss: 0.9248
Epoch: 75, Training Loss: 0.7954, Test Loss: 0.9332
Epoch: 76, Training Loss: 0.7926, Test Loss: 0.9242
Epoch: 77, Training Loss: 0.7858, Test Loss: 0.9218
Epoch: 78, Training Loss: 0.7841, Test Loss: 0.9285
Epoch: 79, Training Loss: 0.7790, Test Loss: 0.9176
Epoch: 80, Training Loss: 0.7782, Test Loss: 0.9192
 33%|████████████████████████████████▏                                                                 | 82/250 [00:06<00:14, 11.39it/s, loss_test=0.928]
Epoch: 81, Training Loss: 0.7690, Test Loss: 0.9277
Epoch: 82, Training Loss: 0.7724, Test Loss: 0.9181
Epoch: 83, Training Loss: 0.7640, Test Loss: 0.9179
Epoch: 84, Training Loss: 0.7626, Test Loss: 0.8973
Epoch: 85, Training Loss: 0.7612, Test Loss: 0.9058
Epoch: 86, Training Loss: 0.7564, Test Loss: 0.9151
Epoch: 87, Training Loss: 0.7536, Test Loss: 0.9099
Epoch: 88, Training Loss: 0.7504, Test Loss: 0.8952
Epoch: 89, Training Loss: 0.7465, Test Loss: 0.9167
Epoch: 90, Training Loss: 0.7417, Test Loss: 0.9038
Epoch: 91, Training Loss: 0.7390, Test Loss: 0.9155
Epoch: 92, Training Loss: 0.7354, Test Loss: 0.8965
Epoch: 93, Training Loss: 0.7284, Test Loss: 0.9219
Epoch: 94, Training Loss: 0.7291, Test Loss: 0.9139
Epoch: 95, Training Loss: 0.7220, Test Loss: 0.9018
Epoch: 96, Training Loss: 0.7211, Test Loss: 0.9042
Epoch: 97, Training Loss: 0.7140, Test Loss: 0.9061
Epoch: 98, Training Loss: 0.7115, Test Loss: 0.8907
Epoch: 99, Training Loss: 0.7065, Test Loss: 0.8880
Epoch: 100, Training Loss: 0.7050, Test Loss: 0.9030
Epoch: 101, Training Loss: 0.7012, Test Loss: 0.8905
Epoch: 102, Training Loss: 0.6979, Test Loss: 0.9095
Epoch: 103, Training Loss: 0.6931, Test Loss: 0.9028
Epoch: 104, Training Loss: 0.6925, Test Loss: 0.8965

 42%|█████████████████████████████████████████▏                                                       | 106/250 [00:08<00:11, 12.73it/s, loss_test=0.898]
Epoch: 106, Training Loss: 0.6856, Test Loss: 0.8983
Epoch: 107, Training Loss: 0.6783, Test Loss: 0.8887
Epoch: 108, Training Loss: 0.6776, Test Loss: 0.9032
Epoch: 109, Training Loss: 0.6733, Test Loss: 0.8959
Epoch: 110, Training Loss: 0.6699, Test Loss: 0.8944
Epoch: 111, Training Loss: 0.6671, Test Loss: 0.9073
Epoch: 112, Training Loss: 0.6648, Test Loss: 0.9023
Epoch: 113, Training Loss: 0.6591, Test Loss: 0.9020
Epoch: 114, Training Loss: 0.6539, Test Loss: 0.9105
Epoch: 115, Training Loss: 0.6537, Test Loss: 0.8947
Epoch: 116, Training Loss: 0.6491, Test Loss: 0.9217
Epoch: 117, Training Loss: 0.6471, Test Loss: 0.9179
Epoch: 118, Training Loss: 0.6420, Test Loss: 0.8962
Epoch: 119, Training Loss: 0.6419, Test Loss: 0.9180
Epoch: 120, Training Loss: 0.6368, Test Loss: 0.8960
Epoch: 121, Training Loss: 0.6316, Test Loss: 0.9031
Epoch: 122, Training Loss: 0.6291, Test Loss: 0.9024
Epoch: 123, Training Loss: 0.6264, Test Loss: 0.9102
Epoch: 124, Training Loss: 0.6210, Test Loss: 0.9084
Epoch: 125, Training Loss: 0.6201, Test Loss: 0.8893
Epoch: 126, Training Loss: 0.6192, Test Loss: 0.8981
Epoch: 127, Training Loss: 0.6151, Test Loss: 0.9132
Epoch: 128, Training Loss: 0.6099, Test Loss: 0.9164

 52%|██████████████████████████████████████████████████▍                                              | 130/250 [00:10<00:09, 12.19it/s, loss_test=0.905]
Epoch: 130, Training Loss: 0.6048, Test Loss: 0.9047
Epoch: 131, Training Loss: 0.5995, Test Loss: 0.9101
Epoch: 132, Training Loss: 0.5990, Test Loss: 0.9083
Epoch: 133, Training Loss: 0.5948, Test Loss: 0.9250
Epoch: 134, Training Loss: 0.5926, Test Loss: 0.9076
Epoch: 135, Training Loss: 0.5891, Test Loss: 0.9100
Epoch: 136, Training Loss: 0.5834, Test Loss: 0.9319
Epoch: 137, Training Loss: 0.5830, Test Loss: 0.9198
Epoch: 138, Training Loss: 0.5799, Test Loss: 0.9119
Epoch: 139, Training Loss: 0.5753, Test Loss: 0.9206
Epoch: 140, Training Loss: 0.5732, Test Loss: 0.9269
Epoch: 141, Training Loss: 0.5683, Test Loss: 0.9129
Epoch: 142, Training Loss: 0.5647, Test Loss: 0.9247
Epoch: 143, Training Loss: 0.5652, Test Loss: 0.9384
Epoch: 144, Training Loss: 0.5613, Test Loss: 0.9361
Epoch: 145, Training Loss: 0.5593, Test Loss: 0.9232
Epoch: 146, Training Loss: 0.5555, Test Loss: 0.9399
Epoch: 147, Training Loss: 0.5548, Test Loss: 0.9297
Epoch: 148, Training Loss: 0.5504, Test Loss: 0.9226
Epoch: 149, Training Loss: 0.5454, Test Loss: 0.9277
Epoch: 150, Training Loss: 0.5448, Test Loss: 0.9176
Epoch: 151, Training Loss: 0.5430, Test Loss: 0.9295
Epoch: 152, Training Loss: 0.5362, Test Loss: 0.9412

 62%|███████████████████████████████████████████████████████████▊                                     | 154/250 [00:12<00:07, 12.14it/s, loss_test=0.937]
Epoch: 154, Training Loss: 0.5321, Test Loss: 0.9373
Epoch: 155, Training Loss: 0.5317, Test Loss: 0.9503
Epoch: 156, Training Loss: 0.5285, Test Loss: 0.9427
Epoch: 157, Training Loss: 0.5221, Test Loss: 0.9313
Epoch: 158, Training Loss: 0.5209, Test Loss: 0.9384
Epoch: 159, Training Loss: 0.5188, Test Loss: 0.9356
Epoch: 160, Training Loss: 0.5154, Test Loss: 0.9392
Epoch: 161, Training Loss: 0.5113, Test Loss: 0.9435
Epoch: 162, Training Loss: 0.5093, Test Loss: 0.9556
Epoch: 163, Training Loss: 0.5080, Test Loss: 0.9536
Epoch: 164, Training Loss: 0.5061, Test Loss: 0.9553
Epoch: 165, Training Loss: 0.5018, Test Loss: 0.9563
Epoch: 166, Training Loss: 0.4990, Test Loss: 0.9317
Epoch: 167, Training Loss: 0.4978, Test Loss: 0.9604
Epoch: 168, Training Loss: 0.4950, Test Loss: 0.9518
Epoch: 169, Training Loss: 0.4909, Test Loss: 0.9589
Epoch: 170, Training Loss: 0.4892, Test Loss: 0.9546
Epoch: 171, Training Loss: 0.4863, Test Loss: 0.9620
Epoch: 172, Training Loss: 0.4858, Test Loss: 0.9593
Epoch: 173, Training Loss: 0.4808, Test Loss: 0.9511
Epoch: 174, Training Loss: 0.4773, Test Loss: 0.9640
Epoch: 175, Training Loss: 0.4774, Test Loss: 0.9655
Epoch: 176, Training Loss: 0.4745, Test Loss: 0.9528

 72%|█████████████████████████████████████████████████████████████████████▊                           | 180/250 [00:14<00:05, 12.55it/s, loss_test=0.945]
Epoch: 178, Training Loss: 0.4694, Test Loss: 0.9661
Epoch: 179, Training Loss: 0.4674, Test Loss: 0.9450
Epoch: 180, Training Loss: 0.4641, Test Loss: 0.9644
Epoch: 181, Training Loss: 0.4617, Test Loss: 0.9732
Epoch: 182, Training Loss: 0.4588, Test Loss: 0.9759
Epoch: 183, Training Loss: 0.4569, Test Loss: 0.9764
Epoch: 184, Training Loss: 0.4536, Test Loss: 0.9522
Epoch: 185, Training Loss: 0.4521, Test Loss: 0.9771
Epoch: 186, Training Loss: 0.4497, Test Loss: 0.9621
Epoch: 187, Training Loss: 0.4459, Test Loss: 0.9786
Epoch: 188, Training Loss: 0.4437, Test Loss: 0.9857
Epoch: 189, Training Loss: 0.4416, Test Loss: 0.9485
Epoch: 190, Training Loss: 0.4406, Test Loss: 0.9843
Epoch: 191, Training Loss: 0.4358, Test Loss: 0.9734
Epoch: 192, Training Loss: 0.4336, Test Loss: 0.9815
Epoch: 193, Training Loss: 0.4318, Test Loss: 0.9886
Epoch: 194, Training Loss: 0.4301, Test Loss: 0.9696
Epoch: 195, Training Loss: 0.4274, Test Loss: 0.9757
Epoch: 196, Training Loss: 0.4249, Test Loss: 0.9753
Epoch: 197, Training Loss: 0.4224, Test Loss: 0.9839
Epoch: 198, Training Loss: 0.4219, Test Loss: 0.9731
Epoch: 199, Training Loss: 0.4191, Test Loss: 0.9814
Epoch: 200, Training Loss: 0.4158, Test Loss: 0.9626
Epoch: 201, Training Loss: 0.4160, Test Loss: 0.9768

 82%|███████████████████████████████████████████████████████████████████████████████▏                 | 204/250 [00:16<00:03, 12.23it/s, loss_test=0.982]
Epoch: 203, Training Loss: 0.4100, Test Loss: 0.9819
Epoch: 204, Training Loss: 0.4075, Test Loss: 1.0014
Epoch: 205, Training Loss: 0.4056, Test Loss: 0.9804
Epoch: 206, Training Loss: 0.4050, Test Loss: 0.9834
Epoch: 207, Training Loss: 0.4019, Test Loss: 0.9782
Epoch: 208, Training Loss: 0.4005, Test Loss: 0.9885
Epoch: 209, Training Loss: 0.3986, Test Loss: 0.9862
Epoch: 210, Training Loss: 0.3969, Test Loss: 0.9824
Epoch: 211, Training Loss: 0.3940, Test Loss: 0.9838
Epoch: 212, Training Loss: 0.3937, Test Loss: 0.9953
Epoch: 213, Training Loss: 0.3903, Test Loss: 0.9751
Epoch: 214, Training Loss: 0.3891, Test Loss: 1.0111
Epoch: 215, Training Loss: 0.3869, Test Loss: 0.9894
Epoch: 216, Training Loss: 0.3856, Test Loss: 0.9991
Epoch: 217, Training Loss: 0.3838, Test Loss: 0.9924
Epoch: 218, Training Loss: 0.3815, Test Loss: 0.9974
Epoch: 219, Training Loss: 0.3797, Test Loss: 0.9911
Epoch: 220, Training Loss: 0.3770, Test Loss: 0.9972
Epoch: 221, Training Loss: 0.3756, Test Loss: 1.0154
Epoch: 222, Training Loss: 0.3729, Test Loss: 0.9947
Epoch: 223, Training Loss: 0.3712, Test Loss: 1.0191
Epoch: 224, Training Loss: 0.3696, Test Loss: 0.9975
Epoch: 225, Training Loss: 0.3677, Test Loss: 0.9996


100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.14it/s, loss_test=1.014]
Epoch: 227, Training Loss: 0.3646, Test Loss: 1.0142
Epoch: 228, Training Loss: 0.3627, Test Loss: 1.0014
Epoch: 229, Training Loss: 0.3613, Test Loss: 1.0012
Epoch: 230, Training Loss: 0.3600, Test Loss: 1.0114
Epoch: 231, Training Loss: 0.3577, Test Loss: 1.0004
Epoch: 232, Training Loss: 0.3558, Test Loss: 1.0113
Epoch: 233, Training Loss: 0.3538, Test Loss: 1.0063
Epoch: 234, Training Loss: 0.3526, Test Loss: 0.9994
Epoch: 235, Training Loss: 0.3508, Test Loss: 1.0221
Epoch: 236, Training Loss: 0.3496, Test Loss: 1.0185
Epoch: 237, Training Loss: 0.3486, Test Loss: 0.9869
Epoch: 238, Training Loss: 0.3463, Test Loss: 1.0146
Epoch: 239, Training Loss: 0.3451, Test Loss: 0.9990
Epoch: 240, Training Loss: 0.3430, Test Loss: 1.0170
Epoch: 241, Training Loss: 0.3414, Test Loss: 1.0110
Epoch: 242, Training Loss: 0.3402, Test Loss: 1.0270
Epoch: 243, Training Loss: 0.3379, Test Loss: 1.0139
Epoch: 244, Training Loss: 0.3375, Test Loss: 1.0324
Epoch: 245, Training Loss: 0.3363, Test Loss: 1.0061
Epoch: 246, Training Loss: 0.3330, Test Loss: 1.0361
Epoch: 247, Training Loss: 0.3321, Test Loss: 1.0323
Epoch: 248, Training Loss: 0.3310, Test Loss: 1.0278
Epoch: 249, Training Loss: 0.3308, Test Loss: 1.0135
Model saved as model_3160395np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-12110000-3160395np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9953535795211792, 0.9966917276382447, 0.9889621376991272, 0.9913879036903381, 0.995021641254425, 0.9960657835006714, 0.989756441116333, 0.9922859787940979, 0.9880884885787964, 0.990737009048462, 0.9907105445861817, 0.9893489956855774, 0.989946973323822, 0.985961651802063, 0.9910071611404419, 0.987535035610199, 0.9857625603675843, 0.9875018835067749, 0.9856993079185485, 0.9828384757041931, 0.9837220430374145, 0.9857566118240356, 0.9918463468551636, 0.986786425113678, 0.9812292337417603, 0.9807052135467529, 0.9800079345703125, 0.9748480081558227, 0.9771155476570129, 0.9738956809043884, 0.9769508600234985, 0.9747630953788757, 0.9681106567382812, 0.9608443021774292, 0.9495579719543457, 0.9472320556640625, 0.9386929035186767, 0.9315562844276428, 0.9263134121894836, 0.9183655261993409, 0.9143265008926391, 0.9081169247627259, 0.9050810694694519, 0.9007896780967712, 0.893582534790039, 0.8933433055877685, 0.8864041090011596, 0.8799737572669983, 0.8815918445587159, 0.8764219999313354, 0.8725823640823365, 0.8703980326652527, 0.8638309240341187, 0.8604437947273255, 0.8558164119720459, 0.8563619732856751, 0.85242999792099, 0.8507206439971924, 0.8478052973747253, 0.8458433032035828, 0.8403982639312744, 0.8376306891441345, 0.8380163550376892, 0.832883358001709, 0.8295780181884765, 0.8322056889533996, 0.8242629528045654, 0.8178623795509339, 0.8172863006591797, 0.8182982087135315, 0.8126587748527527, 0.8090226531028748, 0.804307758808136, 0.8027286171913147, 0.7975490093231201, 0.7953513264656067, 0.7925963878631592, 0.785818362236023, 0.7840875148773193, 0.7790222287178039, 0.7782118082046509, 0.7690243482589721, 0.7723698258399964, 0.7639656305313111, 0.7625669836997986, 0.7612115979194641, 0.7563701391220092, 0.7536172270774841, 0.7503947019577026, 0.7464855551719666, 0.7417182445526123, 0.7389957785606385, 0.7354392170906067, 0.7283912420272827, 0.7290538787841797, 0.7220443606376648, 0.7210956931114196, 0.7139565348625183, 0.7115478634834289, 0.7065341472625732, 0.7050131559371948, 0.7011939525604248, 0.697852635383606, 0.6930805683135987, 0.692470645904541, 0.6888396382331848, 0.6855602502822876, 0.6783066630363465, 0.6776232242584228, 0.6733177065849304, 0.6699026823043823, 0.6671003818511962, 0.6647706508636475, 0.6591379404067993, 0.6538544416427612, 0.6536989450454712, 0.6491465330123901, 0.6470791697502136, 0.6420104742050171, 0.6418522119522094, 0.6368030428886413, 0.6315938591957092, 0.6290558815002442, 0.6263720035552979, 0.6209581971168519, 0.620097553730011, 0.6192441940307617, 0.6150547742843628, 0.6098806262016296, 0.6084470987319947, 0.6047708511352539, 0.59951753616333, 0.5990138649940491, 0.5948052525520324, 0.5926443457603454, 0.5891109704971313, 0.5833822250366211, 0.5830260753631592, 0.5799076914787292, 0.5753215432167054, 0.573233699798584, 0.5682782292366028, 0.5647066950798034, 0.5651788949966431, 0.5613479614257812, 0.5592735648155213, 0.5555214405059814, 0.554805588722229, 0.5504183888435363, 0.5454041361808777, 0.5447867393493653, 0.54298255443573, 0.5362035751342773, 0.5357798099517822, 0.5320582389831543, 0.5317360401153565, 0.5284517765045166, 0.5220650792121887, 0.5209470152854919, 0.5187994360923767, 0.5154496908187867, 0.5113114535808563, 0.5092921316623688, 0.5080001473426818, 0.5060716211795807, 0.5017887651920319, 0.49898133277893064, 0.4977970659732819, 0.4950040698051453, 0.4909329771995544, 0.48915167450904845, 0.48633614778518675, 0.48581141233444214, 0.4807611644268036, 0.4773413479328156, 0.4774165689945221, 0.47449750304222105, 0.4706021070480347, 0.4694430291652679, 0.4674492239952087, 0.4640528678894043, 0.4616998672485352, 0.4587548434734344, 0.4569398403167725, 0.45359246134757997, 0.4520706295967102, 0.449655020236969, 0.4459055006504059, 0.44366180896759033, 0.44159849882125857, 0.44063389897346494, 0.4357792139053345, 0.4335885465145111, 0.4318043887615204, 0.4301247954368591, 0.4274301290512085, 0.42492942214012147, 0.42238011956214905, 0.42189077734947206, 0.4191373884677887, 0.41581226587295533, 0.4159661591053009, 0.4134821593761444, 0.410020238161087, 0.4075137674808502, 0.40562201738357545, 0.4049669861793518, 0.40187970399856565, 0.40048693418502807, 0.3985852479934692, 0.3968502044677734, 0.39396587014198303, 0.3937036991119385, 0.39025766849517823, 0.38907747268676757, 0.386863774061203, 0.3855993032455444, 0.38377058506011963, 0.3815293967723846, 0.3796816051006317, 0.37702465057373047, 0.37563847899436953, 0.3729219436645508, 0.37118520140647887, 0.3696464002132416, 0.3677056610584259, 0.36806907653808596, 0.36455497741699217, 0.3626594305038452, 0.36133928298950196, 0.3599797189235687, 0.35769095420837405, 0.3557592272758484, 0.35378055572509765, 0.3526125133037567, 0.3508201003074646, 0.3495922267436981, 0.3486111342906952, 0.3462974846363068, 0.3450661778450012, 0.3430407404899597, 0.3413715660572052, 0.3402228832244873, 0.33785589337348937, 0.3375228404998779, 0.3362912774085999, 0.3329928696155548, 0.33214598298072817, 0.33104764819145205, 0.3308160424232483], 'loss_test': [1.078101396560669, 1.08914315700531, 1.0629154443740845, 1.0724585056304932, 1.0776972770690918, 1.0749647617340088, 1.0758838653564453, 1.0579310655593872, 1.0782089233398438, 1.0736260414123535, 1.0742868185043335, 1.0777091979980469, 1.0793153047561646, 1.060089349746704, 1.065372109413147, 1.0624275207519531, 1.07807195186615, 1.088059425354004, 1.0827734470367432, 1.0674152374267578, 1.0896368026733398, 1.0685830116271973, 1.071340799331665, 1.0776653289794922, 1.0739140510559082, 1.0805109739303589, 1.0822018384933472, 1.071881890296936, 1.0802303552627563, 1.0721487998962402, 1.0804369449615479, 1.0703511238098145, 1.0766143798828125, 1.0714837312698364, 1.078179121017456, 1.0465577840805054, 1.0379371643066406, 1.0528085231781006, 1.0359359979629517, 1.0207544565200806, 1.0388977527618408, 1.0404237508773804, 1.0344425439834595, 1.030282735824585, 1.0136526823043823, 1.0102276802062988, 0.9961022734642029, 1.0103105306625366, 1.0096269845962524, 0.9983689188957214, 0.9765976071357727, 0.9954932332038879, 0.9565624594688416, 0.9840818047523499, 0.9643492102622986, 0.9785636067390442, 0.965151846408844, 0.961015522480011, 0.963792622089386, 0.9569467306137085, 0.9586714506149292, 0.9652853608131409, 0.961846113204956, 0.9325478672981262, 0.9464545845985413, 0.9617011547088623, 0.9422792196273804, 0.9453853964805603, 0.9531209468841553, 0.9346300363540649, 0.9398193359375, 0.9323396682739258, 0.9298890233039856, 0.9360154867172241, 0.9247638583183289, 0.9332216382026672, 0.9242214560508728, 0.921791672706604, 0.9285324215888977, 0.9176496863365173, 0.919238805770874, 0.9277289509773254, 0.9181126952171326, 0.9178938269615173, 0.8972521424293518, 0.9057731032371521, 0.9151228666305542, 0.9099146127700806, 0.8951838612556458, 0.916650652885437, 0.9037981629371643, 0.9154931306838989, 0.8965320587158203, 0.9219149351119995, 0.9138514399528503, 0.90176922082901, 0.9041873812675476, 0.9061489701271057, 0.8907214999198914, 0.8880149126052856, 0.9030327796936035, 0.8904672861099243, 0.9094911813735962, 0.9028434157371521, 0.896462082862854, 0.9047573804855347, 0.8982772827148438, 0.8886925578117371, 0.9031583666801453, 0.8958674073219299, 0.8943983316421509, 0.9073106646537781, 0.9023370146751404, 0.9020170569419861, 0.9105079174041748, 0.8946835994720459, 0.9216654896736145, 0.9179206490516663, 0.8961833715438843, 0.9179598093032837, 0.8960441946983337, 0.9030911922454834, 0.9023762941360474, 0.9102473855018616, 0.9084396362304688, 0.8892833590507507, 0.8980953693389893, 0.9131500720977783, 0.916370689868927, 0.8995628356933594, 0.9046919345855713, 0.910079836845398, 0.9083418250083923, 0.9249622225761414, 0.9075695872306824, 0.9099648594856262, 0.931911826133728, 0.9198464751243591, 0.9118820428848267, 0.9206395745277405, 0.9268824458122253, 0.9129199981689453, 0.9246841073036194, 0.9383795857429504, 0.9361358880996704, 0.9231687784194946, 0.939866304397583, 0.9297356605529785, 0.9226270318031311, 0.9276987314224243, 0.9175931811332703, 0.929478645324707, 0.9411953687667847, 0.9336794018745422, 0.9373363852500916, 0.9502906203269958, 0.9427409768104553, 0.9312742948532104, 0.9384381175041199, 0.9355878233909607, 0.9391617178916931, 0.9435243606567383, 0.9556185603141785, 0.9535982012748718, 0.9552553296089172, 0.9563435316085815, 0.93172287940979, 0.9603899121284485, 0.9518209099769592, 0.9588829874992371, 0.9545733332633972, 0.9619689583778381, 0.9592892527580261, 0.9511201977729797, 0.9639531970024109, 0.9654881954193115, 0.9527751803398132, 0.9585396647453308, 0.9660577774047852, 0.944961428642273, 0.9643750786781311, 0.9732396602630615, 0.9758940935134888, 0.9764360189437866, 0.9522144198417664, 0.9770581126213074, 0.9620766639709473, 0.9786466360092163, 0.985748827457428, 0.9485366940498352, 0.9843103289604187, 0.9734193682670593, 0.9814739227294922, 0.9886142611503601, 0.9696399569511414, 0.9756638407707214, 0.9753168821334839, 0.9839242100715637, 0.9731378555297852, 0.9814057350158691, 0.9625998139381409, 0.9767687916755676, 0.9872153401374817, 0.9818964004516602, 1.0013736486434937, 0.9804125428199768, 0.9833897352218628, 0.9781752824783325, 0.9884662628173828, 0.9861983060836792, 0.9823588132858276, 0.9837560057640076, 0.9953243136405945, 0.9750560522079468, 1.0111032724380493, 0.9893823862075806, 0.9990780353546143, 0.9924153089523315, 0.9973921179771423, 0.9910851716995239, 0.9971500039100647, 1.0153567790985107, 0.9946742653846741, 1.01907217502594, 0.9974702000617981, 0.9995554685592651, 1.0157469511032104, 1.0142154693603516, 1.0014326572418213, 1.0011810064315796, 1.0114184617996216, 1.0004395246505737, 1.0112582445144653, 1.0063358545303345, 0.9993566870689392, 1.0221354961395264, 1.0185431241989136, 0.9869182109832764, 1.0146137475967407, 0.9990244507789612, 1.017001986503601, 1.0109729766845703, 1.0269566774368286, 1.0139038562774658, 1.0323642492294312, 1.006057620048523, 1.036070466041565, 1.032288670539856, 1.0278136730194092, 1.013510823249817], 'identifier': '3160395np'}