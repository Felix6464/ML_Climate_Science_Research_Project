Epoch: 00, Training Loss: 0.9993, Test Loss: 1.0658
Epoch: 01, Training Loss: 0.9915, Test Loss: 1.0750
Epoch: 02, Training Loss: 0.9908, Test Loss: 1.0881
Epoch: 03, Training Loss: 0.9923, Test Loss: 1.0912
Epoch: 04, Training Loss: 0.9924, Test Loss: 1.0803
Epoch: 05, Training Loss: 0.9914, Test Loss: 1.0811
Epoch: 06, Training Loss: 0.9911, Test Loss: 1.0886
Epoch: 07, Training Loss: 0.9937, Test Loss: 1.0833
Epoch: 08, Training Loss: 0.9881, Test Loss: 1.0643
Epoch: 09, Training Loss: 0.9883, Test Loss: 1.0634
Epoch: 10, Training Loss: 0.9865, Test Loss: 1.0777
Epoch: 11, Training Loss: 0.9892, Test Loss: 1.0704
Epoch: 12, Training Loss: 0.9882, Test Loss: 1.0878
Epoch: 13, Training Loss: 0.9866, Test Loss: 1.0720
Epoch: 14, Training Loss: 0.9860, Test Loss: 1.0963
Epoch: 15, Training Loss: 0.9883, Test Loss: 1.0853
Epoch: 16, Training Loss: 0.9862, Test Loss: 1.0568
Epoch: 17, Training Loss: 0.9882, Test Loss: 1.0762
Epoch: 18, Training Loss: 0.9878, Test Loss: 1.0724
Epoch: 19, Training Loss: 0.9888, Test Loss: 1.0596
Epoch: 20, Training Loss: 0.9906, Test Loss: 1.0737
 20%|███████████████████▌                                                                              | 20/100 [00:01<00:06, 11.86it/s, loss_test=1.074]
Epoch: 21, Training Loss: 0.9845, Test Loss: 1.0853
Epoch: 22, Training Loss: 0.9845, Test Loss: 1.0925
Epoch: 23, Training Loss: 0.9837, Test Loss: 1.0764
Epoch: 24, Training Loss: 0.9859, Test Loss: 1.0693
Epoch: 25, Training Loss: 0.9846, Test Loss: 1.0745
Epoch: 26, Training Loss: 0.9806, Test Loss: 1.0669
Epoch: 27, Training Loss: 0.9806, Test Loss: 1.0828
Epoch: 28, Training Loss: 0.9812, Test Loss: 1.0643
Epoch: 29, Training Loss: 0.9792, Test Loss: 1.0972
Epoch: 30, Training Loss: 0.9805, Test Loss: 1.0782
Epoch: 31, Training Loss: 0.9762, Test Loss: 1.0656
Epoch: 32, Training Loss: 0.9700, Test Loss: 1.0804
Epoch: 33, Training Loss: 0.9690, Test Loss: 1.0732
Epoch: 34, Training Loss: 0.9652, Test Loss: 1.0799
Epoch: 35, Training Loss: 0.9577, Test Loss: 1.0756
Epoch: 36, Training Loss: 0.9488, Test Loss: 1.0782
Epoch: 37, Training Loss: 0.9375, Test Loss: 1.0604
Epoch: 38, Training Loss: 0.9376, Test Loss: 1.0508
Epoch: 39, Training Loss: 0.9295, Test Loss: 1.0687
Epoch: 40, Training Loss: 0.9164, Test Loss: 1.0308
Epoch: 41, Training Loss: 0.9117, Test Loss: 1.0547
Epoch: 42, Training Loss: 0.9147, Test Loss: 1.0308
Epoch: 43, Training Loss: 0.9045, Test Loss: 1.0370
Epoch: 44, Training Loss: 0.8979, Test Loss: 1.0354

 46%|█████████████████████████████████████████████                                                     | 46/100 [00:03<00:04, 12.51it/s, loss_test=1.024]
Epoch: 46, Training Loss: 0.8884, Test Loss: 1.0139
Epoch: 47, Training Loss: 0.8833, Test Loss: 1.0251
Epoch: 48, Training Loss: 0.8772, Test Loss: 1.0113
Epoch: 49, Training Loss: 0.8750, Test Loss: 1.0119
Epoch: 50, Training Loss: 0.8700, Test Loss: 0.9927
Epoch: 51, Training Loss: 0.8598, Test Loss: 0.9893
Epoch: 52, Training Loss: 0.8628, Test Loss: 0.9864
Epoch: 53, Training Loss: 0.8565, Test Loss: 0.9800
Epoch: 54, Training Loss: 0.8527, Test Loss: 0.9687
Epoch: 55, Training Loss: 0.8453, Test Loss: 0.9667
Epoch: 56, Training Loss: 0.8423, Test Loss: 0.9696
Epoch: 57, Training Loss: 0.8390, Test Loss: 0.9592
Epoch: 58, Training Loss: 0.8335, Test Loss: 0.9436
Epoch: 59, Training Loss: 0.8303, Test Loss: 0.9489
Epoch: 60, Training Loss: 0.8261, Test Loss: 0.9645
Epoch: 61, Training Loss: 0.8216, Test Loss: 0.9394
Epoch: 62, Training Loss: 0.8172, Test Loss: 0.9353
Epoch: 63, Training Loss: 0.8073, Test Loss: 0.9476
Epoch: 64, Training Loss: 0.8054, Test Loss: 0.9361
Epoch: 65, Training Loss: 0.8030, Test Loss: 0.9309
Epoch: 66, Training Loss: 0.7917, Test Loss: 0.9325
Epoch: 67, Training Loss: 0.7926, Test Loss: 0.9301
Epoch: 68, Training Loss: 0.7882, Test Loss: 0.9111
Epoch: 69, Training Loss: 0.7831, Test Loss: 0.9294

 70%|████████████████████████████████████████████████████████████████████▌                             | 70/100 [00:05<00:02, 12.48it/s, loss_test=0.925]
Epoch: 71, Training Loss: 0.7737, Test Loss: 0.9221
Epoch: 72, Training Loss: 0.7671, Test Loss: 0.9295
Epoch: 73, Training Loss: 0.7630, Test Loss: 0.8999
Epoch: 74, Training Loss: 0.7575, Test Loss: 0.9201
Epoch: 75, Training Loss: 0.7566, Test Loss: 0.9145
Epoch: 76, Training Loss: 0.7517, Test Loss: 0.9173
Epoch: 77, Training Loss: 0.7502, Test Loss: 0.9101
Epoch: 78, Training Loss: 0.7435, Test Loss: 0.9091
Epoch: 79, Training Loss: 0.7420, Test Loss: 0.9097
Epoch: 80, Training Loss: 0.7350, Test Loss: 0.8978
Epoch: 81, Training Loss: 0.7325, Test Loss: 0.8918
Epoch: 82, Training Loss: 0.7277, Test Loss: 0.8987
Epoch: 83, Training Loss: 0.7244, Test Loss: 0.8968
Epoch: 84, Training Loss: 0.7206, Test Loss: 0.8970
Epoch: 85, Training Loss: 0.7201, Test Loss: 0.8960
Epoch: 86, Training Loss: 0.7167, Test Loss: 0.9011
Epoch: 87, Training Loss: 0.7093, Test Loss: 0.8881
Epoch: 88, Training Loss: 0.7059, Test Loss: 0.8915
Epoch: 89, Training Loss: 0.7032, Test Loss: 0.8948
Epoch: 90, Training Loss: 0.6993, Test Loss: 0.9020
Epoch: 91, Training Loss: 0.6952, Test Loss: 0.8867
Epoch: 92, Training Loss: 0.6926, Test Loss: 0.8956
Epoch: 93, Training Loss: 0.6891, Test Loss: 0.8793


100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.22it/s, loss_test=0.885]
Epoch: 95, Training Loss: 0.6796, Test Loss: 0.8858
Epoch: 96, Training Loss: 0.6787, Test Loss: 0.8850
Epoch: 97, Training Loss: 0.6716, Test Loss: 0.8756
Epoch: 98, Training Loss: 0.6699, Test Loss: 0.8928
Epoch: 99, Training Loss: 0.6667, Test Loss: 0.8848
Model saved as model_1168319np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-124000-1168319np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 100, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.999273931980133, 0.991526436805725, 0.9907646179199219, 0.9923394680023193, 0.9923530697822571, 0.9914494395256043, 0.9910635232925415, 0.9937314748764038, 0.9881024956703186, 0.988252067565918, 0.9865219473838807, 0.9891955733299256, 0.9881750345230103, 0.9866063475608826, 0.9860286712646484, 0.9883032083511353, 0.986223304271698, 0.9881795763969421, 0.9878146648406982, 0.9887947320938111, 0.9905534386634827, 0.9844566583633423, 0.9844718098640441, 0.983680260181427, 0.9858661413192749, 0.9845733761787414, 0.9805838465690613, 0.9806068778038025, 0.9812031745910644, 0.9791707277297974, 0.9804715156555176, 0.9762461662292481, 0.9700361371040345, 0.9689676642417908, 0.9651575088500977, 0.957692277431488, 0.9487904667854309, 0.9375132083892822, 0.9376174926757812, 0.9295168519020081, 0.9163983702659607, 0.9116926908493042, 0.9146554350852967, 0.9045419335365296, 0.8979198217391968, 0.8914967894554138, 0.8884118914604187, 0.8832871198654175, 0.8771802067756653, 0.8750276207923889, 0.8699711799621582, 0.8597977876663208, 0.8628432035446167, 0.8564582109451294, 0.852698540687561, 0.8452769041061401, 0.8422629356384277, 0.8390244722366333, 0.8335401892662049, 0.8302767515182495, 0.8261417984962464, 0.8216082096099854, 0.8172386884689331, 0.8072774648666382, 0.8053669929504395, 0.8030297040939331, 0.7917451500892639, 0.7925790429115296, 0.7882275462150574, 0.7831069469451905, 0.7738505959510803, 0.7736737847328186, 0.7670579791069031, 0.7630298256874084, 0.7575224280357361, 0.7566142797470092, 0.7517235994338989, 0.7502102136611939, 0.7435144186019897, 0.7419528484344482, 0.7349562168121337, 0.732464325428009, 0.7277050733566284, 0.7244061589241028, 0.7206295490264892, 0.7201053500175476, 0.7166847348213196, 0.7093390822410583, 0.7058768510818482, 0.7031588196754456, 0.6993430614471435, 0.6951978206634521, 0.6925575733184814, 0.6890949964523315, 0.684926962852478, 0.6796340465545654, 0.6787437081336976, 0.6715912699699402, 0.6699128031730652, 0.6666595935821533], 'loss_test': [1.0658279657363892, 1.075042486190796, 1.088067650794983, 1.0912199020385742, 1.0803462266921997, 1.08111572265625, 1.0885813236236572, 1.0832650661468506, 1.0643043518066406, 1.0634231567382812, 1.0776787996292114, 1.0704350471496582, 1.0878232717514038, 1.0719703435897827, 1.0963155031204224, 1.0853387117385864, 1.0568093061447144, 1.0761593580245972, 1.0724108219146729, 1.059570550918579, 1.0736992359161377, 1.0853207111358643, 1.0925332307815552, 1.0763627290725708, 1.0693073272705078, 1.0744516849517822, 1.066892385482788, 1.0827583074569702, 1.064278483390808, 1.0971643924713135, 1.0781995058059692, 1.065598726272583, 1.0803780555725098, 1.0732381343841553, 1.0798594951629639, 1.0755548477172852, 1.0781995058059692, 1.0604383945465088, 1.0508341789245605, 1.0686726570129395, 1.0308077335357666, 1.054671049118042, 1.0308297872543335, 1.0370198488235474, 1.035401701927185, 1.0242079496383667, 1.0138847827911377, 1.0250598192214966, 1.011281132698059, 1.011850357055664, 0.9926785826683044, 0.9893392324447632, 0.9864339232444763, 0.9799895882606506, 0.9687015414237976, 0.9666975736618042, 0.96955806016922, 0.9592087864875793, 0.9436131119728088, 0.9488639235496521, 0.9645170569419861, 0.9393910765647888, 0.935326337814331, 0.9475806355476379, 0.9361055493354797, 0.9309325814247131, 0.9325489401817322, 0.9301006197929382, 0.9110565185546875, 0.9293679594993591, 0.9247740507125854, 0.9220888018608093, 0.9295478463172913, 0.8998553156852722, 0.9200902581214905, 0.9144936800003052, 0.9173469543457031, 0.9101069569587708, 0.9090734124183655, 0.9097438454627991, 0.8978288769721985, 0.8918267488479614, 0.8986839652061462, 0.8967556357383728, 0.8969888687133789, 0.8959947824478149, 0.9010792970657349, 0.8881170749664307, 0.8915239572525024, 0.8947761654853821, 0.902043342590332, 0.8867278695106506, 0.8956295847892761, 0.8792547583580017, 0.8854967951774597, 0.8857845664024353, 0.884996235370636, 0.8755573034286499, 0.8927589058876038, 0.88478022813797], 'identifier': '1168319np'}