
  8%|███████▊                                                                                          | 20/250 [00:01<00:18, 12.20it/s, loss_test=1.094]
Epoch: 00, Training Loss: 0.9994, Test Loss: 1.0755
Epoch: 01, Training Loss: 0.9958, Test Loss: 1.0647
Epoch: 02, Training Loss: 0.9875, Test Loss: 1.0872
Epoch: 03, Training Loss: 0.9942, Test Loss: 1.0824
Epoch: 04, Training Loss: 0.9952, Test Loss: 1.0792
Epoch: 05, Training Loss: 0.9931, Test Loss: 1.0814
Epoch: 06, Training Loss: 0.9910, Test Loss: 1.0646
Epoch: 07, Training Loss: 0.9940, Test Loss: 1.0700
Epoch: 08, Training Loss: 0.9928, Test Loss: 1.0733
Epoch: 09, Training Loss: 0.9948, Test Loss: 1.0859
Epoch: 10, Training Loss: 0.9906, Test Loss: 1.0735
Epoch: 11, Training Loss: 0.9877, Test Loss: 1.0792
Epoch: 12, Training Loss: 0.9939, Test Loss: 1.0652
Epoch: 13, Training Loss: 0.9826, Test Loss: 1.0705
Epoch: 14, Training Loss: 0.9915, Test Loss: 1.0809
Epoch: 15, Training Loss: 0.9856, Test Loss: 1.0829
Epoch: 16, Training Loss: 0.9850, Test Loss: 1.0834
Epoch: 17, Training Loss: 0.9866, Test Loss: 1.0845
Epoch: 18, Training Loss: 0.9896, Test Loss: 1.0693
Epoch: 19, Training Loss: 0.9855, Test Loss: 1.0727

 18%|█████████████████▏                                                                                | 44/250 [00:03<00:16, 12.22it/s, loss_test=0.985]
Epoch: 21, Training Loss: 0.9845, Test Loss: 1.0902
Epoch: 22, Training Loss: 0.9852, Test Loss: 1.0741
Epoch: 23, Training Loss: 0.9827, Test Loss: 1.0653
Epoch: 24, Training Loss: 0.9842, Test Loss: 1.0696
Epoch: 25, Training Loss: 0.9810, Test Loss: 1.0662
Epoch: 26, Training Loss: 0.9808, Test Loss: 1.0891
Epoch: 27, Training Loss: 0.9821, Test Loss: 1.0767
Epoch: 28, Training Loss: 0.9786, Test Loss: 1.0775
Epoch: 29, Training Loss: 0.9798, Test Loss: 1.0901
Epoch: 30, Training Loss: 0.9804, Test Loss: 1.0742
Epoch: 31, Training Loss: 0.9804, Test Loss: 1.0808
Epoch: 32, Training Loss: 0.9736, Test Loss: 1.0746
Epoch: 33, Training Loss: 0.9693, Test Loss: 1.0799
Epoch: 34, Training Loss: 0.9634, Test Loss: 1.0519
Epoch: 35, Training Loss: 0.9582, Test Loss: 1.0635
Epoch: 36, Training Loss: 0.9499, Test Loss: 1.0638
Epoch: 37, Training Loss: 0.9378, Test Loss: 1.0484
Epoch: 38, Training Loss: 0.9260, Test Loss: 1.0273
Epoch: 39, Training Loss: 0.9154, Test Loss: 1.0320
Epoch: 40, Training Loss: 0.9136, Test Loss: 1.0128
Epoch: 41, Training Loss: 0.9065, Test Loss: 1.0161
Epoch: 42, Training Loss: 0.9038, Test Loss: 0.9972
Epoch: 43, Training Loss: 0.8974, Test Loss: 0.9939

 28%|███████████████████████████▍                                                                      | 70/250 [00:05<00:15, 11.89it/s, loss_test=0.924]
Epoch: 45, Training Loss: 0.8829, Test Loss: 0.9858
Epoch: 46, Training Loss: 0.8851, Test Loss: 0.9693
Epoch: 47, Training Loss: 0.8751, Test Loss: 0.9820
Epoch: 48, Training Loss: 0.8733, Test Loss: 0.9684
Epoch: 49, Training Loss: 0.8677, Test Loss: 0.9663
Epoch: 50, Training Loss: 0.8614, Test Loss: 0.9818
Epoch: 51, Training Loss: 0.8608, Test Loss: 0.9566
Epoch: 52, Training Loss: 0.8593, Test Loss: 0.9556
Epoch: 53, Training Loss: 0.8508, Test Loss: 0.9618
Epoch: 54, Training Loss: 0.8473, Test Loss: 0.9571
Epoch: 55, Training Loss: 0.8477, Test Loss: 0.9405
Epoch: 56, Training Loss: 0.8414, Test Loss: 0.9590
Epoch: 57, Training Loss: 0.8405, Test Loss: 0.9468
Epoch: 58, Training Loss: 0.8358, Test Loss: 0.9681
Epoch: 59, Training Loss: 0.8313, Test Loss: 0.9439
Epoch: 60, Training Loss: 0.8278, Test Loss: 0.9473
Epoch: 61, Training Loss: 0.8282, Test Loss: 0.9502
Epoch: 62, Training Loss: 0.8183, Test Loss: 0.9372
Epoch: 63, Training Loss: 0.8179, Test Loss: 0.9433
Epoch: 64, Training Loss: 0.8170, Test Loss: 0.9491
Epoch: 65, Training Loss: 0.8138, Test Loss: 0.9270
Epoch: 66, Training Loss: 0.8085, Test Loss: 0.9251
Epoch: 67, Training Loss: 0.8050, Test Loss: 0.9176

 38%|████████████████████████████████████▊                                                             | 94/250 [00:07<00:12, 12.28it/s, loss_test=0.895]
Epoch: 69, Training Loss: 0.7980, Test Loss: 0.9238
Epoch: 70, Training Loss: 0.7921, Test Loss: 0.9133
Epoch: 71, Training Loss: 0.7910, Test Loss: 0.9178
Epoch: 72, Training Loss: 0.7870, Test Loss: 0.9256
Epoch: 73, Training Loss: 0.7810, Test Loss: 0.9309
Epoch: 74, Training Loss: 0.7805, Test Loss: 0.9316
Epoch: 75, Training Loss: 0.7740, Test Loss: 0.9047
Epoch: 76, Training Loss: 0.7724, Test Loss: 0.9224
Epoch: 77, Training Loss: 0.7687, Test Loss: 0.9174
Epoch: 78, Training Loss: 0.7659, Test Loss: 0.9207
Epoch: 79, Training Loss: 0.7600, Test Loss: 0.9202
Epoch: 80, Training Loss: 0.7550, Test Loss: 0.9106
Epoch: 81, Training Loss: 0.7511, Test Loss: 0.9047
Epoch: 82, Training Loss: 0.7497, Test Loss: 0.8896
Epoch: 83, Training Loss: 0.7460, Test Loss: 0.9052
Epoch: 84, Training Loss: 0.7446, Test Loss: 0.8951
Epoch: 85, Training Loss: 0.7378, Test Loss: 0.9033
Epoch: 86, Training Loss: 0.7348, Test Loss: 0.8964
Epoch: 87, Training Loss: 0.7324, Test Loss: 0.9085
Epoch: 88, Training Loss: 0.7237, Test Loss: 0.8941
Epoch: 89, Training Loss: 0.7225, Test Loss: 0.9085
Epoch: 90, Training Loss: 0.7166, Test Loss: 0.9078
Epoch: 91, Training Loss: 0.7138, Test Loss: 0.8928
Epoch: 92, Training Loss: 0.7141, Test Loss: 0.9084

 47%|█████████████████████████████████████████████▊                                                   | 118/250 [00:09<00:11, 11.74it/s, loss_test=0.904]
Epoch: 94, Training Loss: 0.7015, Test Loss: 0.8928
Epoch: 95, Training Loss: 0.7021, Test Loss: 0.9029
Epoch: 96, Training Loss: 0.6980, Test Loss: 0.8967
Epoch: 97, Training Loss: 0.6943, Test Loss: 0.8998
Epoch: 98, Training Loss: 0.6916, Test Loss: 0.9032
Epoch: 99, Training Loss: 0.6891, Test Loss: 0.8942
Epoch: 100, Training Loss: 0.6854, Test Loss: 0.9032
Epoch: 101, Training Loss: 0.6812, Test Loss: 0.9053
Epoch: 102, Training Loss: 0.6818, Test Loss: 0.9098
Epoch: 103, Training Loss: 0.6748, Test Loss: 0.9063
Epoch: 104, Training Loss: 0.6732, Test Loss: 0.8927
Epoch: 105, Training Loss: 0.6691, Test Loss: 0.8978
Epoch: 106, Training Loss: 0.6664, Test Loss: 0.8978
Epoch: 107, Training Loss: 0.6660, Test Loss: 0.9092
Epoch: 108, Training Loss: 0.6604, Test Loss: 0.9004
Epoch: 109, Training Loss: 0.6580, Test Loss: 0.8969
Epoch: 110, Training Loss: 0.6563, Test Loss: 0.9086
Epoch: 111, Training Loss: 0.6518, Test Loss: 0.9084
Epoch: 112, Training Loss: 0.6465, Test Loss: 0.8951
Epoch: 113, Training Loss: 0.6449, Test Loss: 0.9079
Epoch: 114, Training Loss: 0.6413, Test Loss: 0.9078
Epoch: 115, Training Loss: 0.6391, Test Loss: 0.9125
Epoch: 116, Training Loss: 0.6359, Test Loss: 0.8996

 57%|███████████████████████████████████████████████████████                                          | 142/250 [00:11<00:08, 12.24it/s, loss_test=0.923]
Epoch: 118, Training Loss: 0.6273, Test Loss: 0.8977
Epoch: 119, Training Loss: 0.6261, Test Loss: 0.9074
Epoch: 120, Training Loss: 0.6228, Test Loss: 0.9052
Epoch: 121, Training Loss: 0.6200, Test Loss: 0.9018
Epoch: 122, Training Loss: 0.6152, Test Loss: 0.9085
Epoch: 123, Training Loss: 0.6135, Test Loss: 0.9058
Epoch: 124, Training Loss: 0.6085, Test Loss: 0.8938
Epoch: 125, Training Loss: 0.6071, Test Loss: 0.9144
Epoch: 126, Training Loss: 0.6060, Test Loss: 0.9001
Epoch: 127, Training Loss: 0.6038, Test Loss: 0.9057
Epoch: 128, Training Loss: 0.5991, Test Loss: 0.9160
Epoch: 129, Training Loss: 0.5978, Test Loss: 0.9133
Epoch: 130, Training Loss: 0.5931, Test Loss: 0.9146
Epoch: 131, Training Loss: 0.5907, Test Loss: 0.9147
Epoch: 132, Training Loss: 0.5879, Test Loss: 0.9123
Epoch: 133, Training Loss: 0.5850, Test Loss: 0.9107
Epoch: 134, Training Loss: 0.5817, Test Loss: 0.9164
Epoch: 135, Training Loss: 0.5757, Test Loss: 0.9168
Epoch: 136, Training Loss: 0.5751, Test Loss: 0.9143
Epoch: 137, Training Loss: 0.5716, Test Loss: 0.9113
Epoch: 138, Training Loss: 0.5675, Test Loss: 0.9095
Epoch: 139, Training Loss: 0.5679, Test Loss: 0.9235
Epoch: 140, Training Loss: 0.5611, Test Loss: 0.9207

 66%|████████████████████████████████████████████████████████████████▍                                | 166/250 [00:13<00:07, 11.88it/s, loss_test=0.947]
Epoch: 142, Training Loss: 0.5581, Test Loss: 0.9227
Epoch: 143, Training Loss: 0.5531, Test Loss: 0.9292
Epoch: 144, Training Loss: 0.5489, Test Loss: 0.9294
Epoch: 145, Training Loss: 0.5490, Test Loss: 0.9133
Epoch: 146, Training Loss: 0.5445, Test Loss: 0.9231
Epoch: 147, Training Loss: 0.5411, Test Loss: 0.9236
Epoch: 148, Training Loss: 0.5386, Test Loss: 0.9266
Epoch: 149, Training Loss: 0.5376, Test Loss: 0.9211
Epoch: 150, Training Loss: 0.5329, Test Loss: 0.9209
Epoch: 151, Training Loss: 0.5300, Test Loss: 0.9435
Epoch: 152, Training Loss: 0.5272, Test Loss: 0.9296
Epoch: 153, Training Loss: 0.5231, Test Loss: 0.9312
Epoch: 154, Training Loss: 0.5220, Test Loss: 0.9336
Epoch: 155, Training Loss: 0.5200, Test Loss: 0.9305
Epoch: 156, Training Loss: 0.5164, Test Loss: 0.9296
Epoch: 157, Training Loss: 0.5143, Test Loss: 0.9460
Epoch: 158, Training Loss: 0.5104, Test Loss: 0.9437
Epoch: 159, Training Loss: 0.5079, Test Loss: 0.9350
Epoch: 160, Training Loss: 0.5055, Test Loss: 0.9388
Epoch: 161, Training Loss: 0.5020, Test Loss: 0.9385
Epoch: 162, Training Loss: 0.4999, Test Loss: 0.9402
Epoch: 163, Training Loss: 0.4958, Test Loss: 0.9511
Epoch: 164, Training Loss: 0.4958, Test Loss: 0.9430

 76%|█████████████████████████████████████████████████████████████████████████▋                       | 190/250 [00:15<00:04, 12.00it/s, loss_test=0.967]
Epoch: 166, Training Loss: 0.4902, Test Loss: 0.9471
Epoch: 167, Training Loss: 0.4858, Test Loss: 0.9453
Epoch: 168, Training Loss: 0.4855, Test Loss: 0.9512
Epoch: 169, Training Loss: 0.4811, Test Loss: 0.9374
Epoch: 170, Training Loss: 0.4788, Test Loss: 0.9588
Epoch: 171, Training Loss: 0.4750, Test Loss: 0.9481
Epoch: 172, Training Loss: 0.4740, Test Loss: 0.9516
Epoch: 173, Training Loss: 0.4704, Test Loss: 0.9423
Epoch: 174, Training Loss: 0.4685, Test Loss: 0.9590
Epoch: 175, Training Loss: 0.4643, Test Loss: 0.9491
Epoch: 176, Training Loss: 0.4631, Test Loss: 0.9540
Epoch: 177, Training Loss: 0.4610, Test Loss: 0.9433
Epoch: 178, Training Loss: 0.4595, Test Loss: 0.9562
Epoch: 179, Training Loss: 0.4560, Test Loss: 0.9644
Epoch: 180, Training Loss: 0.4536, Test Loss: 0.9623
Epoch: 181, Training Loss: 0.4506, Test Loss: 0.9654
Epoch: 182, Training Loss: 0.4491, Test Loss: 0.9607
Epoch: 183, Training Loss: 0.4444, Test Loss: 0.9790
Epoch: 184, Training Loss: 0.4444, Test Loss: 0.9503
Epoch: 185, Training Loss: 0.4419, Test Loss: 0.9595
Epoch: 186, Training Loss: 0.4398, Test Loss: 0.9629
Epoch: 187, Training Loss: 0.4378, Test Loss: 0.9778
Epoch: 188, Training Loss: 0.4343, Test Loss: 0.9684
Epoch: 189, Training Loss: 0.4309, Test Loss: 0.9492

 86%|███████████████████████████████████████████████████████████████████████████████████              | 214/250 [00:17<00:03, 11.89it/s, loss_test=0.986]
Epoch: 191, Training Loss: 0.4307, Test Loss: 0.9671
Epoch: 192, Training Loss: 0.4272, Test Loss: 0.9776
Epoch: 193, Training Loss: 0.4224, Test Loss: 0.9582
Epoch: 194, Training Loss: 0.4234, Test Loss: 0.9723
Epoch: 195, Training Loss: 0.4178, Test Loss: 0.9501
Epoch: 196, Training Loss: 0.4188, Test Loss: 0.9675
Epoch: 197, Training Loss: 0.4152, Test Loss: 0.9871
Epoch: 198, Training Loss: 0.4129, Test Loss: 0.9754
Epoch: 199, Training Loss: 0.4107, Test Loss: 0.9717
Epoch: 200, Training Loss: 0.4100, Test Loss: 0.9625
Epoch: 201, Training Loss: 0.4076, Test Loss: 0.9666
Epoch: 202, Training Loss: 0.4054, Test Loss: 0.9714
Epoch: 203, Training Loss: 0.4030, Test Loss: 0.9937
Epoch: 204, Training Loss: 0.4006, Test Loss: 0.9721
Epoch: 205, Training Loss: 0.3985, Test Loss: 0.9836
Epoch: 206, Training Loss: 0.3976, Test Loss: 0.9922
Epoch: 207, Training Loss: 0.3964, Test Loss: 0.9700
Epoch: 208, Training Loss: 0.3930, Test Loss: 0.9680
Epoch: 209, Training Loss: 0.3913, Test Loss: 0.9826
Epoch: 210, Training Loss: 0.3894, Test Loss: 0.9759
Epoch: 211, Training Loss: 0.3880, Test Loss: 0.9698
Epoch: 212, Training Loss: 0.3861, Test Loss: 0.9740

 96%|█████████████████████████████████████████████████████████████████████████████████████████████    | 240/250 [00:19<00:00, 12.34it/s, loss_test=1.003]
Epoch: 214, Training Loss: 0.3816, Test Loss: 0.9858
Epoch: 215, Training Loss: 0.3808, Test Loss: 0.9836
Epoch: 216, Training Loss: 0.3788, Test Loss: 0.9738
Epoch: 217, Training Loss: 0.3770, Test Loss: 0.9993
Epoch: 218, Training Loss: 0.3743, Test Loss: 0.9649
Epoch: 219, Training Loss: 0.3727, Test Loss: 0.9797
Epoch: 220, Training Loss: 0.3708, Test Loss: 0.9833
Epoch: 221, Training Loss: 0.3688, Test Loss: 0.9904
Epoch: 222, Training Loss: 0.3681, Test Loss: 0.9680
Epoch: 223, Training Loss: 0.3657, Test Loss: 0.9969
Epoch: 224, Training Loss: 0.3636, Test Loss: 0.9809
Epoch: 225, Training Loss: 0.3634, Test Loss: 0.9863
Epoch: 226, Training Loss: 0.3609, Test Loss: 0.9762
Epoch: 227, Training Loss: 0.3589, Test Loss: 1.0123
Epoch: 228, Training Loss: 0.3568, Test Loss: 0.9870
Epoch: 229, Training Loss: 0.3566, Test Loss: 0.9813
Epoch: 230, Training Loss: 0.3548, Test Loss: 0.9830
Epoch: 231, Training Loss: 0.3536, Test Loss: 0.9882
Epoch: 232, Training Loss: 0.3510, Test Loss: 0.9974
Epoch: 233, Training Loss: 0.3502, Test Loss: 0.9945
Epoch: 234, Training Loss: 0.3482, Test Loss: 1.0007
Epoch: 235, Training Loss: 0.3460, Test Loss: 0.9979
Epoch: 236, Training Loss: 0.3457, Test Loss: 1.0026
Epoch: 237, Training Loss: 0.3438, Test Loss: 0.9685

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.09it/s, loss_test=1.010]
Epoch: 239, Training Loss: 0.3407, Test Loss: 1.0026
Epoch: 240, Training Loss: 0.3393, Test Loss: 1.0197
Epoch: 241, Training Loss: 0.3371, Test Loss: 0.9979
Epoch: 242, Training Loss: 0.3362, Test Loss: 0.9924
Epoch: 243, Training Loss: 0.3336, Test Loss: 0.9966
Epoch: 244, Training Loss: 0.3315, Test Loss: 1.0012
Epoch: 245, Training Loss: 0.3313, Test Loss: 1.0027
Epoch: 246, Training Loss: 0.3291, Test Loss: 1.0145
Epoch: 247, Training Loss: 0.3281, Test Loss: 0.9795
Epoch: 248, Training Loss: 0.3272, Test Loss: 0.9984
Epoch: 249, Training Loss: 0.3240, Test Loss: 1.0098
Model saved as model_4621100np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-1260000-4621100np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9993565678596497, 0.9958194255828857, 0.9874954462051392, 0.9941551446914673, 0.9951800465583801, 0.9930636644363403, 0.9910360336303711, 0.9940261125564576, 0.9928102254867553, 0.994781756401062, 0.9905553698539734, 0.9876946806907654, 0.9938655853271484, 0.9825518012046814, 0.9915496230125427, 0.9855995893478393, 0.9850025773048401, 0.986635398864746, 0.9895537376403809, 0.985549533367157, 0.9890115022659302, 0.9845146298408508, 0.9851640701293946, 0.9826884031295776, 0.9841742634773254, 0.9809871554374695, 0.9808360934257507, 0.9821256399154663, 0.9785975575447082, 0.9797877192497253, 0.9803591966629028, 0.9803884029388428, 0.9736499428749085, 0.9692678451538086, 0.963398277759552, 0.958233094215393, 0.9498631477355957, 0.9378161907196045, 0.9259784936904907, 0.9154290437698365, 0.9136165261268616, 0.9064847946166992, 0.9038412809371948, 0.897388243675232, 0.8890040159225464, 0.882931387424469, 0.885073983669281, 0.8751319527626038, 0.8733262181282043, 0.8677018165588379, 0.8614298701286316, 0.8608331322669983, 0.8592719554901123, 0.850817596912384, 0.8472511410713196, 0.8477248549461365, 0.8413503766059875, 0.8404510259628296, 0.8357949376106262, 0.831263792514801, 0.8278120160102844, 0.8281819462776184, 0.8182641386985778, 0.8178748965263367, 0.8170145988464356, 0.8137802720069885, 0.8085163950920105, 0.8050142288208008, 0.8027018308639526, 0.7980149149894714, 0.7920943140983582, 0.791015088558197, 0.7869624137878418, 0.780966293811798, 0.7805344820022583, 0.7740409255027771, 0.7723920702934265, 0.7686928391456604, 0.7659032583236695, 0.7600236892700195, 0.7549640297889709, 0.751068389415741, 0.7496852397918701, 0.7459848284721374, 0.7446412324905396, 0.7378101229667664, 0.7347600579261779, 0.7323942422866822, 0.7237147808074951, 0.722459900379181, 0.7165915250778199, 0.7138412475585938, 0.714065682888031, 0.7101953029632568, 0.7014728426933289, 0.7021020412445068, 0.6979503273963928, 0.6943105936050415, 0.6916032791137695, 0.6891140699386596, 0.6853608250617981, 0.6811598777770996, 0.6818283438682556, 0.6747973799705506, 0.6731746435165405, 0.6690582156181335, 0.6664446830749512, 0.6659573793411255, 0.6604484438896179, 0.6579811215400696, 0.6562735438346863, 0.6517740249633789, 0.6464869856834412, 0.6449003100395203, 0.6413309812545777, 0.6391456484794616, 0.635878574848175, 0.6339095950126648, 0.6273173451423645, 0.6260961532592774, 0.6228405833244324, 0.6199504613876343, 0.6151962399482727, 0.6135083198547363, 0.6084970712661744, 0.6071043252944947, 0.6060130953788757, 0.6037640690803527, 0.5990687131881713, 0.5977675557136536, 0.5931403756141662, 0.5907050490379333, 0.587949001789093, 0.5850465536117554, 0.5817114233970642, 0.5756994366645813, 0.5751009583473206, 0.5715853452682496, 0.5674675107002258, 0.5679493188858032, 0.5611425638198853, 0.5594384670257568, 0.5580691695213318, 0.553129518032074, 0.5489224672317505, 0.5489585876464844, 0.5444650530815125, 0.5411064267158509, 0.5385919570922851, 0.5376397252082825, 0.532892394065857, 0.5299512028694153, 0.5271585941314697, 0.5230898380279541, 0.5220212697982788, 0.5200228929519654, 0.5164262175559997, 0.5142547190189362, 0.5103804647922516, 0.5078683137893677, 0.5054633021354675, 0.5019605219364166, 0.4998730719089508, 0.49581246376037597, 0.49580053687095643, 0.49355642795562743, 0.4901528239250183, 0.4857902407646179, 0.48546329140663147, 0.481080961227417, 0.478816294670105, 0.47495073080062866, 0.4740365743637085, 0.4704055666923523, 0.4685279130935669, 0.4642663478851318, 0.4630763292312622, 0.4609627783298492, 0.45946741104125977, 0.4559957683086395, 0.45356536507606504, 0.4506409227848053, 0.44914391040802004, 0.4444283485412598, 0.44442903995513916, 0.4418880224227905, 0.4398213863372803, 0.4377736508846283, 0.4343222796916962, 0.4309422254562378, 0.43046727776527405, 0.4306779980659485, 0.4272151291370392, 0.42235406637191775, 0.4234450340270996, 0.41781187057495117, 0.4187887072563171, 0.415249639749527, 0.4128523588180542, 0.4107235074043274, 0.4100183010101318, 0.40761169195175173, 0.40543659329414367, 0.4030433058738708, 0.40063645243644713, 0.39846089482307434, 0.39762925505638125, 0.39638779163360593, 0.39301263689994814, 0.39131391048431396, 0.389447945356369, 0.38799413442611697, 0.3861206710338593, 0.3847693741321564, 0.38163411021232607, 0.3808270156383514, 0.3787661731243134, 0.37698684334754945, 0.374303138256073, 0.3727299630641937, 0.3707936882972717, 0.36879247426986694, 0.36810622811317445, 0.365670508146286, 0.36355760097503664, 0.3634094476699829, 0.36091052293777465, 0.3589123547077179, 0.3567975878715515, 0.3565990269184113, 0.3548034965991974, 0.35358440279960635, 0.35098329186439514, 0.3502043545246124, 0.3481558382511139, 0.34602671265602114, 0.34572123885154726, 0.34375243782997134, 0.34156314134597776, 0.3406814455986023, 0.3393136143684387, 0.33707589507102964, 0.33619177937507627, 0.3335600018501282, 0.33152082562446594, 0.3313092291355133, 0.32905670404434206, 0.3280594706535339, 0.3271705687046051, 0.3239663362503052], 'loss_test': [1.0754605531692505, 1.0647023916244507, 1.087177038192749, 1.0824021100997925, 1.0791736841201782, 1.0814355611801147, 1.0645889043807983, 1.0700175762176514, 1.0733304023742676, 1.0858750343322754, 1.073538064956665, 1.0791774988174438, 1.0652368068695068, 1.0704827308654785, 1.0808535814285278, 1.082880973815918, 1.0834366083145142, 1.0845462083816528, 1.069313883781433, 1.0726672410964966, 1.094452977180481, 1.0902448892593384, 1.074082374572754, 1.0652903318405151, 1.06960129737854, 1.0662143230438232, 1.0891495943069458, 1.0767284631729126, 1.0775374174118042, 1.090144395828247, 1.0741945505142212, 1.0807766914367676, 1.0745590925216675, 1.079921007156372, 1.0519360303878784, 1.0635161399841309, 1.0638346672058105, 1.0483827590942383, 1.0272830724716187, 1.0319715738296509, 1.0127507448196411, 1.0161402225494385, 0.9972265958786011, 0.993938148021698, 0.9854960441589355, 0.9857932329177856, 0.9692807197570801, 0.9819684028625488, 0.9683685302734375, 0.9663308262825012, 0.981756865978241, 0.9565872550010681, 0.9555874466896057, 0.9618049263954163, 0.9570543169975281, 0.940479040145874, 0.9590222239494324, 0.9468282461166382, 0.9680727124214172, 0.9438548684120178, 0.947325587272644, 0.9501975178718567, 0.9372130036354065, 0.9433285593986511, 0.9490712881088257, 0.9270480871200562, 0.9251353740692139, 0.9175745248794556, 0.9293405413627625, 0.9238132238388062, 0.9133026599884033, 0.9178394079208374, 0.9255967140197754, 0.9308705925941467, 0.9316377639770508, 0.9047083854675293, 0.9224364757537842, 0.9174458384513855, 0.9206929802894592, 0.9201720356941223, 0.9106400012969971, 0.9047229290008545, 0.8895958065986633, 0.9052466750144958, 0.8950546383857727, 0.9032857418060303, 0.896407961845398, 0.9084842205047607, 0.8940669894218445, 0.9084983468055725, 0.907758891582489, 0.8927629590034485, 0.9083830118179321, 0.8953208923339844, 0.8928025960922241, 0.9029268026351929, 0.8967166543006897, 0.8997677564620972, 0.9031771421432495, 0.8942326307296753, 0.9031668305397034, 0.9052950143814087, 0.9097522497177124, 0.9062605500221252, 0.8927035927772522, 0.897849977016449, 0.8977686762809753, 0.9091920852661133, 0.9004203081130981, 0.8968551754951477, 0.9085558652877808, 0.9083641171455383, 0.8950768709182739, 0.9079240560531616, 0.9078368544578552, 0.9124566316604614, 0.8995819091796875, 0.9038030505180359, 0.8976859450340271, 0.9074171781539917, 0.905208170413971, 0.901782751083374, 0.9085471630096436, 0.9057746529579163, 0.8937554359436035, 0.9144423007965088, 0.9001109004020691, 0.905656635761261, 0.9160344004631042, 0.9133263826370239, 0.9146407246589661, 0.9146546721458435, 0.9123060703277588, 0.9106867909431458, 0.9164465665817261, 0.9167847037315369, 0.9143285155296326, 0.9112723469734192, 0.909529447555542, 0.9234774708747864, 0.9207453727722168, 0.9305905103683472, 0.9227272868156433, 0.9292230010032654, 0.9293652176856995, 0.9132717847824097, 0.9231249094009399, 0.9236313104629517, 0.9265903234481812, 0.9211035966873169, 0.9209464192390442, 0.9435209631919861, 0.9295918941497803, 0.9311737418174744, 0.9335836172103882, 0.9304648637771606, 0.929584801197052, 0.9459936618804932, 0.9436947703361511, 0.9350111484527588, 0.9388388991355896, 0.9384991526603699, 0.940214216709137, 0.9510616660118103, 0.9429970979690552, 0.9415618181228638, 0.9471033215522766, 0.9453317523002625, 0.9511920809745789, 0.9373682737350464, 0.9587941765785217, 0.9481177926063538, 0.9516243934631348, 0.9423028230667114, 0.9590001702308655, 0.9490700960159302, 0.9539816379547119, 0.9433332681655884, 0.9561776518821716, 0.9644197225570679, 0.9623364210128784, 0.9653986692428589, 0.9606591463088989, 0.978973388671875, 0.950344979763031, 0.9594977498054504, 0.9628779292106628, 0.9778212308883667, 0.9684285521507263, 0.949247419834137, 0.9831207394599915, 0.9671319723129272, 0.9775650501251221, 0.9582117795944214, 0.9723290801048279, 0.9501144886016846, 0.9674540162086487, 0.9871277213096619, 0.9754258394241333, 0.9716968536376953, 0.962501049041748, 0.9665642976760864, 0.971407413482666, 0.9936773180961609, 0.9720935821533203, 0.983577311038971, 0.9921613931655884, 0.9700241088867188, 0.9680342078208923, 0.9826034903526306, 0.9758780598640442, 0.9697601795196533, 0.9740471839904785, 0.9769346714019775, 0.9858399629592896, 0.9836387038230896, 0.9738030433654785, 0.9992915987968445, 0.964890718460083, 0.9797362685203552, 0.9832674264907837, 0.9904431104660034, 0.9680472612380981, 0.9969290494918823, 0.9809337258338928, 0.986267626285553, 0.9761761426925659, 1.0122804641723633, 0.987044095993042, 0.9812736511230469, 0.9829512238502502, 0.9881568551063538, 0.9974430799484253, 0.9944936633110046, 1.0006743669509888, 0.9979416131973267, 1.0026462078094482, 0.9684587717056274, 1.0030128955841064, 1.0025522708892822, 1.0197432041168213, 0.9979119300842285, 0.9924111366271973, 0.9965939521789551, 1.0011940002441406, 1.0027376413345337, 1.0144788026809692, 0.9795310497283936, 0.9983847737312317, 1.0097826719284058], 'identifier': '4621100np'}