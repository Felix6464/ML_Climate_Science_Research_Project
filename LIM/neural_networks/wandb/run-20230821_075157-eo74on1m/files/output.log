
  8%|███████▍                                                                                          | 19/250 [00:01<00:19, 12.07it/s, loss_test=1.092]
Epoch: 00, Training Loss: 0.9882, Test Loss: 1.0770
Epoch: 01, Training Loss: 0.9899, Test Loss: 1.0982
Epoch: 02, Training Loss: 0.9923, Test Loss: 1.0713
Epoch: 03, Training Loss: 0.9920, Test Loss: 1.0847
Epoch: 04, Training Loss: 0.9919, Test Loss: 1.0857
Epoch: 05, Training Loss: 0.9903, Test Loss: 1.0794
Epoch: 06, Training Loss: 0.9881, Test Loss: 1.0992
Epoch: 07, Training Loss: 0.9879, Test Loss: 1.0837
Epoch: 08, Training Loss: 0.9918, Test Loss: 1.1023
Epoch: 09, Training Loss: 0.9917, Test Loss: 1.0735
Epoch: 10, Training Loss: 0.9877, Test Loss: 1.0891
Epoch: 11, Training Loss: 0.9864, Test Loss: 1.0931
Epoch: 12, Training Loss: 0.9932, Test Loss: 1.0951
Epoch: 13, Training Loss: 0.9903, Test Loss: 1.1006
Epoch: 14, Training Loss: 0.9847, Test Loss: 1.0941
Epoch: 15, Training Loss: 0.9885, Test Loss: 1.0691
Epoch: 16, Training Loss: 0.9832, Test Loss: 1.0884
Epoch: 17, Training Loss: 0.9862, Test Loss: 1.0744
Epoch: 18, Training Loss: 0.9810, Test Loss: 1.0882

 17%|████████████████▊                                                                                 | 43/250 [00:03<00:17, 12.01it/s, loss_test=0.994]
Epoch: 20, Training Loss: 0.9859, Test Loss: 1.0698
Epoch: 21, Training Loss: 0.9862, Test Loss: 1.0917
Epoch: 22, Training Loss: 0.9821, Test Loss: 1.1021
Epoch: 23, Training Loss: 0.9822, Test Loss: 1.0773
Epoch: 24, Training Loss: 0.9812, Test Loss: 1.0842
Epoch: 25, Training Loss: 0.9822, Test Loss: 1.0828
Epoch: 26, Training Loss: 0.9778, Test Loss: 1.0780
Epoch: 27, Training Loss: 0.9787, Test Loss: 1.0725
Epoch: 28, Training Loss: 0.9785, Test Loss: 1.0866
Epoch: 29, Training Loss: 0.9781, Test Loss: 1.0867
Epoch: 30, Training Loss: 0.9729, Test Loss: 1.0822
Epoch: 31, Training Loss: 0.9670, Test Loss: 1.0621
Epoch: 32, Training Loss: 0.9671, Test Loss: 1.0754
Epoch: 33, Training Loss: 0.9583, Test Loss: 1.0585
Epoch: 34, Training Loss: 0.9465, Test Loss: 1.0679
Epoch: 35, Training Loss: 0.9349, Test Loss: 1.0685
Epoch: 36, Training Loss: 0.9305, Test Loss: 1.0297
Epoch: 37, Training Loss: 0.9197, Test Loss: 1.0446
Epoch: 38, Training Loss: 0.9149, Test Loss: 1.0135
Epoch: 39, Training Loss: 0.9111, Test Loss: 1.0214
Epoch: 40, Training Loss: 0.9076, Test Loss: 1.0091
Epoch: 41, Training Loss: 0.9036, Test Loss: 1.0142
Epoch: 42, Training Loss: 0.8943, Test Loss: 0.9889

 28%|███████████████████████████                                                                       | 69/250 [00:05<00:15, 11.91it/s, loss_test=0.939]
Epoch: 44, Training Loss: 0.8864, Test Loss: 0.9855
Epoch: 45, Training Loss: 0.8828, Test Loss: 0.9900
Epoch: 46, Training Loss: 0.8772, Test Loss: 0.9855
Epoch: 47, Training Loss: 0.8735, Test Loss: 0.9726
Epoch: 48, Training Loss: 0.8737, Test Loss: 0.9642
Epoch: 49, Training Loss: 0.8677, Test Loss: 0.9783
Epoch: 50, Training Loss: 0.8657, Test Loss: 0.9726
Epoch: 51, Training Loss: 0.8640, Test Loss: 0.9862
Epoch: 52, Training Loss: 0.8590, Test Loss: 0.9741
Epoch: 53, Training Loss: 0.8589, Test Loss: 0.9671
Epoch: 54, Training Loss: 0.8516, Test Loss: 0.9687
Epoch: 55, Training Loss: 0.8500, Test Loss: 0.9499
Epoch: 56, Training Loss: 0.8484, Test Loss: 0.9508
Epoch: 57, Training Loss: 0.8453, Test Loss: 0.9363
Epoch: 58, Training Loss: 0.8373, Test Loss: 0.9532
Epoch: 59, Training Loss: 0.8355, Test Loss: 0.9515
Epoch: 60, Training Loss: 0.8319, Test Loss: 0.9466
Epoch: 61, Training Loss: 0.8303, Test Loss: 0.9443
Epoch: 62, Training Loss: 0.8248, Test Loss: 0.9408
Epoch: 63, Training Loss: 0.8164, Test Loss: 0.9171
Epoch: 64, Training Loss: 0.8140, Test Loss: 0.9205
Epoch: 65, Training Loss: 0.8123, Test Loss: 0.9564
Epoch: 66, Training Loss: 0.8065, Test Loss: 0.9312
Epoch: 67, Training Loss: 0.8049, Test Loss: 0.9297

 37%|████████████████████████████████████▍                                                             | 93/250 [00:07<00:12, 12.25it/s, loss_test=0.899]
Epoch: 69, Training Loss: 0.7962, Test Loss: 0.9465
Epoch: 70, Training Loss: 0.7923, Test Loss: 0.9308
Epoch: 71, Training Loss: 0.7890, Test Loss: 0.9069
Epoch: 72, Training Loss: 0.7855, Test Loss: 0.9463
Epoch: 73, Training Loss: 0.7818, Test Loss: 0.9331
Epoch: 74, Training Loss: 0.7788, Test Loss: 0.9236
Epoch: 75, Training Loss: 0.7729, Test Loss: 0.9168
Epoch: 76, Training Loss: 0.7702, Test Loss: 0.9068
Epoch: 77, Training Loss: 0.7657, Test Loss: 0.9247
Epoch: 78, Training Loss: 0.7620, Test Loss: 0.9153
Epoch: 79, Training Loss: 0.7583, Test Loss: 0.9063
Epoch: 80, Training Loss: 0.7558, Test Loss: 0.9141
Epoch: 81, Training Loss: 0.7511, Test Loss: 0.9347
Epoch: 82, Training Loss: 0.7509, Test Loss: 0.9356
Epoch: 83, Training Loss: 0.7452, Test Loss: 0.9262
Epoch: 84, Training Loss: 0.7399, Test Loss: 0.9291
Epoch: 85, Training Loss: 0.7350, Test Loss: 0.9216
Epoch: 86, Training Loss: 0.7352, Test Loss: 0.9234
Epoch: 87, Training Loss: 0.7315, Test Loss: 0.9258
Epoch: 88, Training Loss: 0.7286, Test Loss: 0.9167
Epoch: 89, Training Loss: 0.7214, Test Loss: 0.9122
Epoch: 90, Training Loss: 0.7203, Test Loss: 0.9013
Epoch: 91, Training Loss: 0.7169, Test Loss: 0.8999

 48%|██████████████████████████████████████████████▏                                                  | 119/250 [00:09<00:10, 12.51it/s, loss_test=0.912]
Epoch: 93, Training Loss: 0.7065, Test Loss: 0.8990
Epoch: 94, Training Loss: 0.7037, Test Loss: 0.9125
Epoch: 95, Training Loss: 0.7035, Test Loss: 0.9255
Epoch: 96, Training Loss: 0.7005, Test Loss: 0.9057
Epoch: 97, Training Loss: 0.6941, Test Loss: 0.9059
Epoch: 98, Training Loss: 0.6916, Test Loss: 0.9012
Epoch: 99, Training Loss: 0.6910, Test Loss: 0.9002
Epoch: 100, Training Loss: 0.6857, Test Loss: 0.8999
Epoch: 101, Training Loss: 0.6831, Test Loss: 0.9016
Epoch: 102, Training Loss: 0.6777, Test Loss: 0.8975
Epoch: 103, Training Loss: 0.6727, Test Loss: 0.9028
Epoch: 104, Training Loss: 0.6696, Test Loss: 0.8964
Epoch: 105, Training Loss: 0.6681, Test Loss: 0.9106
Epoch: 106, Training Loss: 0.6626, Test Loss: 0.8986
Epoch: 107, Training Loss: 0.6603, Test Loss: 0.9055
Epoch: 108, Training Loss: 0.6574, Test Loss: 0.8986
Epoch: 109, Training Loss: 0.6541, Test Loss: 0.9046
Epoch: 110, Training Loss: 0.6513, Test Loss: 0.8973
Epoch: 111, Training Loss: 0.6474, Test Loss: 0.8945
Epoch: 112, Training Loss: 0.6432, Test Loss: 0.8980
Epoch: 113, Training Loss: 0.6404, Test Loss: 0.9012
Epoch: 114, Training Loss: 0.6385, Test Loss: 0.9035
Epoch: 115, Training Loss: 0.6343, Test Loss: 0.9002
Epoch: 116, Training Loss: 0.6322, Test Loss: 0.8998
Epoch: 117, Training Loss: 0.6298, Test Loss: 0.9119
Epoch: 118, Training Loss: 0.6246, Test Loss: 0.9119
Epoch: 119, Training Loss: 0.6215, Test Loss: 0.9063
Epoch: 120, Training Loss: 0.6178, Test Loss: 0.8800
Epoch: 121, Training Loss: 0.6177, Test Loss: 0.9141
Epoch: 122, Training Loss: 0.6129, Test Loss: 0.9115
Epoch: 123, Training Loss: 0.6108, Test Loss: 0.9009

 57%|███████████████████████████████████████████████████████▍                                         | 143/250 [00:11<00:09, 11.73it/s, loss_test=0.912]
Epoch: 125, Training Loss: 0.6049, Test Loss: 0.9130
Epoch: 126, Training Loss: 0.6013, Test Loss: 0.8890
Epoch: 127, Training Loss: 0.5971, Test Loss: 0.9072
Epoch: 128, Training Loss: 0.5934, Test Loss: 0.8997
Epoch: 129, Training Loss: 0.5938, Test Loss: 0.9058
Epoch: 130, Training Loss: 0.5909, Test Loss: 0.9020
Epoch: 131, Training Loss: 0.5864, Test Loss: 0.9176
Epoch: 132, Training Loss: 0.5832, Test Loss: 0.9128
Epoch: 133, Training Loss: 0.5814, Test Loss: 0.9080
Epoch: 134, Training Loss: 0.5782, Test Loss: 0.9083
Epoch: 135, Training Loss: 0.5734, Test Loss: 0.9102
Epoch: 136, Training Loss: 0.5699, Test Loss: 0.9176
Epoch: 137, Training Loss: 0.5679, Test Loss: 0.9069
Epoch: 138, Training Loss: 0.5667, Test Loss: 0.9070
Epoch: 139, Training Loss: 0.5609, Test Loss: 0.9043
Epoch: 140, Training Loss: 0.5591, Test Loss: 0.9111
Epoch: 141, Training Loss: 0.5554, Test Loss: 0.9064
Epoch: 142, Training Loss: 0.5514, Test Loss: 0.9117
Epoch: 143, Training Loss: 0.5504, Test Loss: 0.9101
Epoch: 144, Training Loss: 0.5467, Test Loss: 0.9189
Epoch: 145, Training Loss: 0.5454, Test Loss: 0.9159
Epoch: 146, Training Loss: 0.5413, Test Loss: 0.9198

 67%|████████████████████████████████████████████████████████████████▊                                | 167/250 [00:13<00:06, 12.22it/s, loss_test=0.944]
Epoch: 148, Training Loss: 0.5346, Test Loss: 0.9100
Epoch: 149, Training Loss: 0.5322, Test Loss: 0.9273
Epoch: 150, Training Loss: 0.5310, Test Loss: 0.9223
Epoch: 151, Training Loss: 0.5265, Test Loss: 0.9144
Epoch: 152, Training Loss: 0.5259, Test Loss: 0.9336
Epoch: 153, Training Loss: 0.5226, Test Loss: 0.9129
Epoch: 154, Training Loss: 0.5174, Test Loss: 0.9255
Epoch: 155, Training Loss: 0.5163, Test Loss: 0.9242
Epoch: 156, Training Loss: 0.5127, Test Loss: 0.9200
Epoch: 157, Training Loss: 0.5112, Test Loss: 0.9307
Epoch: 158, Training Loss: 0.5086, Test Loss: 0.9397
Epoch: 159, Training Loss: 0.5051, Test Loss: 0.9225
Epoch: 160, Training Loss: 0.5009, Test Loss: 0.9325
Epoch: 161, Training Loss: 0.4980, Test Loss: 0.9232
Epoch: 162, Training Loss: 0.4965, Test Loss: 0.9374
Epoch: 163, Training Loss: 0.4944, Test Loss: 0.9223
Epoch: 164, Training Loss: 0.4907, Test Loss: 0.9354
Epoch: 165, Training Loss: 0.4888, Test Loss: 0.9272

 76%|██████████████████████████████████████████████████████████████████████████                       | 191/250 [00:15<00:04, 12.15it/s, loss_test=0.957]
Epoch: 167, Training Loss: 0.4835, Test Loss: 0.9436
Epoch: 168, Training Loss: 0.4809, Test Loss: 0.9362
Epoch: 169, Training Loss: 0.4780, Test Loss: 0.9272
Epoch: 170, Training Loss: 0.4757, Test Loss: 0.9477
Epoch: 171, Training Loss: 0.4723, Test Loss: 0.9383
Epoch: 172, Training Loss: 0.4705, Test Loss: 0.9539
Epoch: 173, Training Loss: 0.4681, Test Loss: 0.9473
Epoch: 174, Training Loss: 0.4667, Test Loss: 0.9447
Epoch: 175, Training Loss: 0.4633, Test Loss: 0.9532
Epoch: 176, Training Loss: 0.4609, Test Loss: 0.9401
Epoch: 177, Training Loss: 0.4592, Test Loss: 0.9459
Epoch: 178, Training Loss: 0.4568, Test Loss: 0.9549
Epoch: 179, Training Loss: 0.4529, Test Loss: 0.9533
Epoch: 180, Training Loss: 0.4517, Test Loss: 0.9520
Epoch: 181, Training Loss: 0.4472, Test Loss: 0.9502
Epoch: 182, Training Loss: 0.4456, Test Loss: 0.9512
Epoch: 183, Training Loss: 0.4456, Test Loss: 0.9646
Epoch: 184, Training Loss: 0.4420, Test Loss: 0.9573
Epoch: 185, Training Loss: 0.4400, Test Loss: 0.9663
Epoch: 186, Training Loss: 0.4366, Test Loss: 0.9555
Epoch: 187, Training Loss: 0.4358, Test Loss: 0.9614
Epoch: 188, Training Loss: 0.4326, Test Loss: 0.9597
Epoch: 189, Training Loss: 0.4300, Test Loss: 0.9584

 87%|████████████████████████████████████████████████████████████████████████████████████▏            | 217/250 [00:17<00:02, 12.50it/s, loss_test=1.006]
Epoch: 191, Training Loss: 0.4248, Test Loss: 0.9574
Epoch: 192, Training Loss: 0.4234, Test Loss: 0.9606
Epoch: 193, Training Loss: 0.4213, Test Loss: 0.9687
Epoch: 194, Training Loss: 0.4190, Test Loss: 0.9718
Epoch: 195, Training Loss: 0.4163, Test Loss: 0.9678
Epoch: 196, Training Loss: 0.4136, Test Loss: 0.9799
Epoch: 197, Training Loss: 0.4115, Test Loss: 0.9746
Epoch: 198, Training Loss: 0.4093, Test Loss: 0.9815
Epoch: 199, Training Loss: 0.4067, Test Loss: 0.9817
Epoch: 200, Training Loss: 0.4047, Test Loss: 0.9805
Epoch: 201, Training Loss: 0.4037, Test Loss: 0.9741
Epoch: 202, Training Loss: 0.4028, Test Loss: 0.9815
Epoch: 203, Training Loss: 0.3992, Test Loss: 0.9606
Epoch: 204, Training Loss: 0.3970, Test Loss: 0.9746
Epoch: 205, Training Loss: 0.3963, Test Loss: 1.0016
Epoch: 206, Training Loss: 0.3946, Test Loss: 0.9911
Epoch: 207, Training Loss: 0.3906, Test Loss: 0.9885
Epoch: 208, Training Loss: 0.3901, Test Loss: 0.9876
Epoch: 209, Training Loss: 0.3873, Test Loss: 0.9804
Epoch: 210, Training Loss: 0.3854, Test Loss: 0.9865
Epoch: 211, Training Loss: 0.3836, Test Loss: 0.9808
Epoch: 212, Training Loss: 0.3825, Test Loss: 0.9872
Epoch: 213, Training Loss: 0.3785, Test Loss: 0.9739
Epoch: 214, Training Loss: 0.3781, Test Loss: 0.9820

 96%|█████████████████████████████████████████████████████████████████████████████████████████████▌   | 241/250 [00:19<00:00, 12.15it/s, loss_test=1.033]
Epoch: 216, Training Loss: 0.3743, Test Loss: 1.0064
Epoch: 217, Training Loss: 0.3716, Test Loss: 0.9900
Epoch: 218, Training Loss: 0.3702, Test Loss: 0.9818
Epoch: 219, Training Loss: 0.3699, Test Loss: 1.0123
Epoch: 220, Training Loss: 0.3667, Test Loss: 1.0045
Epoch: 221, Training Loss: 0.3640, Test Loss: 1.0031
Epoch: 222, Training Loss: 0.3631, Test Loss: 1.0014
Epoch: 223, Training Loss: 0.3609, Test Loss: 0.9888
Epoch: 224, Training Loss: 0.3593, Test Loss: 1.0064
Epoch: 225, Training Loss: 0.3568, Test Loss: 1.0109
Epoch: 226, Training Loss: 0.3562, Test Loss: 1.0162
Epoch: 227, Training Loss: 0.3539, Test Loss: 1.0046
Epoch: 228, Training Loss: 0.3518, Test Loss: 0.9983
Epoch: 229, Training Loss: 0.3501, Test Loss: 1.0116
Epoch: 230, Training Loss: 0.3488, Test Loss: 1.0142
Epoch: 231, Training Loss: 0.3455, Test Loss: 1.0207
Epoch: 232, Training Loss: 0.3466, Test Loss: 1.0172
Epoch: 233, Training Loss: 0.3438, Test Loss: 1.0145
Epoch: 234, Training Loss: 0.3421, Test Loss: 1.0129
Epoch: 235, Training Loss: 0.3400, Test Loss: 1.0114
Epoch: 236, Training Loss: 0.3380, Test Loss: 1.0099
Epoch: 237, Training Loss: 0.3365, Test Loss: 1.0155
Epoch: 238, Training Loss: 0.3376, Test Loss: 1.0130

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.17it/s, loss_test=1.026]
Epoch: 240, Training Loss: 0.3327, Test Loss: 1.0192
Epoch: 241, Training Loss: 0.3308, Test Loss: 1.0332
Epoch: 242, Training Loss: 0.3291, Test Loss: 1.0310
Epoch: 243, Training Loss: 0.3269, Test Loss: 1.0166
Epoch: 244, Training Loss: 0.3252, Test Loss: 1.0214
Epoch: 245, Training Loss: 0.3250, Test Loss: 1.0201
Epoch: 246, Training Loss: 0.3231, Test Loss: 1.0341
Epoch: 247, Training Loss: 0.3212, Test Loss: 1.0221
Epoch: 248, Training Loss: 0.3194, Test Loss: 1.0189
Epoch: 249, Training Loss: 0.3176, Test Loss: 1.0264
Model saved as model_6332061np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-127000-6332061np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9882232069969177, 0.9898517489433288, 0.9922664046287537, 0.9920347809791565, 0.9918729424476623, 0.9902576088905335, 0.9881425857543945, 0.9878834128379822, 0.9917876243591308, 0.9916562914848328, 0.9877351760864258, 0.986368715763092, 0.9932302951812744, 0.9902985453605652, 0.9847103714942932, 0.9884538412094116, 0.9832461833953857, 0.9861611962318421, 0.9810065031051636, 0.9868523716926575, 0.985896635055542, 0.9861879587173462, 0.9821428656578064, 0.9821979165077209, 0.9812494516372681, 0.9821598291397095, 0.9777757406234742, 0.9787470936775208, 0.9785063743591309, 0.9781453013420105, 0.9728542923927307, 0.9670110940933228, 0.9670938730239869, 0.9583159923553467, 0.9465320110321045, 0.9348721385002137, 0.9305336594581604, 0.91971275806427, 0.9149487137794494, 0.9110896348953247, 0.9075690865516662, 0.9036336302757263, 0.8942727088928223, 0.8920885682106018, 0.8863998889923096, 0.8828231930732727, 0.8772343397140503, 0.8735342741012573, 0.8737440824508667, 0.8677038431167603, 0.865718424320221, 0.8640427827835083, 0.8589886546134948, 0.8588924646377564, 0.8515612483024597, 0.8499796509742736, 0.848358952999115, 0.8452615141868591, 0.8373373746871948, 0.835535979270935, 0.8319462776184082, 0.8302530288696289, 0.8247854232788085, 0.8164442777633667, 0.8140220761299133, 0.8123154878616333, 0.806522524356842, 0.8049471616744995, 0.7993840932846069, 0.7961682677268982, 0.7922512054443359, 0.7890046238899231, 0.7854522109031677, 0.781761622428894, 0.7787753462791442, 0.7728754281997681, 0.7701859951019288, 0.7656739234924317, 0.7620224237442017, 0.7582798004150391, 0.7558136343955993, 0.7510739207267761, 0.7508836507797241, 0.7452114343643188, 0.7398579835891723, 0.7349993228912354, 0.735153079032898, 0.7315380215644837, 0.7285585761070251, 0.7213956475257873, 0.7202675938606262, 0.716868269443512, 0.7138832807540894, 0.7065159559249878, 0.703737735748291, 0.7034585475921631, 0.7004684329032898, 0.6941489100456237, 0.6915858507156372, 0.6910118460655212, 0.6857495188713074, 0.683052122592926, 0.6776630640029907, 0.6726576685905457, 0.6696367979049682, 0.668082594871521, 0.6626099586486817, 0.6603468656539917, 0.6573535084724427, 0.6540939211845398, 0.651304054260254, 0.6474040389060974, 0.6431643962860107, 0.6404235005378723, 0.6385131001472473, 0.6342962384223938, 0.6321895718574524, 0.6297740697860718, 0.6246257305145264, 0.621457290649414, 0.6177592754364014, 0.6176784992218017, 0.6129251956939697, 0.6107705116271973, 0.6082669138908386, 0.6049492478370666, 0.6013253211975098, 0.5970764279365539, 0.5934499502182007, 0.5938185214996338, 0.590879213809967, 0.5863945126533509, 0.5832389831542969, 0.5813746333122254, 0.5782345056533813, 0.5734414100646973, 0.5698649406433105, 0.5678511977195739, 0.5666856527328491, 0.5608873128890991, 0.559135639667511, 0.5554320693016053, 0.5514156222343445, 0.5503521800041199, 0.5467459559440613, 0.5454156279563904, 0.5413244366645813, 0.5383314967155457, 0.5345887660980224, 0.5321983456611633, 0.530996584892273, 0.5265365362167358, 0.5258820652961731, 0.5225834965705871, 0.517391812801361, 0.516285240650177, 0.5126626491546631, 0.5111960709095001, 0.5086091756820679, 0.5051211357116699, 0.5008921086788177, 0.4979811072349548, 0.4964517056941986, 0.49435179233551024, 0.4907430589199066, 0.488792872428894, 0.4882191836833954, 0.4835144758224487, 0.4808913230895996, 0.4780073702335358, 0.4756950974464417, 0.47228955626487734, 0.470546555519104, 0.46805471181869507, 0.46673070192337035, 0.4633431613445282, 0.4609210193157196, 0.45917887091636655, 0.4567634165287018, 0.4528564453125, 0.45165587663650514, 0.4471900939941406, 0.4456068515777588, 0.4455784857273102, 0.441998165845871, 0.4399849593639374, 0.43663929104804994, 0.4357835650444031, 0.43255933523178103, 0.4299502789974213, 0.4279893457889557, 0.42475866675376894, 0.4234407901763916, 0.4212639629840851, 0.4189712405204773, 0.4163185954093933, 0.4136301755905151, 0.4114977717399597, 0.4093061089515686, 0.40674006938934326, 0.4047044575214386, 0.40371112823486327, 0.4028100073337555, 0.39922081828117373, 0.39701271057128906, 0.3963155746459961, 0.39463786482810975, 0.39058771133422854, 0.390106076002121, 0.3872648239135742, 0.3853828370571136, 0.38360750675201416, 0.3825229585170746, 0.3785074472427368, 0.3781002640724182, 0.3755180835723877, 0.37430200576782224, 0.3715657889842987, 0.37018577456474305, 0.36993967890739443, 0.3666965961456299, 0.36401828527450564, 0.3630990326404572, 0.3609422445297241, 0.3593427538871765, 0.35683900117874146, 0.35621157884597776, 0.35392550230026243, 0.35175455212593076, 0.350122457742691, 0.348763906955719, 0.34554588198661806, 0.3466207504272461, 0.3438146412372589, 0.3420617699623108, 0.3400427520275116, 0.3379800200462341, 0.3364755749702454, 0.3376368939876556, 0.3333729863166809, 0.3327132225036621, 0.3308362543582916, 0.32913588285446166, 0.3268661141395569, 0.3252457737922668, 0.32500691413879396, 0.32313511371612547, 0.32115237712860106, 0.3193646252155304, 0.31762685179710387], 'loss_test': [1.0769752264022827, 1.098181128501892, 1.071269154548645, 1.084650993347168, 1.0856598615646362, 1.0793853998184204, 1.0992066860198975, 1.0836509466171265, 1.1022979021072388, 1.073527216911316, 1.0890640020370483, 1.0930718183517456, 1.0950915813446045, 1.1005730628967285, 1.0941375494003296, 1.069064974784851, 1.0883933305740356, 1.074413537979126, 1.0882399082183838, 1.0917502641677856, 1.0698063373565674, 1.091731071472168, 1.1020780801773071, 1.077254295349121, 1.0842103958129883, 1.082770824432373, 1.0779556035995483, 1.0724948644638062, 1.0865681171417236, 1.0866732597351074, 1.0822386741638184, 1.062108039855957, 1.075383186340332, 1.0584901571273804, 1.0678558349609375, 1.0685088634490967, 1.0297027826309204, 1.0446282625198364, 1.0134533643722534, 1.021358609199524, 1.009053349494934, 1.0141798257827759, 0.9888879060745239, 0.9935947060585022, 0.9855382442474365, 0.9900229573249817, 0.9854692220687866, 0.9726108312606812, 0.9642124176025391, 0.9783439040184021, 0.9726459383964539, 0.9861701130867004, 0.9740951657295227, 0.9670525789260864, 0.9687229990959167, 0.9498608112335205, 0.9507923722267151, 0.936276376247406, 0.9532335996627808, 0.9515292048454285, 0.9465600848197937, 0.9443029165267944, 0.9407538771629333, 0.9171069264411926, 0.920460045337677, 0.9564318656921387, 0.9312403798103333, 0.929692804813385, 0.93899005651474, 0.946521520614624, 0.9308419227600098, 0.9068810343742371, 0.9463009238243103, 0.9331048130989075, 0.923631489276886, 0.9167963862419128, 0.9067538976669312, 0.924730122089386, 0.9152755737304688, 0.90634685754776, 0.9140920042991638, 0.9346522092819214, 0.9356327056884766, 0.926177978515625, 0.9290830492973328, 0.9216133952140808, 0.9234095215797424, 0.9257580637931824, 0.9167333841323853, 0.9122433066368103, 0.9013482928276062, 0.8999215364456177, 0.8927025198936462, 0.8990135192871094, 0.9125095009803772, 0.9255428314208984, 0.9057156443595886, 0.9058907628059387, 0.9012280106544495, 0.9001646637916565, 0.8999369740486145, 0.9016445875167847, 0.8974694609642029, 0.9028066396713257, 0.8964340686798096, 0.9106416702270508, 0.8986402750015259, 0.9055421948432922, 0.8986141681671143, 0.9046463966369629, 0.8972594738006592, 0.8944711089134216, 0.8979836702346802, 0.90118408203125, 0.9034854769706726, 0.9001737236976624, 0.8997951149940491, 0.9119039177894592, 0.9118776321411133, 0.9062818884849548, 0.8800471425056458, 0.9140833020210266, 0.9114534258842468, 0.900886058807373, 0.9072167277336121, 0.9129869341850281, 0.8890388011932373, 0.9071948528289795, 0.8996744751930237, 0.9057520627975464, 0.9020336866378784, 0.9176295399665833, 0.9128463268280029, 0.9079602956771851, 0.9083122611045837, 0.9102015495300293, 0.917645275592804, 0.9069241881370544, 0.9070026278495789, 0.9042761325836182, 0.9110751748085022, 0.9064171314239502, 0.9116736650466919, 0.910107433795929, 0.9189020395278931, 0.9159107208251953, 0.9197667837142944, 0.925084114074707, 0.9099941253662109, 0.9272832274436951, 0.9223314523696899, 0.914358377456665, 0.9335872530937195, 0.9129086136817932, 0.925469160079956, 0.9241950511932373, 0.9199770092964172, 0.930730402469635, 0.9396921992301941, 0.9225424528121948, 0.9325152039527893, 0.923240602016449, 0.9373800158500671, 0.9223039746284485, 0.9354302287101746, 0.9271844625473022, 0.9386493563652039, 0.9436164498329163, 0.9362093210220337, 0.9271555542945862, 0.9476771354675293, 0.9382961988449097, 0.953923761844635, 0.9473189115524292, 0.9446924924850464, 0.9532137513160706, 0.9401174187660217, 0.945911705493927, 0.9548926949501038, 0.9533094763755798, 0.9520418047904968, 0.9502476453781128, 0.9512414336204529, 0.9645907878875732, 0.9573156237602234, 0.966280996799469, 0.9555266499519348, 0.961409330368042, 0.9596551060676575, 0.9584121108055115, 0.9831281900405884, 0.957414448261261, 0.9606104493141174, 0.9686946868896484, 0.9718104004859924, 0.9678337574005127, 0.9798617362976074, 0.9745780229568481, 0.9814642667770386, 0.9817481637001038, 0.9804709553718567, 0.974109411239624, 0.9814934134483337, 0.9606482982635498, 0.9745978713035583, 1.0015848875045776, 0.9911189079284668, 0.9884557127952576, 0.9876322746276855, 0.9804279208183289, 0.9865077137947083, 0.9807695150375366, 0.9871915578842163, 0.9738768339157104, 0.9819785356521606, 0.9833795428276062, 1.0064154863357544, 0.9899716377258301, 0.9818180799484253, 1.012292742729187, 1.0044984817504883, 1.0030866861343384, 1.0014139413833618, 0.9887896180152893, 1.006359338760376, 1.0108749866485596, 1.0162014961242676, 1.0045866966247559, 0.9983150959014893, 1.0116320848464966, 1.0141777992248535, 1.020658016204834, 1.0172463655471802, 1.0144941806793213, 1.012882947921753, 1.0113558769226074, 1.0098797082901, 1.0155333280563354, 1.0130242109298706, 1.0130255222320557, 1.0191563367843628, 1.0331681966781616, 1.0310182571411133, 1.0166223049163818, 1.0213947296142578, 1.0201236009597778, 1.034130573272705, 1.0221433639526367, 1.0188980102539062, 1.0263776779174805], 'identifier': '6332061np'}