
 20%|███████████████████▌                                                                              | 20/100 [00:01<00:06, 12.16it/s, loss_test=1.082]
Epoch: 00, Training Loss: 0.9999, Test Loss: 1.0795
Epoch: 01, Training Loss: 0.9933, Test Loss: 1.0795
Epoch: 02, Training Loss: 0.9942, Test Loss: 1.0566
Epoch: 03, Training Loss: 0.9884, Test Loss: 1.0807
Epoch: 04, Training Loss: 0.9949, Test Loss: 1.0787
Epoch: 05, Training Loss: 0.9971, Test Loss: 1.0977
Epoch: 06, Training Loss: 0.9938, Test Loss: 1.0704
Epoch: 07, Training Loss: 0.9882, Test Loss: 1.0730
Epoch: 08, Training Loss: 0.9944, Test Loss: 1.0771
Epoch: 09, Training Loss: 0.9931, Test Loss: 1.0815
Epoch: 10, Training Loss: 0.9934, Test Loss: 1.1026
Epoch: 11, Training Loss: 0.9911, Test Loss: 1.0891
Epoch: 12, Training Loss: 0.9844, Test Loss: 1.0872
Epoch: 13, Training Loss: 0.9846, Test Loss: 1.0973
Epoch: 14, Training Loss: 0.9840, Test Loss: 1.0756
Epoch: 15, Training Loss: 0.9841, Test Loss: 1.0832
Epoch: 16, Training Loss: 0.9887, Test Loss: 1.0889
Epoch: 17, Training Loss: 0.9884, Test Loss: 1.0851
Epoch: 18, Training Loss: 0.9815, Test Loss: 1.0803
Epoch: 19, Training Loss: 0.9892, Test Loss: 1.0816
Epoch: 20, Training Loss: 0.9845, Test Loss: 1.0775
Epoch: 21, Training Loss: 0.9837, Test Loss: 1.0805
Epoch: 22, Training Loss: 0.9832, Test Loss: 1.0987
Epoch: 23, Training Loss: 0.9859, Test Loss: 1.0759
Epoch: 24, Training Loss: 0.9850, Test Loss: 1.0659
Epoch: 25, Training Loss: 0.9773, Test Loss: 1.0660
Epoch: 26, Training Loss: 0.9808, Test Loss: 1.0935
Epoch: 27, Training Loss: 0.9848, Test Loss: 1.0801
Epoch: 28, Training Loss: 0.9750, Test Loss: 1.0755
Epoch: 29, Training Loss: 0.9786, Test Loss: 1.0757
Epoch: 30, Training Loss: 0.9739, Test Loss: 1.0628
Epoch: 31, Training Loss: 0.9670, Test Loss: 1.0661
Epoch: 32, Training Loss: 0.9653, Test Loss: 1.0842
Epoch: 33, Training Loss: 0.9607, Test Loss: 1.0599
Epoch: 34, Training Loss: 0.9456, Test Loss: 1.0455
Epoch: 35, Training Loss: 0.9419, Test Loss: 1.0511
Epoch: 36, Training Loss: 0.9356, Test Loss: 1.0463
Epoch: 37, Training Loss: 0.9292, Test Loss: 1.0331
Epoch: 38, Training Loss: 0.9220, Test Loss: 1.0175
Epoch: 39, Training Loss: 0.9137, Test Loss: 1.0090
Epoch: 40, Training Loss: 0.9097, Test Loss: 0.9829
Epoch: 41, Training Loss: 0.8997, Test Loss: 0.9893
Epoch: 42, Training Loss: 0.8993, Test Loss: 0.9819

 44%|███████████████████████████████████████████                                                       | 44/100 [00:03<00:04, 12.15it/s, loss_test=1.004]
Epoch: 44, Training Loss: 0.8944, Test Loss: 0.9970
Epoch: 45, Training Loss: 0.8883, Test Loss: 0.9894
Epoch: 46, Training Loss: 0.8853, Test Loss: 0.9736
Epoch: 47, Training Loss: 0.8864, Test Loss: 0.9764
Epoch: 48, Training Loss: 0.8821, Test Loss: 0.9589
Epoch: 49, Training Loss: 0.8819, Test Loss: 0.9732
Epoch: 50, Training Loss: 0.8780, Test Loss: 0.9829
Epoch: 51, Training Loss: 0.8781, Test Loss: 0.9688
Epoch: 52, Training Loss: 0.8723, Test Loss: 0.9628
Epoch: 53, Training Loss: 0.8692, Test Loss: 0.9692
Epoch: 54, Training Loss: 0.8713, Test Loss: 0.9780
Epoch: 55, Training Loss: 0.8651, Test Loss: 0.9662
Epoch: 56, Training Loss: 0.8652, Test Loss: 0.9697
Epoch: 57, Training Loss: 0.8589, Test Loss: 0.9595
Epoch: 58, Training Loss: 0.8565, Test Loss: 0.9517
Epoch: 59, Training Loss: 0.8506, Test Loss: 0.9564
Epoch: 60, Training Loss: 0.8474, Test Loss: 0.9454
Epoch: 61, Training Loss: 0.8453, Test Loss: 0.9663
Epoch: 62, Training Loss: 0.8326, Test Loss: 0.9360
Epoch: 63, Training Loss: 0.8330, Test Loss: 0.9532
Epoch: 64, Training Loss: 0.8287, Test Loss: 0.9392
Epoch: 65, Training Loss: 0.8255, Test Loss: 0.9624
Epoch: 66, Training Loss: 0.8213, Test Loss: 0.9534
Epoch: 67, Training Loss: 0.8163, Test Loss: 0.9606

 68%|██████████████████████████████████████████████████████████████████▋                               | 68/100 [00:05<00:02, 12.62it/s, loss_test=0.944]
Epoch: 69, Training Loss: 0.8074, Test Loss: 0.9600
Epoch: 70, Training Loss: 0.8016, Test Loss: 0.9328
Epoch: 71, Training Loss: 0.8001, Test Loss: 0.9446
Epoch: 72, Training Loss: 0.7947, Test Loss: 0.9356
Epoch: 73, Training Loss: 0.7894, Test Loss: 0.9334
Epoch: 74, Training Loss: 0.7849, Test Loss: 0.9313
Epoch: 75, Training Loss: 0.7795, Test Loss: 0.9334
Epoch: 76, Training Loss: 0.7744, Test Loss: 0.9252
Epoch: 77, Training Loss: 0.7706, Test Loss: 0.9377
Epoch: 78, Training Loss: 0.7631, Test Loss: 0.9239
Epoch: 79, Training Loss: 0.7621, Test Loss: 0.9084
Epoch: 80, Training Loss: 0.7575, Test Loss: 0.9190
Epoch: 81, Training Loss: 0.7523, Test Loss: 0.9159
Epoch: 82, Training Loss: 0.7488, Test Loss: 0.9102
Epoch: 83, Training Loss: 0.7446, Test Loss: 0.9180
Epoch: 84, Training Loss: 0.7420, Test Loss: 0.9073
Epoch: 85, Training Loss: 0.7332, Test Loss: 0.9162
Epoch: 86, Training Loss: 0.7314, Test Loss: 0.9147
Epoch: 87, Training Loss: 0.7285, Test Loss: 0.9096
Epoch: 88, Training Loss: 0.7250, Test Loss: 0.9041
Epoch: 89, Training Loss: 0.7231, Test Loss: 0.9121
Epoch: 90, Training Loss: 0.7194, Test Loss: 0.9064
Epoch: 91, Training Loss: 0.7127, Test Loss: 0.8940
Epoch: 92, Training Loss: 0.7101, Test Loss: 0.8952


100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.23it/s, loss_test=0.894]
Epoch: 94, Training Loss: 0.7008, Test Loss: 0.8989
Epoch: 95, Training Loss: 0.6977, Test Loss: 0.9051
Epoch: 96, Training Loss: 0.6947, Test Loss: 0.9016
Epoch: 97, Training Loss: 0.6872, Test Loss: 0.9011
Epoch: 98, Training Loss: 0.6851, Test Loss: 0.8842
Epoch: 99, Training Loss: 0.6801, Test Loss: 0.8937
Model saved as model_3243576np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-1220000-3243576np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 100, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9998605489730835, 0.9933170080184937, 0.9942291140556335, 0.9884236454963684, 0.9948694229125976, 0.9971150517463684, 0.9937545537948609, 0.988168203830719, 0.9943910241127014, 0.9930515170097352, 0.9934218525886536, 0.9911019325256347, 0.9844024062156678, 0.984577989578247, 0.9839929223060608, 0.9841107726097107, 0.9886553645133972, 0.9883809328079224, 0.9815035581588745, 0.9892312049865722, 0.9845037698745728, 0.9837469100952149, 0.9831616878509521, 0.9858530282974243, 0.9849813938140869, 0.9772809386253357, 0.9807502508163453, 0.9847681283950805, 0.9750120401382446, 0.9785990953445435, 0.9738689184188842, 0.9670466542243957, 0.9652585864067078, 0.9606987953186035, 0.9455954909324646, 0.9418622493743897, 0.935586953163147, 0.9291739106178284, 0.9220478177070618, 0.9136580228805542, 0.909714686870575, 0.8996774554252625, 0.899322235584259, 0.8979106664657592, 0.894356107711792, 0.888313353061676, 0.885345208644867, 0.8864351391792298, 0.8821054816246032, 0.8818707585334777, 0.8780034065246582, 0.8781116485595704, 0.8722737789154053, 0.8691770076751709, 0.8713223457336425, 0.8650936126708985, 0.8652460455894471, 0.8589393496513367, 0.8564857959747314, 0.8506151437759399, 0.847389566898346, 0.8452639818191529, 0.8325533270835876, 0.8330008506774902, 0.828689181804657, 0.8254713416099548, 0.8213343977928161, 0.816277277469635, 0.8096028685569763, 0.8074023842811584, 0.8016171216964721, 0.8000558257102967, 0.7946632385253907, 0.7893709301948547, 0.7849133253097534, 0.7794668674468994, 0.7743916749954224, 0.7705880045890808, 0.7630775213241577, 0.7620702743530273, 0.7574702501296997, 0.7522992134094239, 0.748814606666565, 0.7446125984191895, 0.7420337557792663, 0.7332347154617309, 0.7313566684722901, 0.7285146236419677, 0.7250132441520691, 0.7231473326683044, 0.7193748235702515, 0.7126751661300659, 0.7101377010345459, 0.70213223695755, 0.7008375883102417, 0.6976671576499939, 0.6947394609451294, 0.687247383594513, 0.6850608229637146, 0.6800807595252991], 'loss_test': [1.079479694366455, 1.0794745683670044, 1.056633710861206, 1.0807266235351562, 1.0787042379379272, 1.0976662635803223, 1.0703608989715576, 1.0730055570602417, 1.077062964439392, 1.0815311670303345, 1.1025631427764893, 1.0891426801681519, 1.087217092514038, 1.0972731113433838, 1.0755828619003296, 1.083182454109192, 1.0889240503311157, 1.0850812196731567, 1.0803463459014893, 1.0815602540969849, 1.0774903297424316, 1.080519437789917, 1.0987213850021362, 1.0759129524230957, 1.0659407377243042, 1.0659774541854858, 1.0935488939285278, 1.0801483392715454, 1.075526237487793, 1.075736165046692, 1.0627514123916626, 1.0660580396652222, 1.0842044353485107, 1.05989670753479, 1.0454832315444946, 1.0511125326156616, 1.0462629795074463, 1.033117651939392, 1.017513632774353, 1.0089718103408813, 0.9828768372535706, 0.9893190264701843, 0.9818520545959473, 1.0043771266937256, 0.9970493316650391, 0.9893937110900879, 0.9736494421958923, 0.9764311909675598, 0.9588764905929565, 0.9732478260993958, 0.9829123020172119, 0.9687591791152954, 0.9628313183784485, 0.9692023396492004, 0.9779565930366516, 0.9662352800369263, 0.9696752429008484, 0.9595499634742737, 0.9516556262969971, 0.9563608169555664, 0.9454266428947449, 0.9663003087043762, 0.9360475540161133, 0.9532409906387329, 0.9392154216766357, 0.9624308347702026, 0.9533904194831848, 0.9605504870414734, 0.9442669749259949, 0.9599538445472717, 0.9328427910804749, 0.9445993304252625, 0.9356092810630798, 0.9334119558334351, 0.9313185214996338, 0.9334182739257812, 0.9251631498336792, 0.9377314448356628, 0.923922061920166, 0.9084339141845703, 0.9190160632133484, 0.9159030914306641, 0.9102488160133362, 0.9180442094802856, 0.9072592854499817, 0.9162126779556274, 0.9147284030914307, 0.9096444845199585, 0.9041271209716797, 0.9121039509773254, 0.9064416289329529, 0.8940262198448181, 0.8952409029006958, 0.896961510181427, 0.8988739252090454, 0.9051255583763123, 0.9015793204307556, 0.9010929465293884, 0.88423752784729, 0.8937316536903381], 'identifier': '3243576np'}