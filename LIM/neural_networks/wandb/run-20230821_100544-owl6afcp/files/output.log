
 20%|███████████████████▌                                                                              | 20/100 [00:01<00:06, 11.74it/s, loss_test=1.091]
Epoch: 00, Training Loss: 0.9926, Test Loss: 1.0744
Epoch: 01, Training Loss: 0.9868, Test Loss: 1.0684
Epoch: 02, Training Loss: 0.9986, Test Loss: 1.0750
Epoch: 03, Training Loss: 0.9856, Test Loss: 1.0870
Epoch: 04, Training Loss: 0.9878, Test Loss: 1.0630
Epoch: 05, Training Loss: 0.9932, Test Loss: 1.0726
Epoch: 06, Training Loss: 0.9903, Test Loss: 1.0761
Epoch: 07, Training Loss: 0.9871, Test Loss: 1.0698
Epoch: 08, Training Loss: 0.9902, Test Loss: 1.0745
Epoch: 09, Training Loss: 0.9896, Test Loss: 1.0746
Epoch: 10, Training Loss: 0.9892, Test Loss: 1.0794
Epoch: 11, Training Loss: 0.9865, Test Loss: 1.0780
Epoch: 12, Training Loss: 0.9875, Test Loss: 1.0708
Epoch: 13, Training Loss: 0.9861, Test Loss: 1.0774
Epoch: 14, Training Loss: 0.9913, Test Loss: 1.0894
Epoch: 15, Training Loss: 0.9866, Test Loss: 1.0795
Epoch: 16, Training Loss: 0.9890, Test Loss: 1.0700
Epoch: 17, Training Loss: 0.9915, Test Loss: 1.0773
Epoch: 18, Training Loss: 0.9861, Test Loss: 1.0851

 44%|███████████████████████████████████████████                                                       | 44/100 [00:03<00:04, 12.12it/s, loss_test=1.011]
Epoch: 20, Training Loss: 0.9880, Test Loss: 1.0845
Epoch: 21, Training Loss: 0.9875, Test Loss: 1.0947
Epoch: 22, Training Loss: 0.9868, Test Loss: 1.0701
Epoch: 23, Training Loss: 0.9872, Test Loss: 1.0934
Epoch: 24, Training Loss: 0.9864, Test Loss: 1.0819
Epoch: 25, Training Loss: 0.9854, Test Loss: 1.0992
Epoch: 26, Training Loss: 0.9814, Test Loss: 1.0772
Epoch: 27, Training Loss: 0.9860, Test Loss: 1.0898
Epoch: 28, Training Loss: 0.9802, Test Loss: 1.0665
Epoch: 29, Training Loss: 0.9818, Test Loss: 1.0781
Epoch: 30, Training Loss: 0.9841, Test Loss: 1.0698
Epoch: 31, Training Loss: 0.9815, Test Loss: 1.0837
Epoch: 32, Training Loss: 0.9773, Test Loss: 1.0550
Epoch: 33, Training Loss: 0.9725, Test Loss: 1.0708
Epoch: 34, Training Loss: 0.9677, Test Loss: 1.0655
Epoch: 35, Training Loss: 0.9646, Test Loss: 1.0912
Epoch: 36, Training Loss: 0.9524, Test Loss: 1.0726
Epoch: 37, Training Loss: 0.9439, Test Loss: 1.0654
Epoch: 38, Training Loss: 0.9330, Test Loss: 1.0350
Epoch: 39, Training Loss: 0.9230, Test Loss: 1.0385
Epoch: 40, Training Loss: 0.9150, Test Loss: 1.0315
Epoch: 41, Training Loss: 0.9057, Test Loss: 1.0376
Epoch: 42, Training Loss: 0.8974, Test Loss: 1.0175

 68%|██████████████████████████████████████████████████████████████████▋                               | 68/100 [00:05<00:02, 12.45it/s, loss_test=0.905]
Epoch: 44, Training Loss: 0.8894, Test Loss: 1.0138
Epoch: 45, Training Loss: 0.8874, Test Loss: 0.9960
Epoch: 46, Training Loss: 0.8828, Test Loss: 0.9866
Epoch: 47, Training Loss: 0.8754, Test Loss: 0.9963
Epoch: 48, Training Loss: 0.8693, Test Loss: 0.9883
Epoch: 49, Training Loss: 0.8649, Test Loss: 0.9924
Epoch: 50, Training Loss: 0.8637, Test Loss: 0.9785
Epoch: 51, Training Loss: 0.8601, Test Loss: 0.9744
Epoch: 52, Training Loss: 0.8543, Test Loss: 0.9712
Epoch: 53, Training Loss: 0.8482, Test Loss: 0.9442
Epoch: 54, Training Loss: 0.8490, Test Loss: 0.9455
Epoch: 55, Training Loss: 0.8434, Test Loss: 0.9556
Epoch: 56, Training Loss: 0.8426, Test Loss: 0.9473
Epoch: 57, Training Loss: 0.8385, Test Loss: 0.9533
Epoch: 58, Training Loss: 0.8315, Test Loss: 0.9511
Epoch: 59, Training Loss: 0.8296, Test Loss: 0.9325
Epoch: 60, Training Loss: 0.8250, Test Loss: 0.9372
Epoch: 61, Training Loss: 0.8241, Test Loss: 0.9279
Epoch: 62, Training Loss: 0.8170, Test Loss: 0.9307
Epoch: 63, Training Loss: 0.8132, Test Loss: 0.9302
Epoch: 64, Training Loss: 0.8132, Test Loss: 0.9224
Epoch: 65, Training Loss: 0.8092, Test Loss: 0.9225
Epoch: 66, Training Loss: 0.8046, Test Loss: 0.9219
Epoch: 67, Training Loss: 0.7980, Test Loss: 0.9287

 92%|██████████████████████████████████████████████████████████████████████████████████████████▏       | 92/100 [00:07<00:00, 12.03it/s, loss_test=0.883]
Epoch: 69, Training Loss: 0.7913, Test Loss: 0.9146
Epoch: 70, Training Loss: 0.7871, Test Loss: 0.9151
Epoch: 71, Training Loss: 0.7852, Test Loss: 0.9051
Epoch: 72, Training Loss: 0.7778, Test Loss: 0.8984
Epoch: 73, Training Loss: 0.7781, Test Loss: 0.9026
Epoch: 74, Training Loss: 0.7707, Test Loss: 0.9025
Epoch: 75, Training Loss: 0.7690, Test Loss: 0.9124
Epoch: 76, Training Loss: 0.7648, Test Loss: 0.9118
Epoch: 77, Training Loss: 0.7604, Test Loss: 0.8920
Epoch: 78, Training Loss: 0.7519, Test Loss: 0.9065
Epoch: 79, Training Loss: 0.7530, Test Loss: 0.9014
Epoch: 80, Training Loss: 0.7446, Test Loss: 0.9144
Epoch: 81, Training Loss: 0.7442, Test Loss: 0.9067
Epoch: 82, Training Loss: 0.7355, Test Loss: 0.8885
Epoch: 83, Training Loss: 0.7324, Test Loss: 0.8911
Epoch: 84, Training Loss: 0.7291, Test Loss: 0.8857
Epoch: 85, Training Loss: 0.7229, Test Loss: 0.9048
Epoch: 86, Training Loss: 0.7221, Test Loss: 0.8822
Epoch: 87, Training Loss: 0.7154, Test Loss: 0.8837
Epoch: 88, Training Loss: 0.7114, Test Loss: 0.8930
Epoch: 89, Training Loss: 0.7044, Test Loss: 0.8861
Epoch: 90, Training Loss: 0.7008, Test Loss: 0.8821
Epoch: 91, Training Loss: 0.6950, Test Loss: 0.8866

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.01it/s, loss_test=0.872]
Epoch: 93, Training Loss: 0.6907, Test Loss: 0.8769
Epoch: 94, Training Loss: 0.6861, Test Loss: 0.8889
Epoch: 95, Training Loss: 0.6826, Test Loss: 0.8907
Epoch: 96, Training Loss: 0.6736, Test Loss: 0.8858
Epoch: 97, Training Loss: 0.6693, Test Loss: 0.8779
Epoch: 98, Training Loss: 0.6684, Test Loss: 0.8772
Epoch: 99, Training Loss: 0.6673, Test Loss: 0.8715
Model saved as model_510349np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-129000-510349np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 100, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9926316380500794, 0.9868300914764404, 0.9986260890960693, 0.9856265544891357, 0.9877590656280517, 0.9932057738304139, 0.9903426885604858, 0.9871341109275817, 0.990228247642517, 0.9896357178688049, 0.9892044544219971, 0.9865241527557373, 0.9874849438667297, 0.9861085414886475, 0.9913146615028381, 0.9866478443145752, 0.9889564156532288, 0.9915431022644043, 0.9860700011253357, 0.9852461099624634, 0.9879912495613098, 0.9874544620513916, 0.9868355393409729, 0.9872283697128296, 0.9864084005355835, 0.9853980660438537, 0.9814207553863525, 0.9859830975532532, 0.9802493095397949, 0.9818351864814758, 0.9841173529624939, 0.9815222382545471, 0.9773174047470092, 0.9724521040916443, 0.9676880478858948, 0.9646117568016053, 0.9524386286735534, 0.9438517451286316, 0.9330197453498841, 0.9229548454284668, 0.9150243759155273, 0.9057348608970642, 0.8974390387535095, 0.8921875715255737, 0.8893994331359864, 0.8874098539352417, 0.8827861905097961, 0.8754052519798279, 0.8692825317382813, 0.8649261355400085, 0.8637089014053345, 0.8600999236106872, 0.8542736172676086, 0.8481749892234802, 0.8489901781082153, 0.8433509826660156, 0.8426013469696045, 0.8384698629379272, 0.8314979791641235, 0.8296213865280151, 0.8250317454338074, 0.8241195678710938, 0.8169954776763916, 0.8132083654403687, 0.8131810188293457, 0.8091816186904908, 0.8045720100402832, 0.798003327846527, 0.795761227607727, 0.7912809729576111, 0.7871003866195678, 0.7852129578590393, 0.7778063535690307, 0.7781066775321961, 0.770664656162262, 0.7689926266670227, 0.7647742033004761, 0.7604263663291931, 0.7518568992614746, 0.7530405998229981, 0.7445980548858643, 0.7442103266716004, 0.7355112314224244, 0.732441782951355, 0.7291005492210388, 0.722937822341919, 0.7220600247383118, 0.7154232621192932, 0.711434829235077, 0.704431664943695, 0.7007500410079956, 0.6950159430503845, 0.6935478210449219, 0.6906883716583252, 0.6861412048339843, 0.6826149702072144, 0.6735715985298156, 0.6693329811096191, 0.6684150576591492, 0.6673142671585083], 'loss_test': [1.0743812322616577, 1.0684428215026855, 1.0749574899673462, 1.0870128870010376, 1.0629503726959229, 1.072554111480713, 1.0761430263519287, 1.0697804689407349, 1.0744601488113403, 1.0745890140533447, 1.0794100761413574, 1.0780456066131592, 1.0708016157150269, 1.0773571729660034, 1.0894213914871216, 1.0795091390609741, 1.0700442790985107, 1.07732355594635, 1.0851304531097412, 1.0911632776260376, 1.0845427513122559, 1.0946979522705078, 1.0700854063034058, 1.0934399366378784, 1.0819324254989624, 1.0992330312728882, 1.077170491218567, 1.0897530317306519, 1.0664713382720947, 1.0780882835388184, 1.0698200464248657, 1.0837374925613403, 1.0550475120544434, 1.0708447694778442, 1.0655006170272827, 1.09120774269104, 1.0725646018981934, 1.0654243230819702, 1.035037636756897, 1.038511037826538, 1.0315362215042114, 1.037563443183899, 1.0175175666809082, 1.010981559753418, 1.0137815475463867, 0.9960130453109741, 0.9865668416023254, 0.9963280558586121, 0.9883027076721191, 0.9923920035362244, 0.9785131216049194, 0.9743710160255432, 0.9711776971817017, 0.944244921207428, 0.9455035328865051, 0.9555709958076477, 0.9473121166229248, 0.9533494710922241, 0.951145589351654, 0.9324904680252075, 0.9371641874313354, 0.9278547763824463, 0.9307412505149841, 0.9302289485931396, 0.922431230545044, 0.9225400686264038, 0.921870768070221, 0.928716242313385, 0.9053902626037598, 0.9145936369895935, 0.9151221513748169, 0.9051336050033569, 0.8984029293060303, 0.9026177525520325, 0.9025458693504333, 0.9124268889427185, 0.9118117690086365, 0.891984224319458, 0.9064836502075195, 0.9013856649398804, 0.9144172072410583, 0.9066933989524841, 0.8885475993156433, 0.8911128044128418, 0.8857396841049194, 0.9047507643699646, 0.8822075724601746, 0.8837414383888245, 0.89301997423172, 0.8860914707183838, 0.8821406364440918, 0.8866440653800964, 0.883415937423706, 0.8768848180770874, 0.8889458775520325, 0.8906679153442383, 0.8858371376991272, 0.8778839111328125, 0.877246618270874, 0.8715271353721619], 'identifier': '510349np'}