
  8%|███████▊                                                                                          | 20/250 [00:01<00:19, 11.89it/s, loss_test=1.080]
Epoch: 00, Training Loss: 0.9929, Test Loss: 1.0837
Epoch: 01, Training Loss: 0.9895, Test Loss: 1.0655
Epoch: 02, Training Loss: 0.9929, Test Loss: 1.0718
Epoch: 03, Training Loss: 0.9909, Test Loss: 1.0692
Epoch: 04, Training Loss: 0.9929, Test Loss: 1.0732
Epoch: 05, Training Loss: 0.9914, Test Loss: 1.0769
Epoch: 06, Training Loss: 0.9920, Test Loss: 1.0738
Epoch: 07, Training Loss: 0.9926, Test Loss: 1.0796
Epoch: 08, Training Loss: 0.9898, Test Loss: 1.0663
Epoch: 09, Training Loss: 0.9888, Test Loss: 1.0877
Epoch: 10, Training Loss: 0.9870, Test Loss: 1.0696
Epoch: 11, Training Loss: 0.9887, Test Loss: 1.0911
Epoch: 12, Training Loss: 0.9856, Test Loss: 1.0870
Epoch: 13, Training Loss: 0.9868, Test Loss: 1.0722
Epoch: 14, Training Loss: 0.9850, Test Loss: 1.0717
Epoch: 15, Training Loss: 0.9897, Test Loss: 1.0986
Epoch: 16, Training Loss: 0.9832, Test Loss: 1.0772
Epoch: 17, Training Loss: 0.9880, Test Loss: 1.0917
Epoch: 18, Training Loss: 0.9862, Test Loss: 1.0675
Epoch: 19, Training Loss: 0.9871, Test Loss: 1.0714

 18%|██████████████████                                                                                | 46/250 [00:03<00:16, 12.25it/s, loss_test=0.988]
Epoch: 21, Training Loss: 0.9832, Test Loss: 1.0673
Epoch: 22, Training Loss: 0.9811, Test Loss: 1.0785
Epoch: 23, Training Loss: 0.9860, Test Loss: 1.0993
Epoch: 24, Training Loss: 0.9774, Test Loss: 1.0943
Epoch: 25, Training Loss: 0.9840, Test Loss: 1.0745
Epoch: 26, Training Loss: 0.9820, Test Loss: 1.0642
Epoch: 27, Training Loss: 0.9790, Test Loss: 1.0839
Epoch: 28, Training Loss: 0.9775, Test Loss: 1.0973
Epoch: 29, Training Loss: 0.9770, Test Loss: 1.0760
Epoch: 30, Training Loss: 0.9757, Test Loss: 1.0787
Epoch: 31, Training Loss: 0.9658, Test Loss: 1.0664
Epoch: 32, Training Loss: 0.9671, Test Loss: 1.0739
Epoch: 33, Training Loss: 0.9641, Test Loss: 1.0614
Epoch: 34, Training Loss: 0.9519, Test Loss: 1.0558
Epoch: 35, Training Loss: 0.9416, Test Loss: 1.0695
Epoch: 36, Training Loss: 0.9325, Test Loss: 1.0388
Epoch: 37, Training Loss: 0.9231, Test Loss: 1.0473
Epoch: 38, Training Loss: 0.9190, Test Loss: 1.0369
Epoch: 39, Training Loss: 0.9109, Test Loss: 1.0205
Epoch: 40, Training Loss: 0.8982, Test Loss: 1.0250
Epoch: 41, Training Loss: 0.8963, Test Loss: 1.0260
Epoch: 42, Training Loss: 0.8925, Test Loss: 1.0089
Epoch: 43, Training Loss: 0.8820, Test Loss: 1.0039
Epoch: 44, Training Loss: 0.8772, Test Loss: 1.0086
Epoch: 45, Training Loss: 0.8720, Test Loss: 0.9883
Epoch: 46, Training Loss: 0.8606, Test Loss: 0.9931
Epoch: 47, Training Loss: 0.8578, Test Loss: 0.9858
Epoch: 48, Training Loss: 0.8538, Test Loss: 0.9842
Epoch: 49, Training Loss: 0.8460, Test Loss: 0.9812
Epoch: 50, Training Loss: 0.8463, Test Loss: 0.9885
Epoch: 51, Training Loss: 0.8391, Test Loss: 0.9730
Epoch: 52, Training Loss: 0.8369, Test Loss: 0.9756
Epoch: 53, Training Loss: 0.8337, Test Loss: 0.9678
Epoch: 54, Training Loss: 0.8261, Test Loss: 0.9664
Epoch: 55, Training Loss: 0.8191, Test Loss: 0.9656
Epoch: 56, Training Loss: 0.8192, Test Loss: 0.9543
Epoch: 57, Training Loss: 0.8172, Test Loss: 0.9409
Epoch: 58, Training Loss: 0.8132, Test Loss: 0.9645
Epoch: 59, Training Loss: 0.8077, Test Loss: 0.9538
Epoch: 60, Training Loss: 0.8065, Test Loss: 0.9519
Epoch: 61, Training Loss: 0.8058, Test Loss: 0.9574
Epoch: 62, Training Loss: 0.8026, Test Loss: 0.9470
Epoch: 63, Training Loss: 0.7988, Test Loss: 0.9373
Epoch: 64, Training Loss: 0.7956, Test Loss: 0.9440
Epoch: 65, Training Loss: 0.7928, Test Loss: 0.9268
Epoch: 66, Training Loss: 0.7906, Test Loss: 0.9229
Epoch: 67, Training Loss: 0.7823, Test Loss: 0.9287
Epoch: 68, Training Loss: 0.7834, Test Loss: 0.9272


 38%|████████████████████████████████████▊                                                             | 94/250 [00:07<00:12, 12.30it/s, loss_test=0.887]
Epoch: 70, Training Loss: 0.7766, Test Loss: 0.9231
Epoch: 71, Training Loss: 0.7758, Test Loss: 0.9341
Epoch: 72, Training Loss: 0.7734, Test Loss: 0.9414
Epoch: 73, Training Loss: 0.7640, Test Loss: 0.9158
Epoch: 74, Training Loss: 0.7680, Test Loss: 0.9208
Epoch: 75, Training Loss: 0.7614, Test Loss: 0.9293
Epoch: 76, Training Loss: 0.7619, Test Loss: 0.8944
Epoch: 77, Training Loss: 0.7579, Test Loss: 0.9112
Epoch: 78, Training Loss: 0.7546, Test Loss: 0.9106
Epoch: 79, Training Loss: 0.7497, Test Loss: 0.9075
Epoch: 80, Training Loss: 0.7471, Test Loss: 0.9071
Epoch: 81, Training Loss: 0.7453, Test Loss: 0.9055
Epoch: 82, Training Loss: 0.7456, Test Loss: 0.8990
Epoch: 83, Training Loss: 0.7407, Test Loss: 0.8958
Epoch: 84, Training Loss: 0.7379, Test Loss: 0.9007
Epoch: 85, Training Loss: 0.7363, Test Loss: 0.9041
Epoch: 86, Training Loss: 0.7329, Test Loss: 0.8925
Epoch: 87, Training Loss: 0.7263, Test Loss: 0.9017
Epoch: 88, Training Loss: 0.7283, Test Loss: 0.9065
Epoch: 89, Training Loss: 0.7232, Test Loss: 0.8932
Epoch: 90, Training Loss: 0.7219, Test Loss: 0.9000
Epoch: 91, Training Loss: 0.7182, Test Loss: 0.8984
Epoch: 92, Training Loss: 0.7168, Test Loss: 0.8804
Epoch: 93, Training Loss: 0.7132, Test Loss: 0.8850

 48%|██████████████████████████████████████████████▌                                                  | 120/250 [00:09<00:10, 12.22it/s, loss_test=0.892]
Epoch: 95, Training Loss: 0.7092, Test Loss: 0.8840
Epoch: 96, Training Loss: 0.7049, Test Loss: 0.8839
Epoch: 97, Training Loss: 0.7029, Test Loss: 0.8896
Epoch: 98, Training Loss: 0.6983, Test Loss: 0.8890
Epoch: 99, Training Loss: 0.6993, Test Loss: 0.8771
Epoch: 100, Training Loss: 0.6974, Test Loss: 0.8904
Epoch: 101, Training Loss: 0.6918, Test Loss: 0.8912
Epoch: 102, Training Loss: 0.6878, Test Loss: 0.8853
Epoch: 103, Training Loss: 0.6893, Test Loss: 0.8804
Epoch: 104, Training Loss: 0.6850, Test Loss: 0.8889
Epoch: 105, Training Loss: 0.6803, Test Loss: 0.8862
Epoch: 106, Training Loss: 0.6782, Test Loss: 0.8794
Epoch: 107, Training Loss: 0.6778, Test Loss: 0.8798
Epoch: 108, Training Loss: 0.6696, Test Loss: 0.8705
Epoch: 109, Training Loss: 0.6705, Test Loss: 0.8757
Epoch: 110, Training Loss: 0.6686, Test Loss: 0.8810
Epoch: 111, Training Loss: 0.6638, Test Loss: 0.8790
Epoch: 112, Training Loss: 0.6627, Test Loss: 0.8887
Epoch: 113, Training Loss: 0.6608, Test Loss: 0.8846
Epoch: 114, Training Loss: 0.6551, Test Loss: 0.8899
Epoch: 115, Training Loss: 0.6548, Test Loss: 0.8843
Epoch: 116, Training Loss: 0.6515, Test Loss: 0.8831
Epoch: 117, Training Loss: 0.6485, Test Loss: 0.8864

 58%|███████████████████████████████████████████████████████▊                                         | 144/250 [00:11<00:08, 12.13it/s, loss_test=0.906]
Epoch: 119, Training Loss: 0.6436, Test Loss: 0.8923
Epoch: 120, Training Loss: 0.6402, Test Loss: 0.8858
Epoch: 121, Training Loss: 0.6394, Test Loss: 0.8879
Epoch: 122, Training Loss: 0.6340, Test Loss: 0.8862
Epoch: 123, Training Loss: 0.6328, Test Loss: 0.8752
Epoch: 124, Training Loss: 0.6286, Test Loss: 0.8828
Epoch: 125, Training Loss: 0.6234, Test Loss: 0.8845
Epoch: 126, Training Loss: 0.6198, Test Loss: 0.8732
Epoch: 127, Training Loss: 0.6198, Test Loss: 0.8838
Epoch: 128, Training Loss: 0.6158, Test Loss: 0.8904
Epoch: 129, Training Loss: 0.6124, Test Loss: 0.8907
Epoch: 130, Training Loss: 0.6104, Test Loss: 0.8878
Epoch: 131, Training Loss: 0.6072, Test Loss: 0.8835
Epoch: 132, Training Loss: 0.6031, Test Loss: 0.8884
Epoch: 133, Training Loss: 0.6006, Test Loss: 0.8941
Epoch: 134, Training Loss: 0.5998, Test Loss: 0.8862
Epoch: 135, Training Loss: 0.5964, Test Loss: 0.8875
Epoch: 136, Training Loss: 0.5912, Test Loss: 0.8906
Epoch: 137, Training Loss: 0.5907, Test Loss: 0.8986
Epoch: 138, Training Loss: 0.5865, Test Loss: 0.8898
Epoch: 139, Training Loss: 0.5794, Test Loss: 0.8835
Epoch: 140, Training Loss: 0.5807, Test Loss: 0.8998
Epoch: 141, Training Loss: 0.5750, Test Loss: 0.8943

 67%|█████████████████████████████████████████████████████████████████▏                               | 168/250 [00:13<00:06, 12.00it/s, loss_test=0.933]
Epoch: 143, Training Loss: 0.5709, Test Loss: 0.9061
Epoch: 144, Training Loss: 0.5698, Test Loss: 0.9082
Epoch: 145, Training Loss: 0.5644, Test Loss: 0.8993
Epoch: 146, Training Loss: 0.5648, Test Loss: 0.9057
Epoch: 147, Training Loss: 0.5618, Test Loss: 0.9049
Epoch: 148, Training Loss: 0.5566, Test Loss: 0.9105
Epoch: 149, Training Loss: 0.5549, Test Loss: 0.9086
Epoch: 150, Training Loss: 0.5525, Test Loss: 0.9096
Epoch: 151, Training Loss: 0.5464, Test Loss: 0.9067
Epoch: 152, Training Loss: 0.5439, Test Loss: 0.9069
Epoch: 153, Training Loss: 0.5435, Test Loss: 0.9070
Epoch: 154, Training Loss: 0.5400, Test Loss: 0.8957
Epoch: 155, Training Loss: 0.5361, Test Loss: 0.9160
Epoch: 156, Training Loss: 0.5338, Test Loss: 0.9193
Epoch: 157, Training Loss: 0.5336, Test Loss: 0.9120
Epoch: 158, Training Loss: 0.5294, Test Loss: 0.9047
Epoch: 159, Training Loss: 0.5288, Test Loss: 0.9208
Epoch: 160, Training Loss: 0.5241, Test Loss: 0.9232
Epoch: 161, Training Loss: 0.5204, Test Loss: 0.9081
Epoch: 162, Training Loss: 0.5207, Test Loss: 0.9164
Epoch: 163, Training Loss: 0.5144, Test Loss: 0.9093
Epoch: 164, Training Loss: 0.5136, Test Loss: 0.9284
Epoch: 165, Training Loss: 0.5105, Test Loss: 0.9200
Epoch: 166, Training Loss: 0.5095, Test Loss: 0.9251

 77%|██████████████████████████████████████████████████████████████████████████▍                      | 192/250 [00:15<00:04, 12.46it/s, loss_test=0.963]
Epoch: 168, Training Loss: 0.5059, Test Loss: 0.9326
Epoch: 169, Training Loss: 0.4993, Test Loss: 0.9365
Epoch: 170, Training Loss: 0.4977, Test Loss: 0.9229
Epoch: 171, Training Loss: 0.4967, Test Loss: 0.9378
Epoch: 172, Training Loss: 0.4917, Test Loss: 0.9352
Epoch: 173, Training Loss: 0.4900, Test Loss: 0.9370
Epoch: 174, Training Loss: 0.4881, Test Loss: 0.9335
Epoch: 175, Training Loss: 0.4850, Test Loss: 0.9309
Epoch: 176, Training Loss: 0.4829, Test Loss: 0.9391
Epoch: 177, Training Loss: 0.4797, Test Loss: 0.9360
Epoch: 178, Training Loss: 0.4773, Test Loss: 0.9432
Epoch: 179, Training Loss: 0.4750, Test Loss: 0.9328
Epoch: 180, Training Loss: 0.4741, Test Loss: 0.9394
Epoch: 181, Training Loss: 0.4718, Test Loss: 0.9324
Epoch: 182, Training Loss: 0.4695, Test Loss: 0.9401
Epoch: 183, Training Loss: 0.4667, Test Loss: 0.9337
Epoch: 184, Training Loss: 0.4646, Test Loss: 0.9301
Epoch: 185, Training Loss: 0.4628, Test Loss: 0.9552
Epoch: 186, Training Loss: 0.4590, Test Loss: 0.9519
Epoch: 187, Training Loss: 0.4567, Test Loss: 0.9478
Epoch: 188, Training Loss: 0.4546, Test Loss: 0.9487
Epoch: 189, Training Loss: 0.4515, Test Loss: 0.9408
Epoch: 190, Training Loss: 0.4507, Test Loss: 0.9577

 87%|████████████████████████████████████████████████████████████████████████████████████▌            | 218/250 [00:17<00:02, 12.15it/s, loss_test=0.980]
Epoch: 192, Training Loss: 0.4450, Test Loss: 0.9628
Epoch: 193, Training Loss: 0.4453, Test Loss: 0.9711
Epoch: 194, Training Loss: 0.4419, Test Loss: 0.9566
Epoch: 195, Training Loss: 0.4381, Test Loss: 0.9585
Epoch: 196, Training Loss: 0.4380, Test Loss: 0.9537
Epoch: 197, Training Loss: 0.4342, Test Loss: 0.9672
Epoch: 198, Training Loss: 0.4321, Test Loss: 0.9563
Epoch: 199, Training Loss: 0.4314, Test Loss: 0.9590
Epoch: 200, Training Loss: 0.4284, Test Loss: 0.9542
Epoch: 201, Training Loss: 0.4270, Test Loss: 0.9501
Epoch: 202, Training Loss: 0.4243, Test Loss: 0.9558
Epoch: 203, Training Loss: 0.4203, Test Loss: 0.9603
Epoch: 204, Training Loss: 0.4216, Test Loss: 0.9645
Epoch: 205, Training Loss: 0.4184, Test Loss: 0.9806
Epoch: 206, Training Loss: 0.4166, Test Loss: 0.9713
Epoch: 207, Training Loss: 0.4117, Test Loss: 0.9582
Epoch: 208, Training Loss: 0.4132, Test Loss: 0.9545
Epoch: 209, Training Loss: 0.4102, Test Loss: 0.9680
Epoch: 210, Training Loss: 0.4080, Test Loss: 0.9610
Epoch: 211, Training Loss: 0.4050, Test Loss: 0.9718
Epoch: 212, Training Loss: 0.4026, Test Loss: 0.9733
Epoch: 213, Training Loss: 0.4023, Test Loss: 0.9758
Epoch: 214, Training Loss: 0.3992, Test Loss: 0.9812
Epoch: 215, Training Loss: 0.3983, Test Loss: 0.9789

 97%|█████████████████████████████████████████████████████████████████████████████████████████████▉   | 242/250 [00:19<00:00, 11.80it/s, loss_test=0.998]
Epoch: 217, Training Loss: 0.3931, Test Loss: 0.9796
Epoch: 218, Training Loss: 0.3912, Test Loss: 0.9910
Epoch: 219, Training Loss: 0.3898, Test Loss: 0.9743
Epoch: 220, Training Loss: 0.3876, Test Loss: 0.9752
Epoch: 221, Training Loss: 0.3874, Test Loss: 0.9741
Epoch: 222, Training Loss: 0.3835, Test Loss: 0.9712
Epoch: 223, Training Loss: 0.3849, Test Loss: 0.9637
Epoch: 224, Training Loss: 0.3810, Test Loss: 0.9843
Epoch: 225, Training Loss: 0.3788, Test Loss: 0.9704
Epoch: 226, Training Loss: 0.3767, Test Loss: 0.9821
Epoch: 227, Training Loss: 0.3758, Test Loss: 0.9858
Epoch: 228, Training Loss: 0.3719, Test Loss: 0.9906
Epoch: 229, Training Loss: 0.3725, Test Loss: 1.0028
Epoch: 230, Training Loss: 0.3690, Test Loss: 0.9924
Epoch: 231, Training Loss: 0.3677, Test Loss: 0.9811
Epoch: 232, Training Loss: 0.3663, Test Loss: 0.9925
Epoch: 233, Training Loss: 0.3658, Test Loss: 0.9812
Epoch: 234, Training Loss: 0.3625, Test Loss: 0.9941
Epoch: 235, Training Loss: 0.3622, Test Loss: 0.9919
Epoch: 236, Training Loss: 0.3610, Test Loss: 0.9879
Epoch: 237, Training Loss: 0.3590, Test Loss: 0.9964
Epoch: 238, Training Loss: 0.3557, Test Loss: 0.9933
Epoch: 239, Training Loss: 0.3540, Test Loss: 0.9847

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.17it/s, loss_test=0.996]
Epoch: 241, Training Loss: 0.3515, Test Loss: 0.9978
Epoch: 242, Training Loss: 0.3507, Test Loss: 1.0041
Epoch: 243, Training Loss: 0.3482, Test Loss: 0.9852
Epoch: 244, Training Loss: 0.3469, Test Loss: 1.0000
Epoch: 245, Training Loss: 0.3446, Test Loss: 1.0039
Epoch: 246, Training Loss: 0.3444, Test Loss: 0.9906
Epoch: 247, Training Loss: 0.3417, Test Loss: 1.0010
Epoch: 248, Training Loss: 0.3393, Test Loss: 0.9978
Epoch: 249, Training Loss: 0.3382, Test Loss: 0.9955
Model saved as model_4640717np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-12170000-4640717np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9928961515426635, 0.9894973993301391, 0.9928530216217041, 0.9909494161605835, 0.9929082870483399, 0.9914478540420533, 0.9920445680618286, 0.9925510048866272, 0.9897504210472107, 0.9888270974159241, 0.9869696259498596, 0.9887409567832947, 0.9856418371200562, 0.9867901802062988, 0.9850324869155884, 0.9897062420845032, 0.9831668138504028, 0.9879652976989746, 0.9862326860427857, 0.9871019840240478, 0.9821163177490234, 0.9832140684127808, 0.9811439871788025, 0.9860429644584656, 0.9774107217788697, 0.9839961051940918, 0.9819660067558289, 0.9790411591529846, 0.9775464773178101, 0.9769680142402649, 0.9756962776184082, 0.965817677974701, 0.9670583724975585, 0.9640569925308228, 0.9518731713294983, 0.9416110754013062, 0.932499098777771, 0.9230597257614136, 0.919024658203125, 0.9108502626419067, 0.898179805278778, 0.8962599277496338, 0.8925259351730347, 0.8819965481758117, 0.877212643623352, 0.8719684839248657, 0.8605809569358825, 0.8577989935874939, 0.8538020730018616, 0.845963978767395, 0.8463001728057862, 0.8390949249267579, 0.8368970274925231, 0.8337240219116211, 0.8261313080787659, 0.8190880060195923, 0.819173538684845, 0.8172413825988769, 0.813202965259552, 0.8076549053192139, 0.8064903974533081, 0.805776035785675, 0.8025924682617187, 0.7988145589828491, 0.7955944061279296, 0.7928293824195862, 0.7905760765075683, 0.7823393821716309, 0.7833793878555297, 0.7806613445281982, 0.7765673637390137, 0.7758318662643433, 0.7734133839607239, 0.7639743089675903, 0.7680012106895446, 0.7614471077919006, 0.7619293332099915, 0.7579016208648681, 0.7545515418052673, 0.7497395753860474, 0.747126841545105, 0.7453093767166138, 0.7456169605255127, 0.7406845450401306, 0.7378807306289673, 0.7363492488861084, 0.7329074621200562, 0.7262870311737061, 0.7283261299133301, 0.7231506466865539, 0.7219069719314575, 0.7182141900062561, 0.716817045211792, 0.7132445454597474, 0.7080489873886109, 0.7091555714607238, 0.704935097694397, 0.7029482364654541, 0.698347008228302, 0.6993021249771119, 0.6974132418632507, 0.6918406248092651, 0.6877611637115478, 0.689272654056549, 0.6849643230438233, 0.6803337097167969, 0.6782132506370544, 0.67783522605896, 0.6695682525634765, 0.6705231308937073, 0.6686287164688111, 0.6637647271156311, 0.6627139091491699, 0.6607887744903564, 0.6550769925117492, 0.6548051476478577, 0.6514584064483643, 0.6485190033912659, 0.6461209058761597, 0.6435622930526733, 0.6401849508285522, 0.6393907546997071, 0.6339962363243103, 0.6328472971916199, 0.628563392162323, 0.6234078049659729, 0.6198222756385803, 0.6197657108306884, 0.615820837020874, 0.6124060392379761, 0.6104162693023681, 0.6072498202323914, 0.6031029701232911, 0.6006029605865478, 0.5997663855552673, 0.5964414954185486, 0.5911899924278259, 0.5906633853912353, 0.5864925980567932, 0.5793726563453674, 0.5806538581848144, 0.5749537348747253, 0.5714047312736511, 0.5708670616149902, 0.5698270201683044, 0.564374303817749, 0.5647629857063293, 0.561778461933136, 0.5565561413764953, 0.5549055337905884, 0.5524620175361633, 0.5463705062866211, 0.5439444661140442, 0.5434929847717285, 0.539964497089386, 0.5360831260681153, 0.5337534666061401, 0.5335500717163086, 0.5294484734535218, 0.5288498520851135, 0.5240608334541321, 0.5203811168670655, 0.5207186937332153, 0.5144113779067994, 0.5136382937431335, 0.5104686617851257, 0.5095115542411804, 0.5043000876903534, 0.5058868587017059, 0.49930709004402163, 0.4976733088493347, 0.4967358112335205, 0.4916934370994568, 0.4900360405445099, 0.48810372948646547, 0.4850490093231201, 0.4828918755054474, 0.47967838048934935, 0.4773250937461853, 0.4749639928340912, 0.47406927943229676, 0.47183342576026915, 0.4695201635360718, 0.46666539311408994, 0.46463328003883364, 0.4628239989280701, 0.4589525580406189, 0.4567456662654877, 0.45455932021141054, 0.4514622211456299, 0.45074147582054136, 0.45024293661117554, 0.44503198862075805, 0.4453398406505585, 0.44193345308303833, 0.438105982542038, 0.4380032539367676, 0.4342077076435089, 0.432107150554657, 0.43141157031059263, 0.42841513752937316, 0.42701342701911926, 0.4242996335029602, 0.42026437520980836, 0.4216072201728821, 0.41840503811836244, 0.4165661156177521, 0.4117043733596802, 0.4132101655006409, 0.41021092534065245, 0.4080281913280487, 0.40495465993881224, 0.40262898802757263, 0.40234093070030214, 0.39916170239448545, 0.3982896327972412, 0.3954227209091187, 0.3931379556655884, 0.39123417139053346, 0.38979387283325195, 0.3875993609428406, 0.38737747073173523, 0.3834874749183655, 0.3848893344402313, 0.3809809505939484, 0.3787710964679718, 0.3767448365688324, 0.3758283853530884, 0.371928870677948, 0.37250953912734985, 0.3689960718154907, 0.3677253067493439, 0.3662944436073303, 0.3658392369747162, 0.36254783272743224, 0.3621691644191742, 0.360967630147934, 0.3589920222759247, 0.3556522727012634, 0.3539550960063934, 0.3539647102355957, 0.3514633119106293, 0.35071932077407836, 0.3482462167739868, 0.34690492749214175, 0.34457569718360903, 0.344427216053009, 0.34166776537895205, 0.3392652690410614, 0.3382121562957764], 'loss_test': [1.0837284326553345, 1.0655295848846436, 1.0718289613723755, 1.0691841840744019, 1.0731984376907349, 1.0768712759017944, 1.0737557411193848, 1.079556941986084, 1.0662903785705566, 1.0876966714859009, 1.069644808769226, 1.0911321640014648, 1.0870393514633179, 1.0722332000732422, 1.0717198848724365, 1.0986109972000122, 1.0771712064743042, 1.0916659832000732, 1.0674878358840942, 1.0714472532272339, 1.0798996686935425, 1.0673025846481323, 1.0784810781478882, 1.0993114709854126, 1.0942732095718384, 1.074487328529358, 1.0641989707946777, 1.0838568210601807, 1.0972950458526611, 1.0760066509246826, 1.078658938407898, 1.06643545627594, 1.0739269256591797, 1.0614017248153687, 1.0557948350906372, 1.0695170164108276, 1.038763165473938, 1.0473376512527466, 1.0368989706039429, 1.0204737186431885, 1.0249828100204468, 1.0259679555892944, 1.0089457035064697, 1.0038830041885376, 1.0086437463760376, 0.9883217811584473, 0.993105411529541, 0.9857845902442932, 0.9841857552528381, 0.98117595911026, 0.9884801506996155, 0.9730095267295837, 0.9756090044975281, 0.9678199291229248, 0.9664120674133301, 0.9655980467796326, 0.9543450474739075, 0.9408773183822632, 0.9645146727561951, 0.953829824924469, 0.9518875479698181, 0.9573895335197449, 0.9469824433326721, 0.9372504353523254, 0.9440421462059021, 0.9267519116401672, 0.9228722453117371, 0.9287440180778503, 0.9271796345710754, 0.9425076246261597, 0.923124372959137, 0.9340556263923645, 0.9413519501686096, 0.9158214926719666, 0.9208353161811829, 0.9293477535247803, 0.8943735957145691, 0.9112036824226379, 0.9106436967849731, 0.9074656963348389, 0.9071155190467834, 0.905523955821991, 0.8989627361297607, 0.8957781791687012, 0.9007195830345154, 0.9040561318397522, 0.8925373554229736, 0.9017080664634705, 0.9064857959747314, 0.8932349681854248, 0.8999610543251038, 0.8983515501022339, 0.880357563495636, 0.8850178122520447, 0.8873358964920044, 0.8840232491493225, 0.8839184641838074, 0.8895584940910339, 0.8890241980552673, 0.8771494030952454, 0.8904336094856262, 0.8911507725715637, 0.8852542638778687, 0.8803799152374268, 0.8889148235321045, 0.8861528635025024, 0.8794457316398621, 0.8797618746757507, 0.8705064654350281, 0.8757056593894958, 0.8810257315635681, 0.8789571523666382, 0.88870769739151, 0.8846234679222107, 0.8898566365242004, 0.8842945694923401, 0.8830748200416565, 0.8864151239395142, 0.8801848888397217, 0.8922717571258545, 0.8857572078704834, 0.8879209756851196, 0.8862367868423462, 0.8751577734947205, 0.8828279376029968, 0.8845046162605286, 0.8731623291969299, 0.8837826251983643, 0.8903547525405884, 0.8906925916671753, 0.8877795934677124, 0.8834527134895325, 0.8883567452430725, 0.8940727710723877, 0.8862473368644714, 0.8875381946563721, 0.8905521035194397, 0.8985611200332642, 0.8898012042045593, 0.883472740650177, 0.8997609615325928, 0.8942564725875854, 0.9006178975105286, 0.9060869216918945, 0.9082251787185669, 0.8992677927017212, 0.9056849479675293, 0.9048894643783569, 0.9104953408241272, 0.9086269736289978, 0.9095674753189087, 0.9066791534423828, 0.9068984985351562, 0.9069883823394775, 0.8957356214523315, 0.9159774780273438, 0.9192776083946228, 0.9120221734046936, 0.9046761393547058, 0.9208306670188904, 0.9231561422348022, 0.908095121383667, 0.9164345860481262, 0.9093090891838074, 0.9284105896949768, 0.9199565649032593, 0.9250605702400208, 0.9290849566459656, 0.9325597286224365, 0.9364856481552124, 0.9229031205177307, 0.9377782344818115, 0.9352421164512634, 0.9369798898696899, 0.9335120320320129, 0.9308570027351379, 0.9391011595726013, 0.9360031485557556, 0.9432309865951538, 0.9328113198280334, 0.9393988847732544, 0.9323768019676208, 0.9401106238365173, 0.9337177872657776, 0.9300914406776428, 0.9552129507064819, 0.9518606066703796, 0.9477863311767578, 0.9487035274505615, 0.9407804012298584, 0.9577114582061768, 0.9532111287117004, 0.9627707004547119, 0.9711470007896423, 0.9566239714622498, 0.9585238099098206, 0.9536687731742859, 0.9672056436538696, 0.9562875032424927, 0.9590063691139221, 0.9541980624198914, 0.9501270651817322, 0.9558175206184387, 0.9603070020675659, 0.9644901156425476, 0.9805880188941956, 0.9712621569633484, 0.9582085013389587, 0.9545139074325562, 0.9679915904998779, 0.9610185623168945, 0.9717809557914734, 0.973342776298523, 0.9757721424102783, 0.9811661243438721, 0.9789125323295593, 0.99489426612854, 0.9796256422996521, 0.9909669756889343, 0.9742559194564819, 0.975222647190094, 0.9741197824478149, 0.9712010025978088, 0.9636884927749634, 0.9843317866325378, 0.9704408645629883, 0.9821416735649109, 0.9858022332191467, 0.990565836429596, 1.0027787685394287, 0.9923524260520935, 0.9810552000999451, 0.9925035238265991, 0.9812381267547607, 0.9941494464874268, 0.9918567538261414, 0.9878937005996704, 0.9963610768318176, 0.9933304190635681, 0.984697163105011, 0.9813923835754395, 0.9977515339851379, 1.0040793418884277, 0.9852218627929688, 0.9999691843986511, 1.0038567781448364, 0.9906440377235413, 1.0009597539901733, 0.9978032112121582, 0.9955129027366638], 'identifier': '4640717np'}