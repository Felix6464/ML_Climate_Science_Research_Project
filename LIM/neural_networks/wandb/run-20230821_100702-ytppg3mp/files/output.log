Epoch: 00, Training Loss: 0.9978, Test Loss: 1.0702
Epoch: 01, Training Loss: 0.9950, Test Loss: 1.0823
Epoch: 02, Training Loss: 0.9895, Test Loss: 1.0861
Epoch: 03, Training Loss: 0.9883, Test Loss: 1.0851
Epoch: 04, Training Loss: 0.9928, Test Loss: 1.0654
Epoch: 05, Training Loss: 0.9950, Test Loss: 1.0848
Epoch: 06, Training Loss: 0.9934, Test Loss: 1.0776
Epoch: 07, Training Loss: 0.9920, Test Loss: 1.0903
Epoch: 08, Training Loss: 0.9897, Test Loss: 1.0866
Epoch: 09, Training Loss: 0.9968, Test Loss: 1.0753
Epoch: 10, Training Loss: 0.9880, Test Loss: 1.0935
Epoch: 11, Training Loss: 0.9896, Test Loss: 1.0894
Epoch: 12, Training Loss: 0.9863, Test Loss: 1.0800
Epoch: 13, Training Loss: 0.9874, Test Loss: 1.0728
Epoch: 14, Training Loss: 0.9862, Test Loss: 1.0756
Epoch: 15, Training Loss: 0.9890, Test Loss: 1.0692
Epoch: 16, Training Loss: 0.9835, Test Loss: 1.0729
Epoch: 17, Training Loss: 0.9885, Test Loss: 1.0637
Epoch: 18, Training Loss: 0.9905, Test Loss: 1.0820
Epoch: 19, Training Loss: 0.9860, Test Loss: 1.0853

 45%|████████████████████████████████████████████                                                      | 45/100 [00:03<00:04, 11.94it/s, loss_test=0.979]
Epoch: 20, Training Loss: 0.9860, Test Loss: 1.0855
Epoch: 21, Training Loss: 0.9810, Test Loss: 1.0721
Epoch: 22, Training Loss: 0.9836, Test Loss: 1.0932
Epoch: 23, Training Loss: 0.9846, Test Loss: 1.0917
Epoch: 24, Training Loss: 0.9805, Test Loss: 1.0929
Epoch: 25, Training Loss: 0.9846, Test Loss: 1.0906
Epoch: 26, Training Loss: 0.9801, Test Loss: 1.0725
Epoch: 27, Training Loss: 0.9770, Test Loss: 1.0886
Epoch: 28, Training Loss: 0.9815, Test Loss: 1.0754
Epoch: 29, Training Loss: 0.9774, Test Loss: 1.0719
Epoch: 30, Training Loss: 0.9719, Test Loss: 1.0749
Epoch: 31, Training Loss: 0.9719, Test Loss: 1.0641
Epoch: 32, Training Loss: 0.9652, Test Loss: 1.0567
Epoch: 33, Training Loss: 0.9610, Test Loss: 1.0723
Epoch: 34, Training Loss: 0.9496, Test Loss: 1.0605
Epoch: 35, Training Loss: 0.9482, Test Loss: 1.0336
Epoch: 36, Training Loss: 0.9393, Test Loss: 1.0408
Epoch: 37, Training Loss: 0.9348, Test Loss: 1.0292
Epoch: 38, Training Loss: 0.9241, Test Loss: 1.0125
Epoch: 39, Training Loss: 0.9176, Test Loss: 1.0249
Epoch: 40, Training Loss: 0.9133, Test Loss: 1.0050
Epoch: 41, Training Loss: 0.9055, Test Loss: 1.0054
Epoch: 42, Training Loss: 0.8920, Test Loss: 0.9936
Epoch: 43, Training Loss: 0.8971, Test Loss: 0.9883
Epoch: 44, Training Loss: 0.8896, Test Loss: 0.9785

 69%|███████████████████████████████████████████████████████████████████▌                              | 69/100 [00:05<00:02, 11.48it/s, loss_test=0.941]
Epoch: 46, Training Loss: 0.8790, Test Loss: 0.9568
Epoch: 47, Training Loss: 0.8731, Test Loss: 0.9626
Epoch: 48, Training Loss: 0.8619, Test Loss: 0.9777
Epoch: 49, Training Loss: 0.8612, Test Loss: 0.9632
Epoch: 50, Training Loss: 0.8575, Test Loss: 0.9644
Epoch: 51, Training Loss: 0.8557, Test Loss: 0.9556
Epoch: 52, Training Loss: 0.8443, Test Loss: 0.9577
Epoch: 53, Training Loss: 0.8422, Test Loss: 0.9579
Epoch: 54, Training Loss: 0.8435, Test Loss: 0.9440
Epoch: 55, Training Loss: 0.8357, Test Loss: 0.9575
Epoch: 56, Training Loss: 0.8329, Test Loss: 0.9560
Epoch: 57, Training Loss: 0.8318, Test Loss: 0.9461
Epoch: 58, Training Loss: 0.8260, Test Loss: 0.9402
Epoch: 59, Training Loss: 0.8247, Test Loss: 0.9456
Epoch: 60, Training Loss: 0.8180, Test Loss: 0.9523
Epoch: 61, Training Loss: 0.8201, Test Loss: 0.9215
Epoch: 62, Training Loss: 0.8143, Test Loss: 0.9303
Epoch: 63, Training Loss: 0.8098, Test Loss: 0.9350
Epoch: 64, Training Loss: 0.8100, Test Loss: 0.9328
Epoch: 65, Training Loss: 0.8038, Test Loss: 0.9275
Epoch: 66, Training Loss: 0.8040, Test Loss: 0.9352
Epoch: 67, Training Loss: 0.7983, Test Loss: 0.9320
Epoch: 68, Training Loss: 0.7950, Test Loss: 0.9409
Epoch: 69, Training Loss: 0.7883, Test Loss: 0.9306
Epoch: 70, Training Loss: 0.7897, Test Loss: 0.9132
Epoch: 71, Training Loss: 0.7873, Test Loss: 0.9213
Epoch: 72, Training Loss: 0.7861, Test Loss: 0.9190
Epoch: 73, Training Loss: 0.7800, Test Loss: 0.9300
Epoch: 74, Training Loss: 0.7773, Test Loss: 0.9355
Epoch: 75, Training Loss: 0.7768, Test Loss: 0.9244
Epoch: 76, Training Loss: 0.7744, Test Loss: 0.9312
Epoch: 77, Training Loss: 0.7665, Test Loss: 0.9219
Epoch: 78, Training Loss: 0.7665, Test Loss: 0.9299
Epoch: 79, Training Loss: 0.7591, Test Loss: 0.9122
Epoch: 80, Training Loss: 0.7574, Test Loss: 0.9246
Epoch: 81, Training Loss: 0.7555, Test Loss: 0.9214
Epoch: 82, Training Loss: 0.7524, Test Loss: 0.9170
Epoch: 83, Training Loss: 0.7515, Test Loss: 0.9215
Epoch: 84, Training Loss: 0.7454, Test Loss: 0.9123
Epoch: 85, Training Loss: 0.7431, Test Loss: 0.9126
Epoch: 86, Training Loss: 0.7392, Test Loss: 0.9202
Epoch: 87, Training Loss: 0.7343, Test Loss: 0.9311
Epoch: 88, Training Loss: 0.7283, Test Loss: 0.9239
Epoch: 89, Training Loss: 0.7276, Test Loss: 0.9214
Epoch: 90, Training Loss: 0.7233, Test Loss: 0.9217
Epoch: 91, Training Loss: 0.7186, Test Loss: 0.9209
Epoch: 92, Training Loss: 0.7137, Test Loss: 0.9077


100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 11.95it/s, loss_test=0.916]
Epoch: 94, Training Loss: 0.7087, Test Loss: 0.9238
Epoch: 95, Training Loss: 0.7004, Test Loss: 0.9078
Epoch: 96, Training Loss: 0.6942, Test Loss: 0.9180
Epoch: 97, Training Loss: 0.6940, Test Loss: 0.9224
Epoch: 98, Training Loss: 0.6914, Test Loss: 0.9123
Epoch: 99, Training Loss: 0.6859, Test Loss: 0.9159
Model saved as model_1744130np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-1240000-1744130np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 100, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9977724552154541, 0.995003879070282, 0.9894953370094299, 0.9883325219154357, 0.9928231954574585, 0.9949820756912231, 0.9933579444885254, 0.9920341372489929, 0.9897137761116028, 0.9968066811561584, 0.9879542112350463, 0.9895930647850036, 0.9863432765007019, 0.98741375207901, 0.9862336158752442, 0.9890262603759765, 0.983490777015686, 0.9885497212409973, 0.9904800176620483, 0.9860380530357361, 0.9859758853912354, 0.9809603691101074, 0.9835504651069641, 0.9846330761909485, 0.9804767370223999, 0.9845864057540894, 0.9801178336143493, 0.976957130432129, 0.9814820885658264, 0.9774229049682617, 0.971894371509552, 0.9719174146652222, 0.9651629447937011, 0.9610392689704895, 0.9495904922485352, 0.948234760761261, 0.93925701379776, 0.9348187208175659, 0.9241453647613526, 0.9176119208335877, 0.9133440852165222, 0.905490267276764, 0.8919809818267822, 0.8970833539962768, 0.8896285176277161, 0.8802256107330322, 0.8789666175842286, 0.8730956554412842, 0.8618815779685974, 0.8612435340881348, 0.8574832439422607, 0.8556922316551209, 0.8442975282669067, 0.8421581864356995, 0.8435487270355224, 0.8356904268264771, 0.8329351902008056, 0.8317697763442993, 0.8260491251945495, 0.8246611714363098, 0.8180248141288757, 0.8200891256332398, 0.8142566442489624, 0.809776508808136, 0.8100028872489929, 0.8038138151168823, 0.8039843678474426, 0.7982929706573486, 0.7949663758277893, 0.7883063435554505, 0.7897009372711181, 0.7872959256172181, 0.7860664248466491, 0.780028247833252, 0.7772791981697083, 0.7768249154090882, 0.7744302868843078, 0.7665280818939209, 0.7664567232131958, 0.7590767502784729, 0.7573655486106873, 0.7555010914802551, 0.7523910522460937, 0.7514573812484742, 0.7453794479370117, 0.7431367635726929, 0.7391793370246887, 0.7343325972557068, 0.7283328294754028, 0.7276360988616943, 0.7232748746871949, 0.7185522794723511, 0.7136672258377075, 0.7109002232551574, 0.7086532831192016, 0.7003620743751526, 0.6941894173622132, 0.6939730167388916, 0.6913561820983887, 0.6859391570091248], 'loss_test': [1.0701768398284912, 1.0822559595108032, 1.0860968828201294, 1.0851140022277832, 1.0653629302978516, 1.0848363637924194, 1.0776034593582153, 1.0903213024139404, 1.0865638256072998, 1.0752830505371094, 1.0935381650924683, 1.0893990993499756, 1.0800063610076904, 1.072845697402954, 1.0756444931030273, 1.069212794303894, 1.072872281074524, 1.0636866092681885, 1.0820131301879883, 1.0852546691894531, 1.0854649543762207, 1.072119116783142, 1.0931622982025146, 1.0917415618896484, 1.0929471254348755, 1.090618371963501, 1.072504997253418, 1.0885905027389526, 1.0753834247589111, 1.071892261505127, 1.0748671293258667, 1.0641286373138428, 1.056730031967163, 1.072251796722412, 1.0604978799819946, 1.0335795879364014, 1.0408291816711426, 1.0292021036148071, 1.0125077962875366, 1.024949550628662, 1.004989504814148, 1.005444884300232, 0.9935928583145142, 0.9883289337158203, 0.9785099625587463, 0.9562395215034485, 0.9568349719047546, 0.9626469612121582, 0.9777281880378723, 0.9631613492965698, 0.9643704295158386, 0.9555593132972717, 0.9576765298843384, 0.9579285979270935, 0.9440302848815918, 0.9574909210205078, 0.9559656381607056, 0.9461285471916199, 0.9402459263801575, 0.9456051588058472, 0.9523037672042847, 0.9214630126953125, 0.9302904009819031, 0.9350335001945496, 0.9327865242958069, 0.9274641275405884, 0.935181200504303, 0.9320453405380249, 0.9409171342849731, 0.9306097626686096, 0.9131537675857544, 0.9213351011276245, 0.9190285801887512, 0.9300233125686646, 0.9354773759841919, 0.9244478344917297, 0.9311831593513489, 0.9219401478767395, 0.9299106597900391, 0.9121932983398438, 0.9246399402618408, 0.9213514924049377, 0.9169929027557373, 0.921546459197998, 0.9123233556747437, 0.9126465320587158, 0.9201926589012146, 0.9310802221298218, 0.9238895177841187, 0.9214013814926147, 0.9217358231544495, 0.9208697080612183, 0.9077472686767578, 0.9064831137657166, 0.9237814545631409, 0.9078404903411865, 0.91802978515625, 0.9223501086235046, 0.9122596383094788, 0.9158523678779602], 'identifier': '1744130np'}