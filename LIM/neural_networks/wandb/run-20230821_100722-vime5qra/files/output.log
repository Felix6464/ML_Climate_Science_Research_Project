
 20%|███████████████████▌                                                                              | 20/100 [00:01<00:06, 11.70it/s, loss_test=1.085]
Epoch: 00, Training Loss: 0.9894, Test Loss: 1.0723
Epoch: 01, Training Loss: 0.9919, Test Loss: 1.0602
Epoch: 02, Training Loss: 0.9881, Test Loss: 1.0876
Epoch: 03, Training Loss: 0.9878, Test Loss: 1.0837
Epoch: 04, Training Loss: 0.9936, Test Loss: 1.0831
Epoch: 05, Training Loss: 0.9876, Test Loss: 1.0784
Epoch: 06, Training Loss: 0.9874, Test Loss: 1.0643
Epoch: 07, Training Loss: 0.9852, Test Loss: 1.0938
Epoch: 08, Training Loss: 0.9900, Test Loss: 1.0859
Epoch: 09, Training Loss: 0.9881, Test Loss: 1.0790
Epoch: 10, Training Loss: 0.9884, Test Loss: 1.0857
Epoch: 11, Training Loss: 0.9839, Test Loss: 1.0755
Epoch: 12, Training Loss: 0.9874, Test Loss: 1.0931
Epoch: 13, Training Loss: 0.9871, Test Loss: 1.0733
Epoch: 14, Training Loss: 0.9814, Test Loss: 1.0789
Epoch: 15, Training Loss: 0.9845, Test Loss: 1.0844
Epoch: 16, Training Loss: 0.9855, Test Loss: 1.0830
Epoch: 17, Training Loss: 0.9826, Test Loss: 1.0824
Epoch: 18, Training Loss: 0.9829, Test Loss: 1.0968

 44%|███████████████████████████████████████████                                                       | 44/100 [00:03<00:04, 12.16it/s, loss_test=0.992]
Epoch: 20, Training Loss: 0.9884, Test Loss: 1.0832
Epoch: 21, Training Loss: 0.9806, Test Loss: 1.0927
Epoch: 22, Training Loss: 0.9825, Test Loss: 1.1012
Epoch: 23, Training Loss: 0.9815, Test Loss: 1.0600
Epoch: 24, Training Loss: 0.9833, Test Loss: 1.0834
Epoch: 25, Training Loss: 0.9768, Test Loss: 1.0910
Epoch: 26, Training Loss: 0.9785, Test Loss: 1.0775
Epoch: 27, Training Loss: 0.9786, Test Loss: 1.0773
Epoch: 28, Training Loss: 0.9690, Test Loss: 1.0604
Epoch: 29, Training Loss: 0.9721, Test Loss: 1.0859
Epoch: 30, Training Loss: 0.9688, Test Loss: 1.0585
Epoch: 31, Training Loss: 0.9595, Test Loss: 1.0729
Epoch: 32, Training Loss: 0.9491, Test Loss: 1.0644
Epoch: 33, Training Loss: 0.9465, Test Loss: 1.0363
Epoch: 34, Training Loss: 0.9401, Test Loss: 1.0397
Epoch: 35, Training Loss: 0.9380, Test Loss: 1.0416
Epoch: 36, Training Loss: 0.9266, Test Loss: 1.0337
Epoch: 37, Training Loss: 0.9238, Test Loss: 1.0314
Epoch: 38, Training Loss: 0.9207, Test Loss: 1.0331
Epoch: 39, Training Loss: 0.9157, Test Loss: 1.0180
Epoch: 40, Training Loss: 0.9094, Test Loss: 0.9960
Epoch: 41, Training Loss: 0.9001, Test Loss: 1.0136
Epoch: 42, Training Loss: 0.9001, Test Loss: 0.9917

 68%|██████████████████████████████████████████████████████████████████▋                               | 68/100 [00:05<00:02, 11.94it/s, loss_test=0.917]
Epoch: 44, Training Loss: 0.8898, Test Loss: 0.9915
Epoch: 45, Training Loss: 0.8868, Test Loss: 0.9771
Epoch: 46, Training Loss: 0.8808, Test Loss: 0.9818
Epoch: 47, Training Loss: 0.8805, Test Loss: 0.9854
Epoch: 48, Training Loss: 0.8748, Test Loss: 0.9737
Epoch: 49, Training Loss: 0.8685, Test Loss: 0.9724
Epoch: 50, Training Loss: 0.8656, Test Loss: 0.9749
Epoch: 51, Training Loss: 0.8683, Test Loss: 0.9543
Epoch: 52, Training Loss: 0.8615, Test Loss: 0.9545
Epoch: 53, Training Loss: 0.8582, Test Loss: 0.9517
Epoch: 54, Training Loss: 0.8511, Test Loss: 0.9724
Epoch: 55, Training Loss: 0.8469, Test Loss: 0.9586
Epoch: 56, Training Loss: 0.8457, Test Loss: 0.9550
Epoch: 57, Training Loss: 0.8414, Test Loss: 0.9424
Epoch: 58, Training Loss: 0.8385, Test Loss: 0.9386
Epoch: 59, Training Loss: 0.8331, Test Loss: 0.9458
Epoch: 60, Training Loss: 0.8266, Test Loss: 0.9342
Epoch: 61, Training Loss: 0.8262, Test Loss: 0.9390
Epoch: 62, Training Loss: 0.8200, Test Loss: 0.9290
Epoch: 63, Training Loss: 0.8143, Test Loss: 0.9248
Epoch: 64, Training Loss: 0.8103, Test Loss: 0.9460
Epoch: 65, Training Loss: 0.8055, Test Loss: 0.9309
Epoch: 66, Training Loss: 0.8007, Test Loss: 0.9293
Epoch: 67, Training Loss: 0.7982, Test Loss: 0.9431

 94%|████████████████████████████████████████████████████████████████████████████████████████████      | 94/100 [00:07<00:00, 12.27it/s, loss_test=0.910]
Epoch: 69, Training Loss: 0.7898, Test Loss: 0.9202
Epoch: 70, Training Loss: 0.7870, Test Loss: 0.9124
Epoch: 71, Training Loss: 0.7829, Test Loss: 0.9155
Epoch: 72, Training Loss: 0.7789, Test Loss: 0.9195
Epoch: 73, Training Loss: 0.7771, Test Loss: 0.9303
Epoch: 74, Training Loss: 0.7727, Test Loss: 0.9184
Epoch: 75, Training Loss: 0.7717, Test Loss: 0.9196
Epoch: 76, Training Loss: 0.7656, Test Loss: 0.9173
Epoch: 77, Training Loss: 0.7614, Test Loss: 0.9041
Epoch: 78, Training Loss: 0.7574, Test Loss: 0.9129
Epoch: 79, Training Loss: 0.7563, Test Loss: 0.9066
Epoch: 80, Training Loss: 0.7504, Test Loss: 0.9120
Epoch: 81, Training Loss: 0.7477, Test Loss: 0.9121
Epoch: 82, Training Loss: 0.7454, Test Loss: 0.9130
Epoch: 83, Training Loss: 0.7426, Test Loss: 0.9165
Epoch: 84, Training Loss: 0.7370, Test Loss: 0.9191
Epoch: 85, Training Loss: 0.7361, Test Loss: 0.8992
Epoch: 86, Training Loss: 0.7332, Test Loss: 0.9128
Epoch: 87, Training Loss: 0.7266, Test Loss: 0.9047
Epoch: 88, Training Loss: 0.7263, Test Loss: 0.9036
Epoch: 89, Training Loss: 0.7224, Test Loss: 0.9158
Epoch: 90, Training Loss: 0.7213, Test Loss: 0.9081
Epoch: 91, Training Loss: 0.7144, Test Loss: 0.9074
Epoch: 92, Training Loss: 0.7112, Test Loss: 0.9086

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.13it/s, loss_test=0.900]
Epoch: 94, Training Loss: 0.7067, Test Loss: 0.9056
Epoch: 95, Training Loss: 0.7040, Test Loss: 0.8991
Epoch: 96, Training Loss: 0.7001, Test Loss: 0.9166
Epoch: 97, Training Loss: 0.6990, Test Loss: 0.9218
Epoch: 98, Training Loss: 0.6947, Test Loss: 0.9031
Epoch: 99, Training Loss: 0.6940, Test Loss: 0.9002
Model saved as model_4475523np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-1250000-4475523np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 100, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9893608331680298, 0.9919433474540711, 0.9881069779396057, 0.9877781629562378, 0.9936448216438294, 0.9876343965530395, 0.9874309301376343, 0.9851583361625671, 0.9900042176246643, 0.9880855321884155, 0.9883567929267884, 0.983901047706604, 0.9874114751815796, 0.9870866298675537, 0.981437349319458, 0.984526526927948, 0.985459053516388, 0.9825851798057557, 0.9829448819160461, 0.9809293270111084, 0.9883649706840515, 0.9806403875350952, 0.9824777722358704, 0.9814819812774658, 0.9832860112190247, 0.9767785191535949, 0.9784979104995728, 0.9786071300506591, 0.9689817547798156, 0.9720626711845398, 0.9688075065612793, 0.9594850182533264, 0.9490814924240112, 0.9464535474777221, 0.9400918126106262, 0.9380276560783386, 0.9265973091125488, 0.9238383769989014, 0.9206531167030334, 0.9156519174575806, 0.9094287514686584, 0.9001449465751648, 0.9001242876052856, 0.8932969808578491, 0.8898460984230041, 0.886779510974884, 0.8808156013488769, 0.8805336236953736, 0.8747583150863647, 0.8685234189033508, 0.8656004548072815, 0.8683244109153747, 0.8615252375602722, 0.8582347989082336, 0.8511103868484498, 0.8468920946121216, 0.8456989288330078, 0.8414220213890076, 0.8384634971618652, 0.8330574631690979, 0.8265969276428222, 0.8261799573898315, 0.8200297951698303, 0.8143312573432923, 0.8102622389793396, 0.8054935812950135, 0.8007317781448364, 0.7982091784477234, 0.7926759958267212, 0.7897853016853332, 0.7870260119438172, 0.782906973361969, 0.7789031386375427, 0.7771313428878784, 0.7727286577224731, 0.7717139005661011, 0.7655652165412903, 0.7613883256912232, 0.7574093580245972, 0.75627201795578, 0.7504073023796082, 0.7477459669113159, 0.7454007863998413, 0.7425545692443848, 0.7369556546211242, 0.7361211895942688, 0.7331968426704407, 0.7265743613243103, 0.7263179421424866, 0.7224295616149903, 0.721307384967804, 0.7143672943115235, 0.7111976146697998, 0.711310625076294, 0.7067449808120727, 0.7040016293525696, 0.7000807404518128, 0.6989872336387635, 0.6947394132614135, 0.6940396666526795], 'loss_test': [1.0723267793655396, 1.060226321220398, 1.087558388710022, 1.0837397575378418, 1.0831046104431152, 1.0783977508544922, 1.0643075704574585, 1.0937587022781372, 1.0858699083328247, 1.0790127515792847, 1.0856677293777466, 1.0754872560501099, 1.0930708646774292, 1.0732964277267456, 1.0788538455963135, 1.0843517780303955, 1.0829986333847046, 1.0824363231658936, 1.0968210697174072, 1.0845929384231567, 1.0831804275512695, 1.0926893949508667, 1.1012049913406372, 1.0599664449691772, 1.0833808183670044, 1.0909725427627563, 1.0775386095046997, 1.07729971408844, 1.0603728294372559, 1.085935354232788, 1.0584847927093506, 1.0729010105133057, 1.0643962621688843, 1.0363073348999023, 1.0397071838378906, 1.041638970375061, 1.0336644649505615, 1.0314078330993652, 1.033139705657959, 1.017991542816162, 0.995997965335846, 1.0136399269104004, 0.9917099475860596, 0.9980144500732422, 0.9915410876274109, 0.97712242603302, 0.9818305969238281, 0.9854463338851929, 0.973744809627533, 0.9724043607711792, 0.9749438762664795, 0.9543318152427673, 0.9545254111289978, 0.9516690969467163, 0.9723959565162659, 0.9585725665092468, 0.9550433158874512, 0.9423647522926331, 0.9385647177696228, 0.94580078125, 0.9341915845870972, 0.9390290379524231, 0.9289778470993042, 0.9248130321502686, 0.946038007736206, 0.9308828115463257, 0.929277241230011, 0.9430609941482544, 0.9166221618652344, 0.9202316403388977, 0.9124154448509216, 0.9154880046844482, 0.9194673299789429, 0.9303045868873596, 0.9183943271636963, 0.9195969104766846, 0.917309582233429, 0.904076337814331, 0.9128792881965637, 0.9065503478050232, 0.9120281934738159, 0.9121429920196533, 0.9129594564437866, 0.9164981842041016, 0.9190769791603088, 0.899211049079895, 0.9127715229988098, 0.9046955704689026, 0.9035853147506714, 0.915846586227417, 0.9081048369407654, 0.9073837399482727, 0.9086212515830994, 0.909916877746582, 0.9056379795074463, 0.8991036415100098, 0.9165632724761963, 0.9217641353607178, 0.9031074047088623, 0.9001895785331726], 'identifier': '4475523np'}