
 19%|██████████████████▌                                                                               | 19/100 [00:01<00:07, 11.47it/s, loss_test=1.080]
Epoch: 00, Training Loss: 0.9948, Test Loss: 1.0682
Epoch: 01, Training Loss: 0.9940, Test Loss: 1.0752
Epoch: 02, Training Loss: 0.9921, Test Loss: 1.0854
Epoch: 03, Training Loss: 0.9924, Test Loss: 1.0631
Epoch: 04, Training Loss: 0.9921, Test Loss: 1.0623
Epoch: 05, Training Loss: 0.9957, Test Loss: 1.0906
Epoch: 06, Training Loss: 0.9924, Test Loss: 1.0804
Epoch: 07, Training Loss: 0.9915, Test Loss: 1.0818
Epoch: 08, Training Loss: 0.9926, Test Loss: 1.0768
Epoch: 09, Training Loss: 0.9934, Test Loss: 1.0813
Epoch: 10, Training Loss: 0.9880, Test Loss: 1.0554
Epoch: 11, Training Loss: 0.9859, Test Loss: 1.0739
Epoch: 12, Training Loss: 0.9906, Test Loss: 1.0893
Epoch: 13, Training Loss: 0.9906, Test Loss: 1.0809
Epoch: 14, Training Loss: 0.9869, Test Loss: 1.0848
Epoch: 15, Training Loss: 0.9887, Test Loss: 1.0799
Epoch: 16, Training Loss: 0.9854, Test Loss: 1.0749
Epoch: 17, Training Loss: 0.9847, Test Loss: 1.0693
Epoch: 18, Training Loss: 0.9878, Test Loss: 1.0827

 45%|████████████████████████████████████████████                                                      | 45/100 [00:03<00:04, 12.36it/s, loss_test=1.003]
Epoch: 20, Training Loss: 0.9879, Test Loss: 1.0780
Epoch: 21, Training Loss: 0.9873, Test Loss: 1.0858
Epoch: 22, Training Loss: 0.9838, Test Loss: 1.0949
Epoch: 23, Training Loss: 0.9816, Test Loss: 1.0786
Epoch: 24, Training Loss: 0.9792, Test Loss: 1.0802
Epoch: 25, Training Loss: 0.9808, Test Loss: 1.0833
Epoch: 26, Training Loss: 0.9790, Test Loss: 1.0860
Epoch: 27, Training Loss: 0.9824, Test Loss: 1.0750
Epoch: 28, Training Loss: 0.9748, Test Loss: 1.0735
Epoch: 29, Training Loss: 0.9776, Test Loss: 1.0696
Epoch: 30, Training Loss: 0.9731, Test Loss: 1.0796
Epoch: 31, Training Loss: 0.9722, Test Loss: 1.0583
Epoch: 32, Training Loss: 0.9660, Test Loss: 1.0578
Epoch: 33, Training Loss: 0.9550, Test Loss: 1.0549
Epoch: 34, Training Loss: 0.9513, Test Loss: 1.0552
Epoch: 35, Training Loss: 0.9454, Test Loss: 1.0490
Epoch: 36, Training Loss: 0.9383, Test Loss: 1.0512
Epoch: 37, Training Loss: 0.9363, Test Loss: 1.0523
Epoch: 38, Training Loss: 0.9298, Test Loss: 1.0424
Epoch: 39, Training Loss: 0.9210, Test Loss: 1.0582
Epoch: 40, Training Loss: 0.9133, Test Loss: 1.0109
Epoch: 41, Training Loss: 0.9110, Test Loss: 1.0228
Epoch: 42, Training Loss: 0.9033, Test Loss: 1.0176
Epoch: 43, Training Loss: 0.8961, Test Loss: 1.0127

 69%|███████████████████████████████████████████████████████████████████▌                              | 69/100 [00:05<00:02, 12.19it/s, loss_test=0.947]
Epoch: 45, Training Loss: 0.8882, Test Loss: 1.0018
Epoch: 46, Training Loss: 0.8803, Test Loss: 0.9962
Epoch: 47, Training Loss: 0.8766, Test Loss: 1.0021
Epoch: 48, Training Loss: 0.8725, Test Loss: 0.9807
Epoch: 49, Training Loss: 0.8674, Test Loss: 0.9774
Epoch: 50, Training Loss: 0.8612, Test Loss: 0.9812
Epoch: 51, Training Loss: 0.8564, Test Loss: 0.9760
Epoch: 52, Training Loss: 0.8525, Test Loss: 0.9706
Epoch: 53, Training Loss: 0.8504, Test Loss: 0.9652
Epoch: 54, Training Loss: 0.8422, Test Loss: 0.9798
Epoch: 55, Training Loss: 0.8387, Test Loss: 0.9640
Epoch: 56, Training Loss: 0.8384, Test Loss: 0.9688
Epoch: 57, Training Loss: 0.8286, Test Loss: 0.9421
Epoch: 58, Training Loss: 0.8301, Test Loss: 0.9760
Epoch: 59, Training Loss: 0.8268, Test Loss: 0.9658
Epoch: 60, Training Loss: 0.8215, Test Loss: 0.9516
Epoch: 61, Training Loss: 0.8111, Test Loss: 0.9567
Epoch: 62, Training Loss: 0.8072, Test Loss: 0.9524
Epoch: 63, Training Loss: 0.8076, Test Loss: 0.9481
Epoch: 64, Training Loss: 0.8003, Test Loss: 0.9338
Epoch: 65, Training Loss: 0.7986, Test Loss: 0.9456
Epoch: 66, Training Loss: 0.7931, Test Loss: 0.9412
Epoch: 67, Training Loss: 0.7894, Test Loss: 0.9546
Epoch: 68, Training Loss: 0.7837, Test Loss: 0.9253
Epoch: 69, Training Loss: 0.7835, Test Loss: 0.9472
Epoch: 70, Training Loss: 0.7804, Test Loss: 0.9324
Epoch: 71, Training Loss: 0.7764, Test Loss: 0.9221
Epoch: 72, Training Loss: 0.7691, Test Loss: 0.9218
Epoch: 73, Training Loss: 0.7697, Test Loss: 0.9326
Epoch: 74, Training Loss: 0.7628, Test Loss: 0.9113
Epoch: 75, Training Loss: 0.7587, Test Loss: 0.9287
Epoch: 76, Training Loss: 0.7591, Test Loss: 0.9120
Epoch: 77, Training Loss: 0.7561, Test Loss: 0.9145
Epoch: 78, Training Loss: 0.7514, Test Loss: 0.9129
Epoch: 79, Training Loss: 0.7501, Test Loss: 0.9071
Epoch: 80, Training Loss: 0.7449, Test Loss: 0.9110
Epoch: 81, Training Loss: 0.7414, Test Loss: 0.9262
Epoch: 82, Training Loss: 0.7392, Test Loss: 0.9146
Epoch: 83, Training Loss: 0.7361, Test Loss: 0.9091
Epoch: 84, Training Loss: 0.7338, Test Loss: 0.9072
Epoch: 85, Training Loss: 0.7272, Test Loss: 0.8995
Epoch: 86, Training Loss: 0.7262, Test Loss: 0.8999
Epoch: 87, Training Loss: 0.7232, Test Loss: 0.8963
Epoch: 88, Training Loss: 0.7203, Test Loss: 0.9194
Epoch: 89, Training Loss: 0.7185, Test Loss: 0.9004
Epoch: 90, Training Loss: 0.7118, Test Loss: 0.9037
Epoch: 91, Training Loss: 0.7118, Test Loss: 0.8877
Epoch: 92, Training Loss: 0.7046, Test Loss: 0.9114


100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.16it/s, loss_test=0.898]
Epoch: 94, Training Loss: 0.7026, Test Loss: 0.8903
Epoch: 95, Training Loss: 0.6921, Test Loss: 0.8998
Epoch: 96, Training Loss: 0.6920, Test Loss: 0.8947
Epoch: 97, Training Loss: 0.6885, Test Loss: 0.8945
Epoch: 98, Training Loss: 0.6878, Test Loss: 0.8893
Epoch: 99, Training Loss: 0.6851, Test Loss: 0.8976
Model saved as model_4796364np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-125000-4796364np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 100, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9948095679283142, 0.9940312623977661, 0.9921136021614074, 0.9923872590065003, 0.9921109676361084, 0.9957153081893921, 0.9923635482788086, 0.9914637804031372, 0.992635989189148, 0.9933797597885132, 0.9879730224609375, 0.9859093189239502, 0.990576708316803, 0.9905646562576294, 0.9869247198104858, 0.9886835217475891, 0.985374140739441, 0.9847242832183838, 0.9878399968147278, 0.9842360854148865, 0.9879210472106934, 0.9872964143753051, 0.9838472127914428, 0.981628131866455, 0.9791734457015991, 0.9807793259620666, 0.9790016055107117, 0.9823721766471862, 0.9747852802276611, 0.9776302337646484, 0.9730850458145142, 0.9721546053886414, 0.9660096287727356, 0.9550391793251037, 0.9513457417488098, 0.9454113602638244, 0.9383165717124939, 0.9363450050354004, 0.9297701954841614, 0.9210124254226685, 0.9132560610771179, 0.9110291481018067, 0.9032524704933167, 0.8961252570152283, 0.8906577587127685, 0.8882217168807983, 0.8803319215774537, 0.876632559299469, 0.872546911239624, 0.867352819442749, 0.8611516356468201, 0.8563669919967651, 0.8524867296218872, 0.8503573417663575, 0.842210876941681, 0.8387142777442932, 0.8384487271308899, 0.8286337971687316, 0.8301279902458191, 0.8267640590667724, 0.8214561343193054, 0.8110768079757691, 0.8071810603141785, 0.8076160311698913, 0.8002703428268433, 0.7986245274543762, 0.7931476712226868, 0.7893968224525452, 0.7837418794631958, 0.7834739685058594, 0.7803632497787476, 0.7764420986175538, 0.7690922856330872, 0.7696578741073609, 0.7627987146377564, 0.7586613655090332, 0.7591265916824341, 0.7560661315917969, 0.7514036059379577, 0.7500748634338379, 0.744864821434021, 0.7413872480392456, 0.7392434716224671, 0.7360989332199097, 0.7338368654251098, 0.7271973252296448, 0.7261922955513, 0.7231641292572022, 0.720284378528595, 0.7184962153434753, 0.7118486881256103, 0.7118475675582886, 0.7046268224716187, 0.7028853535652161, 0.7026104688644409, 0.6920601606369019, 0.6920129776000976, 0.6884759545326233, 0.687824547290802, 0.685133421421051], 'loss_test': [1.068198323249817, 1.0752166509628296, 1.0854368209838867, 1.0631272792816162, 1.0622817277908325, 1.0905593633651733, 1.080445408821106, 1.081803321838379, 1.0767848491668701, 1.0813075304031372, 1.0553812980651855, 1.073930025100708, 1.0893125534057617, 1.0808850526809692, 1.0848033428192139, 1.0799421072006226, 1.074890375137329, 1.0693408250808716, 1.0826562643051147, 1.079593539237976, 1.0779544115066528, 1.085779070854187, 1.094915509223938, 1.0785866975784302, 1.0802314281463623, 1.0833429098129272, 1.085957407951355, 1.0750130414962769, 1.0735000371932983, 1.0696053504943848, 1.0796444416046143, 1.0582715272903442, 1.0578101873397827, 1.0549085140228271, 1.0551745891571045, 1.0489658117294312, 1.0512285232543945, 1.0522645711898804, 1.0424472093582153, 1.0582044124603271, 1.010896921157837, 1.0228241682052612, 1.017623782157898, 1.0126726627349854, 1.002820611000061, 1.0017650127410889, 0.9962019324302673, 1.0020822286605835, 0.9806627035140991, 0.9774143695831299, 0.9811804294586182, 0.9760138988494873, 0.9705654978752136, 0.965229332447052, 0.9798245429992676, 0.96401047706604, 0.968816339969635, 0.9420856237411499, 0.9759933948516846, 0.9658089876174927, 0.9516444206237793, 0.9566991329193115, 0.9523937702178955, 0.9481360912322998, 0.9338490962982178, 0.9456093907356262, 0.9412153959274292, 0.9546236991882324, 0.9253419041633606, 0.9472132325172424, 0.9323815703392029, 0.9220979809761047, 0.9218448400497437, 0.9325817823410034, 0.9112614989280701, 0.9286984205245972, 0.9119746685028076, 0.9144970774650574, 0.9128979444503784, 0.9071445465087891, 0.9110240936279297, 0.926152229309082, 0.9145697951316833, 0.9091155529022217, 0.9071862101554871, 0.8995035290718079, 0.8999153971672058, 0.8963162302970886, 0.9193757772445679, 0.900387167930603, 0.9037379026412964, 0.8876902461051941, 0.9114181995391846, 0.9069778919219971, 0.8902920484542847, 0.8997934460639954, 0.8947069048881531, 0.8944529294967651, 0.8893014192581177, 0.897568941116333], 'identifier': '4796364np'}