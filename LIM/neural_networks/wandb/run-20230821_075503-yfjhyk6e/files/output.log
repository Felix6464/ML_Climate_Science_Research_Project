
  7%|███████                                                                                           | 18/250 [00:01<00:19, 11.64it/s, loss_test=1.072]
Epoch: 00, Training Loss: 0.9922, Test Loss: 1.0562
Epoch: 01, Training Loss: 0.9923, Test Loss: 1.0654
Epoch: 02, Training Loss: 0.9941, Test Loss: 1.0774
Epoch: 03, Training Loss: 0.9912, Test Loss: 1.0823
Epoch: 04, Training Loss: 0.9959, Test Loss: 1.0761
Epoch: 05, Training Loss: 0.9888, Test Loss: 1.0830
Epoch: 06, Training Loss: 0.9916, Test Loss: 1.0660
Epoch: 07, Training Loss: 0.9914, Test Loss: 1.0874
Epoch: 08, Training Loss: 0.9936, Test Loss: 1.0865
Epoch: 09, Training Loss: 0.9890, Test Loss: 1.0782
Epoch: 10, Training Loss: 0.9929, Test Loss: 1.0760
Epoch: 11, Training Loss: 0.9837, Test Loss: 1.0809
Epoch: 12, Training Loss: 0.9852, Test Loss: 1.0705
Epoch: 13, Training Loss: 0.9847, Test Loss: 1.0741
Epoch: 14, Training Loss: 0.9870, Test Loss: 1.0663
Epoch: 15, Training Loss: 0.9875, Test Loss: 1.0746
Epoch: 16, Training Loss: 0.9875, Test Loss: 1.0669
Epoch: 17, Training Loss: 0.9836, Test Loss: 1.0833

 17%|████████████████▍                                                                                 | 42/250 [00:03<00:17, 12.11it/s, loss_test=0.991]
Epoch: 19, Training Loss: 0.9855, Test Loss: 1.0836
Epoch: 20, Training Loss: 0.9837, Test Loss: 1.0645
Epoch: 21, Training Loss: 0.9829, Test Loss: 1.0994
Epoch: 22, Training Loss: 0.9827, Test Loss: 1.0855
Epoch: 23, Training Loss: 0.9768, Test Loss: 1.0869
Epoch: 24, Training Loss: 0.9812, Test Loss: 1.0663
Epoch: 25, Training Loss: 0.9813, Test Loss: 1.0895
Epoch: 26, Training Loss: 0.9748, Test Loss: 1.0863
Epoch: 27, Training Loss: 0.9765, Test Loss: 1.0733
Epoch: 28, Training Loss: 0.9779, Test Loss: 1.0873
Epoch: 29, Training Loss: 0.9745, Test Loss: 1.0892
Epoch: 30, Training Loss: 0.9677, Test Loss: 1.0754
Epoch: 31, Training Loss: 0.9558, Test Loss: 1.0800
Epoch: 32, Training Loss: 0.9510, Test Loss: 1.0665
Epoch: 33, Training Loss: 0.9408, Test Loss: 1.0560
Epoch: 34, Training Loss: 0.9353, Test Loss: 1.0369
Epoch: 35, Training Loss: 0.9223, Test Loss: 1.0240
Epoch: 36, Training Loss: 0.9211, Test Loss: 1.0421
Epoch: 37, Training Loss: 0.9123, Test Loss: 1.0258
Epoch: 38, Training Loss: 0.9068, Test Loss: 1.0045
Epoch: 39, Training Loss: 0.9015, Test Loss: 1.0069
Epoch: 40, Training Loss: 0.8986, Test Loss: 1.0101
Epoch: 41, Training Loss: 0.8923, Test Loss: 0.9781

 27%|██████████████████████████▋                                                                       | 68/250 [00:05<00:14, 12.43it/s, loss_test=0.935]
Epoch: 43, Training Loss: 0.8824, Test Loss: 0.9884
Epoch: 44, Training Loss: 0.8796, Test Loss: 0.9718
Epoch: 45, Training Loss: 0.8760, Test Loss: 0.9783
Epoch: 46, Training Loss: 0.8767, Test Loss: 0.9633
Epoch: 47, Training Loss: 0.8662, Test Loss: 0.9798
Epoch: 48, Training Loss: 0.8655, Test Loss: 0.9753
Epoch: 49, Training Loss: 0.8577, Test Loss: 0.9739
Epoch: 50, Training Loss: 0.8555, Test Loss: 0.9657
Epoch: 51, Training Loss: 0.8574, Test Loss: 0.9771
Epoch: 52, Training Loss: 0.8487, Test Loss: 0.9663
Epoch: 53, Training Loss: 0.8493, Test Loss: 0.9774
Epoch: 54, Training Loss: 0.8443, Test Loss: 0.9571
Epoch: 55, Training Loss: 0.8396, Test Loss: 0.9584
Epoch: 56, Training Loss: 0.8381, Test Loss: 0.9491
Epoch: 57, Training Loss: 0.8387, Test Loss: 0.9582
Epoch: 58, Training Loss: 0.8316, Test Loss: 0.9380
Epoch: 59, Training Loss: 0.8313, Test Loss: 0.9425
Epoch: 60, Training Loss: 0.8301, Test Loss: 0.9397
Epoch: 61, Training Loss: 0.8255, Test Loss: 0.9543
Epoch: 62, Training Loss: 0.8251, Test Loss: 0.9444
Epoch: 63, Training Loss: 0.8240, Test Loss: 0.9176
Epoch: 64, Training Loss: 0.8215, Test Loss: 0.9357
Epoch: 65, Training Loss: 0.8179, Test Loss: 0.9513
Epoch: 66, Training Loss: 0.8161, Test Loss: 0.9288

 37%|████████████████████████████████████                                                              | 92/250 [00:07<00:12, 12.34it/s, loss_test=0.911]
Epoch: 68, Training Loss: 0.8136, Test Loss: 0.9172
Epoch: 69, Training Loss: 0.8073, Test Loss: 0.9262
Epoch: 70, Training Loss: 0.8092, Test Loss: 0.9422
Epoch: 71, Training Loss: 0.8054, Test Loss: 0.9269
Epoch: 72, Training Loss: 0.8013, Test Loss: 0.9322
Epoch: 73, Training Loss: 0.8012, Test Loss: 0.9183
Epoch: 74, Training Loss: 0.7975, Test Loss: 0.9308
Epoch: 75, Training Loss: 0.7956, Test Loss: 0.9382
Epoch: 76, Training Loss: 0.7915, Test Loss: 0.9255
Epoch: 77, Training Loss: 0.7885, Test Loss: 0.9282
Epoch: 78, Training Loss: 0.7890, Test Loss: 0.9284
Epoch: 79, Training Loss: 0.7827, Test Loss: 0.9153
Epoch: 80, Training Loss: 0.7770, Test Loss: 0.9301
Epoch: 81, Training Loss: 0.7741, Test Loss: 0.9156
Epoch: 82, Training Loss: 0.7730, Test Loss: 0.9113
Epoch: 83, Training Loss: 0.7698, Test Loss: 0.9130
Epoch: 84, Training Loss: 0.7692, Test Loss: 0.9138
Epoch: 85, Training Loss: 0.7649, Test Loss: 0.9232
Epoch: 86, Training Loss: 0.7624, Test Loss: 0.9133
Epoch: 87, Training Loss: 0.7600, Test Loss: 0.8981
Epoch: 88, Training Loss: 0.7545, Test Loss: 0.9139
Epoch: 89, Training Loss: 0.7536, Test Loss: 0.9109
Epoch: 90, Training Loss: 0.7456, Test Loss: 0.9139
Epoch: 91, Training Loss: 0.7461, Test Loss: 0.9103

 46%|█████████████████████████████████████████████                                                    | 116/250 [00:09<00:11, 12.03it/s, loss_test=0.893]
Epoch: 93, Training Loss: 0.7338, Test Loss: 0.8954
Epoch: 94, Training Loss: 0.7323, Test Loss: 0.9039
Epoch: 95, Training Loss: 0.7294, Test Loss: 0.9137
Epoch: 96, Training Loss: 0.7260, Test Loss: 0.8966
Epoch: 97, Training Loss: 0.7244, Test Loss: 0.8948
Epoch: 98, Training Loss: 0.7191, Test Loss: 0.8971
Epoch: 99, Training Loss: 0.7151, Test Loss: 0.8888
Epoch: 100, Training Loss: 0.7122, Test Loss: 0.8932
Epoch: 101, Training Loss: 0.7072, Test Loss: 0.8998
Epoch: 102, Training Loss: 0.7022, Test Loss: 0.8937
Epoch: 103, Training Loss: 0.7026, Test Loss: 0.8981
Epoch: 104, Training Loss: 0.6960, Test Loss: 0.8903
Epoch: 105, Training Loss: 0.6920, Test Loss: 0.8993
Epoch: 106, Training Loss: 0.6892, Test Loss: 0.8777
Epoch: 107, Training Loss: 0.6851, Test Loss: 0.8983
Epoch: 108, Training Loss: 0.6799, Test Loss: 0.9058
Epoch: 109, Training Loss: 0.6733, Test Loss: 0.8837
Epoch: 110, Training Loss: 0.6759, Test Loss: 0.9025
Epoch: 111, Training Loss: 0.6649, Test Loss: 0.8988
Epoch: 112, Training Loss: 0.6663, Test Loss: 0.8799
Epoch: 113, Training Loss: 0.6623, Test Loss: 0.9055
Epoch: 114, Training Loss: 0.6586, Test Loss: 0.8985
Epoch: 115, Training Loss: 0.6508, Test Loss: 0.9017

 57%|███████████████████████████████████████████████████████                                          | 142/250 [00:11<00:08, 12.26it/s, loss_test=0.922]
Epoch: 117, Training Loss: 0.6445, Test Loss: 0.9008
Epoch: 118, Training Loss: 0.6411, Test Loss: 0.9054
Epoch: 119, Training Loss: 0.6379, Test Loss: 0.8915
Epoch: 120, Training Loss: 0.6333, Test Loss: 0.8966
Epoch: 121, Training Loss: 0.6312, Test Loss: 0.9003
Epoch: 122, Training Loss: 0.6258, Test Loss: 0.8981
Epoch: 123, Training Loss: 0.6215, Test Loss: 0.8979
Epoch: 124, Training Loss: 0.6166, Test Loss: 0.8929
Epoch: 125, Training Loss: 0.6158, Test Loss: 0.9137
Epoch: 126, Training Loss: 0.6142, Test Loss: 0.8969
Epoch: 127, Training Loss: 0.6079, Test Loss: 0.8956
Epoch: 128, Training Loss: 0.6050, Test Loss: 0.8942
Epoch: 129, Training Loss: 0.6004, Test Loss: 0.9043
Epoch: 130, Training Loss: 0.5964, Test Loss: 0.8965
Epoch: 131, Training Loss: 0.5942, Test Loss: 0.9051
Epoch: 132, Training Loss: 0.5900, Test Loss: 0.9071
Epoch: 133, Training Loss: 0.5878, Test Loss: 0.8947
Epoch: 134, Training Loss: 0.5867, Test Loss: 0.8968
Epoch: 135, Training Loss: 0.5808, Test Loss: 0.9038
Epoch: 136, Training Loss: 0.5796, Test Loss: 0.9113
Epoch: 137, Training Loss: 0.5748, Test Loss: 0.9072
Epoch: 138, Training Loss: 0.5735, Test Loss: 0.9025
Epoch: 139, Training Loss: 0.5685, Test Loss: 0.9149
Epoch: 140, Training Loss: 0.5679, Test Loss: 0.9090

 66%|████████████████████████████████████████████████████████████████▍                                | 166/250 [00:13<00:06, 12.96it/s, loss_test=0.946]
Epoch: 142, Training Loss: 0.5595, Test Loss: 0.9068
Epoch: 143, Training Loss: 0.5559, Test Loss: 0.9119
Epoch: 144, Training Loss: 0.5540, Test Loss: 0.8981
Epoch: 145, Training Loss: 0.5514, Test Loss: 0.9147
Epoch: 146, Training Loss: 0.5503, Test Loss: 0.9180
Epoch: 147, Training Loss: 0.5448, Test Loss: 0.9221
Epoch: 148, Training Loss: 0.5434, Test Loss: 0.9147
Epoch: 149, Training Loss: 0.5397, Test Loss: 0.9150
Epoch: 150, Training Loss: 0.5339, Test Loss: 0.9094
Epoch: 151, Training Loss: 0.5349, Test Loss: 0.9201
Epoch: 152, Training Loss: 0.5322, Test Loss: 0.9240
Epoch: 153, Training Loss: 0.5274, Test Loss: 0.9287
Epoch: 154, Training Loss: 0.5234, Test Loss: 0.9440
Epoch: 155, Training Loss: 0.5219, Test Loss: 0.9377
Epoch: 156, Training Loss: 0.5191, Test Loss: 0.9256
Epoch: 157, Training Loss: 0.5172, Test Loss: 0.9292
Epoch: 158, Training Loss: 0.5151, Test Loss: 0.9295
Epoch: 159, Training Loss: 0.5107, Test Loss: 0.9214
Epoch: 160, Training Loss: 0.5102, Test Loss: 0.9402
Epoch: 161, Training Loss: 0.5069, Test Loss: 0.9320
Epoch: 162, Training Loss: 0.5035, Test Loss: 0.9359
Epoch: 163, Training Loss: 0.5010, Test Loss: 0.9312
Epoch: 164, Training Loss: 0.4977, Test Loss: 0.9386
Epoch: 165, Training Loss: 0.4959, Test Loss: 0.9493

 77%|██████████████████████████████████████████████████████████████████████████▍                      | 192/250 [00:15<00:04, 12.69it/s, loss_test=0.960]
Epoch: 167, Training Loss: 0.4902, Test Loss: 0.9476
Epoch: 168, Training Loss: 0.4883, Test Loss: 0.9485
Epoch: 169, Training Loss: 0.4872, Test Loss: 0.9376
Epoch: 170, Training Loss: 0.4821, Test Loss: 0.9348
Epoch: 171, Training Loss: 0.4812, Test Loss: 0.9498
Epoch: 172, Training Loss: 0.4793, Test Loss: 0.9471
Epoch: 173, Training Loss: 0.4757, Test Loss: 0.9463
Epoch: 174, Training Loss: 0.4740, Test Loss: 0.9452
Epoch: 175, Training Loss: 0.4717, Test Loss: 0.9557
Epoch: 176, Training Loss: 0.4682, Test Loss: 0.9531
Epoch: 177, Training Loss: 0.4648, Test Loss: 0.9545
Epoch: 178, Training Loss: 0.4652, Test Loss: 0.9500
Epoch: 179, Training Loss: 0.4600, Test Loss: 0.9571
Epoch: 180, Training Loss: 0.4603, Test Loss: 0.9555
Epoch: 181, Training Loss: 0.4573, Test Loss: 0.9441
Epoch: 182, Training Loss: 0.4548, Test Loss: 0.9567
Epoch: 183, Training Loss: 0.4532, Test Loss: 0.9599
Epoch: 184, Training Loss: 0.4502, Test Loss: 0.9597
Epoch: 185, Training Loss: 0.4486, Test Loss: 0.9564
Epoch: 186, Training Loss: 0.4464, Test Loss: 0.9568
Epoch: 187, Training Loss: 0.4423, Test Loss: 0.9484
Epoch: 188, Training Loss: 0.4402, Test Loss: 0.9607
Epoch: 189, Training Loss: 0.4386, Test Loss: 0.9674
Epoch: 190, Training Loss: 0.4371, Test Loss: 0.9678

 86%|███████████████████████████████████████████████████████████████████████████████████▊             | 216/250 [00:17<00:02, 12.57it/s, loss_test=0.985]
Epoch: 192, Training Loss: 0.4324, Test Loss: 0.9669
Epoch: 193, Training Loss: 0.4318, Test Loss: 0.9634
Epoch: 194, Training Loss: 0.4285, Test Loss: 0.9757
Epoch: 195, Training Loss: 0.4262, Test Loss: 0.9682
Epoch: 196, Training Loss: 0.4253, Test Loss: 0.9648
Epoch: 197, Training Loss: 0.4222, Test Loss: 0.9697
Epoch: 198, Training Loss: 0.4194, Test Loss: 0.9636
Epoch: 199, Training Loss: 0.4180, Test Loss: 0.9707
Epoch: 200, Training Loss: 0.4169, Test Loss: 0.9774
Epoch: 201, Training Loss: 0.4109, Test Loss: 0.9680
Epoch: 202, Training Loss: 0.4120, Test Loss: 0.9700
Epoch: 203, Training Loss: 0.4099, Test Loss: 0.9712
Epoch: 204, Training Loss: 0.4071, Test Loss: 0.9867
Epoch: 205, Training Loss: 0.4073, Test Loss: 0.9762
Epoch: 206, Training Loss: 0.4026, Test Loss: 0.9721
Epoch: 207, Training Loss: 0.4014, Test Loss: 0.9714
Epoch: 208, Training Loss: 0.4007, Test Loss: 0.9913
Epoch: 209, Training Loss: 0.3962, Test Loss: 0.9787
Epoch: 210, Training Loss: 0.3960, Test Loss: 0.9890
Epoch: 211, Training Loss: 0.3960, Test Loss: 0.9779
Epoch: 212, Training Loss: 0.3905, Test Loss: 0.9654
Epoch: 213, Training Loss: 0.3898, Test Loss: 0.9850
Epoch: 214, Training Loss: 0.3892, Test Loss: 0.9887
Epoch: 215, Training Loss: 0.3852, Test Loss: 0.9724
Epoch: 216, Training Loss: 0.3855, Test Loss: 0.9845
Epoch: 217, Training Loss: 0.3839, Test Loss: 0.9836
Epoch: 218, Training Loss: 0.3812, Test Loss: 0.9990
Epoch: 219, Training Loss: 0.3800, Test Loss: 0.9903
Epoch: 220, Training Loss: 0.3779, Test Loss: 0.9853
Epoch: 221, Training Loss: 0.3760, Test Loss: 0.9879
Epoch: 222, Training Loss: 0.3745, Test Loss: 0.9931
Epoch: 223, Training Loss: 0.3718, Test Loss: 0.9886
Epoch: 224, Training Loss: 0.3706, Test Loss: 0.9792
Epoch: 225, Training Loss: 0.3688, Test Loss: 1.0037
Epoch: 226, Training Loss: 0.3655, Test Loss: 1.0018
Epoch: 227, Training Loss: 0.3639, Test Loss: 0.9943
Epoch: 228, Training Loss: 0.3635, Test Loss: 0.9896
Epoch: 229, Training Loss: 0.3625, Test Loss: 0.9982
Epoch: 230, Training Loss: 0.3598, Test Loss: 0.9930
Epoch: 231, Training Loss: 0.3579, Test Loss: 0.9977
Epoch: 232, Training Loss: 0.3571, Test Loss: 0.9907
Epoch: 233, Training Loss: 0.3557, Test Loss: 1.0041
Epoch: 234, Training Loss: 0.3529, Test Loss: 0.9970
Epoch: 235, Training Loss: 0.3508, Test Loss: 0.9934
Epoch: 236, Training Loss: 0.3499, Test Loss: 1.0075
Epoch: 237, Training Loss: 0.3491, Test Loss: 0.9959
Epoch: 238, Training Loss: 0.3457, Test Loss: 0.9855
Epoch: 239, Training Loss: 0.3448, Test Loss: 1.0040


100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.20it/s, loss_test=1.004]
Epoch: 241, Training Loss: 0.3419, Test Loss: 1.0066
Epoch: 242, Training Loss: 0.3400, Test Loss: 0.9991
Epoch: 243, Training Loss: 0.3390, Test Loss: 1.0050
Epoch: 244, Training Loss: 0.3386, Test Loss: 1.0102
Epoch: 245, Training Loss: 0.3362, Test Loss: 0.9993
Epoch: 246, Training Loss: 0.3343, Test Loss: 0.9961
Epoch: 247, Training Loss: 0.3318, Test Loss: 1.0068
Epoch: 248, Training Loss: 0.3324, Test Loss: 1.0087
Epoch: 249, Training Loss: 0.3302, Test Loss: 1.0039
Model saved as model_8355834np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-1240000-8355834np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9921870589256286, 0.9923413515090942, 0.9940937399864197, 0.9911876320838928, 0.9959357976913452, 0.9888343811035156, 0.9916165709495545, 0.9913676619529724, 0.993613886833191, 0.9890210032463074, 0.9928730368614197, 0.9836540579795837, 0.9852406859397889, 0.9846724748611451, 0.9869810461997985, 0.9874886870384216, 0.9874998211860657, 0.9836105108261108, 0.9877351760864258, 0.9854578137397766, 0.9836556315422058, 0.9829467296600342, 0.9826921224594116, 0.9768255591392517, 0.9812250018119812, 0.9813278675079345, 0.9748260498046875, 0.9764994144439697, 0.9779314398765564, 0.9744724869728089, 0.9676723957061768, 0.9557531952857972, 0.9509532451629639, 0.9407775282859803, 0.935330617427826, 0.9222996950149536, 0.9211305379867554, 0.9122677445411682, 0.9067682504653931, 0.901506495475769, 0.8985896944999695, 0.8923185229301452, 0.888292396068573, 0.8824117541313171, 0.8795573711395264, 0.8759530544281006, 0.8767110586166382, 0.8661790132522583, 0.8655232429504395, 0.8576963901519775, 0.8555050969123841, 0.8574287533760071, 0.848745834827423, 0.8492985010147095, 0.8442618727684021, 0.8395813941955567, 0.8380921363830567, 0.8386851787567139, 0.8315982937812805, 0.8313361525535583, 0.8300946712493896, 0.8254945158958436, 0.8251189708709716, 0.8240481495857239, 0.8214897632598877, 0.817945909500122, 0.8161089897155762, 0.8148019552230835, 0.813567578792572, 0.807333505153656, 0.8092422604560852, 0.8053701162338257, 0.8013024926185608, 0.8011562466621399, 0.7975434422492981, 0.7955618739128113, 0.7915345311164856, 0.7884662508964538, 0.7890398263931274, 0.7827358245849609, 0.7770222902297974, 0.7740594029426575, 0.7730344653129577, 0.7697594642639161, 0.7691753387451172, 0.7649147987365723, 0.762357234954834, 0.7600169658660889, 0.7544854998588562, 0.7535555958747864, 0.7456357717514038, 0.7460582852363586, 0.7395211577415466, 0.7338430643081665, 0.7322991371154786, 0.7294400691986084, 0.7259815454483032, 0.7243678569793701, 0.7191449642181397, 0.7151137709617614, 0.7121733546257019, 0.7071812391281128, 0.70221426486969, 0.7025604963302612, 0.6960303664207459, 0.6920167922973632, 0.6892150282859802, 0.685082757472992, 0.6799079298973083, 0.6733245730400086, 0.6759423851966858, 0.6648747324943542, 0.6663436651229858, 0.6622819185256958, 0.6586422324180603, 0.6508273243904114, 0.6487260937690735, 0.6445414781570434, 0.6410537719726562, 0.6379472494125367, 0.6333222389221191, 0.6312429785728455, 0.625765860080719, 0.6214680194854736, 0.6166196703910828, 0.615824568271637, 0.6142398238182067, 0.6079194188117981, 0.6050271391868591, 0.6003755569458008, 0.5964015841484069, 0.5942317605018616, 0.5899800062179565, 0.587790060043335, 0.5866963982582092, 0.5807582974433899, 0.5795985102653504, 0.5748335242271423, 0.573533833026886, 0.5684681892395019, 0.5679482340812683, 0.5641397118568421, 0.5595303058624268, 0.5559366822242737, 0.5539944052696228, 0.5513500809669495, 0.550345242023468, 0.5448113203048706, 0.5433688759803772, 0.5397111296653747, 0.5338839650154114, 0.5349390745162964, 0.5321722984313965, 0.5274223923683167, 0.5234158873558045, 0.5218993425369263, 0.5190996050834655, 0.517248010635376, 0.5151244640350342, 0.5106909155845643, 0.5101937413215637, 0.5069291591644287, 0.5034676969051362, 0.5009857535362243, 0.49770992398262026, 0.49594504833221437, 0.49458463191986085, 0.4901576697826385, 0.4883084177970886, 0.48719645142555235, 0.48214778304100037, 0.481164824962616, 0.47929545044898986, 0.475724595785141, 0.4739773809909821, 0.4716849267482758, 0.4681636393070221, 0.46483526229858396, 0.4651951014995575, 0.46002321839332583, 0.4602506637573242, 0.4573123812675476, 0.45477780103683474, 0.45316908955574037, 0.45019729137420655, 0.448588490486145, 0.44644981622695923, 0.4422825872898102, 0.4402307033538818, 0.4386390268802643, 0.4371196568012238, 0.4328671991825104, 0.43240171670913696, 0.43183847069740294, 0.428479278087616, 0.42622857093811034, 0.4252890944480896, 0.4222292363643646, 0.41938501596450806, 0.41801475286483764, 0.4169488072395325, 0.4108861982822418, 0.4119885087013245, 0.40992227792739866, 0.4070994734764099, 0.4072893917560577, 0.40261263847351075, 0.4013863027095795, 0.4006847381591797, 0.3961651623249054, 0.39603187441825866, 0.39597479104995725, 0.39047645926475527, 0.3898144543170929, 0.3891567587852478, 0.38524065017700193, 0.3855091392993927, 0.38386501669883727, 0.38115122318267824, 0.3800372242927551, 0.37785791158676146, 0.3760377883911133, 0.37446919083595276, 0.3718284904956818, 0.37062429189682006, 0.3688069641590118, 0.365542471408844, 0.36390005350112914, 0.36345542669296266, 0.36248011589050294, 0.35978569388389586, 0.35785691142082215, 0.3571060478687286, 0.35566036105155946, 0.35292301774024964, 0.35082952976226806, 0.34989518523216245, 0.34905062317848207, 0.3456955015659332, 0.3448130488395691, 0.34547858834266665, 0.341902357339859, 0.3399662017822266, 0.3389525830745697, 0.33862248063087463, 0.3362079083919525, 0.3342969536781311, 0.3318274438381195, 0.33242828249931333, 0.3301726520061493], 'loss_test': [1.056186318397522, 1.0653846263885498, 1.0774229764938354, 1.082306146621704, 1.0761090517044067, 1.0829522609710693, 1.0659904479980469, 1.087356448173523, 1.0864603519439697, 1.0782043933868408, 1.0760356187820435, 1.0808666944503784, 1.0704692602157593, 1.0740573406219482, 1.066310167312622, 1.0745819807052612, 1.0668773651123047, 1.0833333730697632, 1.071797251701355, 1.083644151687622, 1.0644718408584595, 1.0994338989257812, 1.0855052471160889, 1.0869320631027222, 1.066303014755249, 1.089496374130249, 1.0863056182861328, 1.073286771774292, 1.0872554779052734, 1.089247703552246, 1.0753799676895142, 1.079975962638855, 1.0665267705917358, 1.0560089349746704, 1.036874532699585, 1.0240205526351929, 1.0420689582824707, 1.0258421897888184, 1.0045164823532104, 1.0069032907485962, 1.010084867477417, 0.9781439900398254, 0.9912056922912598, 0.9884233474731445, 0.9718296527862549, 0.9782977104187012, 0.963304340839386, 0.9797621369361877, 0.9752897620201111, 0.9738681316375732, 0.9657110571861267, 0.9770969152450562, 0.9662719964981079, 0.9774098992347717, 0.9571014046669006, 0.958380401134491, 0.9491299390792847, 0.9582216143608093, 0.9380016922950745, 0.9425256848335266, 0.9397227764129639, 0.9543361663818359, 0.9443922638893127, 0.9176034331321716, 0.9357017874717712, 0.9512903094291687, 0.9287941455841064, 0.9354470372200012, 0.9172229766845703, 0.9261634349822998, 0.942153811454773, 0.92691570520401, 0.9322091937065125, 0.9183380603790283, 0.9308283925056458, 0.9381667375564575, 0.9254859089851379, 0.9281592965126038, 0.9284412264823914, 0.9152908325195312, 0.930084764957428, 0.9156484007835388, 0.9113034009933472, 0.9130191802978516, 0.9138209223747253, 0.9231764078140259, 0.913316547870636, 0.8980772495269775, 0.9138849973678589, 0.9109318256378174, 0.9138703346252441, 0.9103218913078308, 0.9112691283226013, 0.8954264521598816, 0.9039059281349182, 0.9136688709259033, 0.8965896964073181, 0.894761860370636, 0.8970733284950256, 0.8888208270072937, 0.8931617736816406, 0.8997732400894165, 0.8937134742736816, 0.8981288075447083, 0.8903376460075378, 0.8993433117866516, 0.877685546875, 0.8982993364334106, 0.9057959914207458, 0.8837121725082397, 0.9025009274482727, 0.8988193869590759, 0.8798865675926208, 0.9055460691452026, 0.8985028862953186, 0.9017274975776672, 0.8926204442977905, 0.900801956653595, 0.9053958654403687, 0.8915120363235474, 0.8965640068054199, 0.9003143310546875, 0.8981127738952637, 0.8978782296180725, 0.8929428458213806, 0.9137177467346191, 0.8969178199768066, 0.8956258296966553, 0.8942248821258545, 0.9043360352516174, 0.896530032157898, 0.9050961136817932, 0.9071423411369324, 0.8946707248687744, 0.8968284130096436, 0.9038288593292236, 0.9112517833709717, 0.9072167277336121, 0.9025026559829712, 0.9149021506309509, 0.9089702367782593, 0.9219363927841187, 0.9068232178688049, 0.9118865132331848, 0.8980857133865356, 0.9147427082061768, 0.9179704785346985, 0.9221218824386597, 0.9147326946258545, 0.9149647355079651, 0.9093551635742188, 0.9201123118400574, 0.9240188598632812, 0.928718090057373, 0.9439809322357178, 0.9376921057701111, 0.925610363483429, 0.9291812777519226, 0.9294555187225342, 0.9214433431625366, 0.9402248859405518, 0.9319632649421692, 0.9359161853790283, 0.9311915040016174, 0.938599169254303, 0.9492606520652771, 0.9456549882888794, 0.9475793242454529, 0.9484747648239136, 0.9376204609870911, 0.9347826242446899, 0.9497816562652588, 0.9471449255943298, 0.9462783932685852, 0.9452197551727295, 0.9557051062583923, 0.9531038403511047, 0.9545010924339294, 0.9500316977500916, 0.9571250081062317, 0.9554940462112427, 0.9441160559654236, 0.9566872715950012, 0.959927499294281, 0.9596788287162781, 0.9563654661178589, 0.9567528963088989, 0.948441743850708, 0.9606914520263672, 0.9673541188240051, 0.967758297920227, 0.9602594971656799, 0.9669235348701477, 0.9633665680885315, 0.9756878614425659, 0.9682244658470154, 0.9648047685623169, 0.9696975946426392, 0.9635852575302124, 0.9707021117210388, 0.9774166941642761, 0.9680227041244507, 0.9699591994285583, 0.9712245464324951, 0.9866597652435303, 0.9761656522750854, 0.9720583558082581, 0.9713847041130066, 0.9912869334220886, 0.9787428379058838, 0.989013671875, 0.9778648018836975, 0.9653800129890442, 0.9849674105644226, 0.9887040853500366, 0.9723588824272156, 0.9845070838928223, 0.9836264848709106, 0.998955249786377, 0.9903298616409302, 0.9853103756904602, 0.987896740436554, 0.9931043982505798, 0.9885858297348022, 0.9791620969772339, 1.0037031173706055, 1.0018278360366821, 0.9943098425865173, 0.9896496534347534, 0.9982156753540039, 0.9929710030555725, 0.9976720213890076, 0.9907455444335938, 1.0041242837905884, 0.9970269203186035, 0.9934168457984924, 1.0075139999389648, 0.9958626627922058, 0.9855348467826843, 1.0040323734283447, 0.999929666519165, 1.006569743156433, 0.9991074800491333, 1.0050021409988403, 1.0101819038391113, 0.9992795586585999, 0.9961209297180176, 1.0068094730377197, 1.0087437629699707, 1.003887414932251], 'identifier': '8355834np'}