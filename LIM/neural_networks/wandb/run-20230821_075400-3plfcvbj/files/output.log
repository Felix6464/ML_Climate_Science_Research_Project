
  8%|███████▊                                                                                          | 20/250 [00:01<00:19, 11.85it/s, loss_test=1.076]
Epoch: 00, Training Loss: 0.9936, Test Loss: 1.0987
Epoch: 01, Training Loss: 0.9963, Test Loss: 1.0835
Epoch: 02, Training Loss: 0.9883, Test Loss: 1.0802
Epoch: 03, Training Loss: 0.9945, Test Loss: 1.1011
Epoch: 04, Training Loss: 0.9911, Test Loss: 1.0913
Epoch: 05, Training Loss: 0.9929, Test Loss: 1.0674
Epoch: 06, Training Loss: 0.9907, Test Loss: 1.0933
Epoch: 07, Training Loss: 0.9886, Test Loss: 1.0787
Epoch: 08, Training Loss: 0.9883, Test Loss: 1.0912
Epoch: 09, Training Loss: 0.9922, Test Loss: 1.1043
Epoch: 10, Training Loss: 0.9835, Test Loss: 1.0726
Epoch: 11, Training Loss: 0.9897, Test Loss: 1.0879
Epoch: 12, Training Loss: 0.9894, Test Loss: 1.0850
Epoch: 13, Training Loss: 0.9904, Test Loss: 1.0839
Epoch: 14, Training Loss: 0.9865, Test Loss: 1.0970
Epoch: 15, Training Loss: 0.9833, Test Loss: 1.0849
Epoch: 16, Training Loss: 0.9859, Test Loss: 1.0892
Epoch: 17, Training Loss: 0.9879, Test Loss: 1.0742
Epoch: 18, Training Loss: 0.9864, Test Loss: 1.0807
Epoch: 19, Training Loss: 0.9873, Test Loss: 1.0686

 18%|██████████████████                                                                                | 46/250 [00:03<00:16, 12.42it/s, loss_test=0.997]
Epoch: 21, Training Loss: 0.9829, Test Loss: 1.0681
Epoch: 22, Training Loss: 0.9828, Test Loss: 1.0671
Epoch: 23, Training Loss: 0.9856, Test Loss: 1.0929
Epoch: 24, Training Loss: 0.9809, Test Loss: 1.0858
Epoch: 25, Training Loss: 0.9780, Test Loss: 1.0906
Epoch: 26, Training Loss: 0.9817, Test Loss: 1.0919
Epoch: 27, Training Loss: 0.9790, Test Loss: 1.1016
Epoch: 28, Training Loss: 0.9834, Test Loss: 1.0875
Epoch: 29, Training Loss: 0.9757, Test Loss: 1.0781
Epoch: 30, Training Loss: 0.9791, Test Loss: 1.0563
Epoch: 31, Training Loss: 0.9725, Test Loss: 1.0852
Epoch: 32, Training Loss: 0.9688, Test Loss: 1.0758
Epoch: 33, Training Loss: 0.9693, Test Loss: 1.1037
Epoch: 34, Training Loss: 0.9653, Test Loss: 1.0791
Epoch: 35, Training Loss: 0.9559, Test Loss: 1.0789
Epoch: 36, Training Loss: 0.9476, Test Loss: 1.0664
Epoch: 37, Training Loss: 0.9479, Test Loss: 1.0558
Epoch: 38, Training Loss: 0.9444, Test Loss: 1.0645
Epoch: 39, Training Loss: 0.9356, Test Loss: 1.0506
Epoch: 40, Training Loss: 0.9321, Test Loss: 1.0375
Epoch: 41, Training Loss: 0.9250, Test Loss: 1.0389
Epoch: 42, Training Loss: 0.9202, Test Loss: 1.0262
Epoch: 43, Training Loss: 0.9078, Test Loss: 1.0144
Epoch: 44, Training Loss: 0.9072, Test Loss: 1.0108
Epoch: 45, Training Loss: 0.9001, Test Loss: 0.9972
Epoch: 46, Training Loss: 0.8967, Test Loss: 0.9957
Epoch: 47, Training Loss: 0.8922, Test Loss: 0.9857
Epoch: 48, Training Loss: 0.8839, Test Loss: 0.9890
Epoch: 49, Training Loss: 0.8799, Test Loss: 0.9785
Epoch: 50, Training Loss: 0.8763, Test Loss: 0.9869
Epoch: 51, Training Loss: 0.8688, Test Loss: 0.9802
Epoch: 52, Training Loss: 0.8655, Test Loss: 0.9643
Epoch: 53, Training Loss: 0.8599, Test Loss: 0.9699
Epoch: 54, Training Loss: 0.8568, Test Loss: 0.9760
Epoch: 55, Training Loss: 0.8506, Test Loss: 0.9593
Epoch: 56, Training Loss: 0.8479, Test Loss: 0.9536
Epoch: 57, Training Loss: 0.8426, Test Loss: 0.9550
Epoch: 58, Training Loss: 0.8367, Test Loss: 0.9595
Epoch: 59, Training Loss: 0.8357, Test Loss: 0.9536
Epoch: 60, Training Loss: 0.8293, Test Loss: 0.9502
Epoch: 61, Training Loss: 0.8251, Test Loss: 0.9446
Epoch: 62, Training Loss: 0.8183, Test Loss: 0.9506
Epoch: 63, Training Loss: 0.8173, Test Loss: 0.9427
Epoch: 64, Training Loss: 0.8142, Test Loss: 0.9401
Epoch: 65, Training Loss: 0.8084, Test Loss: 0.9412
Epoch: 66, Training Loss: 0.8055, Test Loss: 0.9417
Epoch: 67, Training Loss: 0.7981, Test Loss: 0.9363
Epoch: 68, Training Loss: 0.7949, Test Loss: 0.9285

 28%|███████████████████████████▍                                                                      | 70/250 [00:05<00:15, 11.83it/s, loss_test=0.928]
Epoch: 70, Training Loss: 0.7847, Test Loss: 0.9290
Epoch: 71, Training Loss: 0.7802, Test Loss: 0.9282
Epoch: 72, Training Loss: 0.7779, Test Loss: 0.9228
Epoch: 73, Training Loss: 0.7730, Test Loss: 0.9315
Epoch: 74, Training Loss: 0.7672, Test Loss: 0.9245
Epoch: 75, Training Loss: 0.7643, Test Loss: 0.9125
Epoch: 76, Training Loss: 0.7585, Test Loss: 0.9212
Epoch: 77, Training Loss: 0.7526, Test Loss: 0.9155
Epoch: 78, Training Loss: 0.7461, Test Loss: 0.9166
Epoch: 79, Training Loss: 0.7452, Test Loss: 0.9334
Epoch: 80, Training Loss: 0.7432, Test Loss: 0.9217
Epoch: 81, Training Loss: 0.7383, Test Loss: 0.9259
Epoch: 82, Training Loss: 0.7333, Test Loss: 0.9377
Epoch: 83, Training Loss: 0.7279, Test Loss: 0.9261
Epoch: 84, Training Loss: 0.7252, Test Loss: 0.9198
Epoch: 85, Training Loss: 0.7189, Test Loss: 0.9125
Epoch: 86, Training Loss: 0.7145, Test Loss: 0.9137
Epoch: 87, Training Loss: 0.7118, Test Loss: 0.9215
Epoch: 88, Training Loss: 0.7056, Test Loss: 0.9164
Epoch: 89, Training Loss: 0.7026, Test Loss: 0.9143
Epoch: 90, Training Loss: 0.6988, Test Loss: 0.9095
Epoch: 91, Training Loss: 0.6984, Test Loss: 0.9077
Epoch: 92, Training Loss: 0.6907, Test Loss: 0.9189

 38%|████████████████████████████████████▊                                                             | 94/250 [00:07<00:12, 12.29it/s, loss_test=0.900]
Epoch: 94, Training Loss: 0.6817, Test Loss: 0.9071
Epoch: 95, Training Loss: 0.6766, Test Loss: 0.9138
Epoch: 96, Training Loss: 0.6747, Test Loss: 0.9128
Epoch: 97, Training Loss: 0.6706, Test Loss: 0.9212
Epoch: 98, Training Loss: 0.6678, Test Loss: 0.9144
Epoch: 99, Training Loss: 0.6635, Test Loss: 0.9095
Epoch: 100, Training Loss: 0.6621, Test Loss: 0.9034
Epoch: 101, Training Loss: 0.6553, Test Loss: 0.9082
Epoch: 102, Training Loss: 0.6523, Test Loss: 0.9092
Epoch: 103, Training Loss: 0.6510, Test Loss: 0.9031
Epoch: 104, Training Loss: 0.6446, Test Loss: 0.9162
Epoch: 105, Training Loss: 0.6419, Test Loss: 0.9151
Epoch: 106, Training Loss: 0.6364, Test Loss: 0.9027
Epoch: 107, Training Loss: 0.6355, Test Loss: 0.9161
Epoch: 108, Training Loss: 0.6330, Test Loss: 0.8950
Epoch: 109, Training Loss: 0.6295, Test Loss: 0.8987
Epoch: 110, Training Loss: 0.6275, Test Loss: 0.9003
Epoch: 111, Training Loss: 0.6246, Test Loss: 0.9095
Epoch: 112, Training Loss: 0.6182, Test Loss: 0.9048
Epoch: 113, Training Loss: 0.6165, Test Loss: 0.9071
Epoch: 114, Training Loss: 0.6127, Test Loss: 0.9078
Epoch: 115, Training Loss: 0.6090, Test Loss: 0.9144
Epoch: 116, Training Loss: 0.6045, Test Loss: 0.9134

 47%|█████████████████████████████████████████████▊                                                   | 118/250 [00:09<00:10, 12.18it/s, loss_test=0.917]
Epoch: 118, Training Loss: 0.6017, Test Loss: 0.9073
Epoch: 119, Training Loss: 0.5986, Test Loss: 0.9132
Epoch: 120, Training Loss: 0.5939, Test Loss: 0.9158
Epoch: 121, Training Loss: 0.5904, Test Loss: 0.9076
Epoch: 122, Training Loss: 0.5880, Test Loss: 0.8815
Epoch: 123, Training Loss: 0.5838, Test Loss: 0.9090
Epoch: 124, Training Loss: 0.5806, Test Loss: 0.9028
Epoch: 125, Training Loss: 0.5812, Test Loss: 0.9031
Epoch: 126, Training Loss: 0.5779, Test Loss: 0.9080
Epoch: 127, Training Loss: 0.5732, Test Loss: 0.8998
Epoch: 128, Training Loss: 0.5697, Test Loss: 0.9126
Epoch: 129, Training Loss: 0.5667, Test Loss: 0.9050
Epoch: 130, Training Loss: 0.5664, Test Loss: 0.9086
Epoch: 131, Training Loss: 0.5607, Test Loss: 0.9093
Epoch: 132, Training Loss: 0.5584, Test Loss: 0.9176
Epoch: 133, Training Loss: 0.5556, Test Loss: 0.9051
Epoch: 134, Training Loss: 0.5532, Test Loss: 0.9069
Epoch: 135, Training Loss: 0.5504, Test Loss: 0.9052
Epoch: 136, Training Loss: 0.5468, Test Loss: 0.9023
Epoch: 137, Training Loss: 0.5435, Test Loss: 0.9123
Epoch: 138, Training Loss: 0.5425, Test Loss: 0.9102
Epoch: 139, Training Loss: 0.5404, Test Loss: 0.9170
Epoch: 140, Training Loss: 0.5364, Test Loss: 0.9009
Epoch: 141, Training Loss: 0.5330, Test Loss: 0.9059

 57%|███████████████████████████████████████████████████████                                          | 142/250 [00:11<00:08, 12.20it/s, loss_test=0.903]
Epoch: 143, Training Loss: 0.5273, Test Loss: 0.9047
Epoch: 144, Training Loss: 0.5247, Test Loss: 0.9022
Epoch: 145, Training Loss: 0.5209, Test Loss: 0.9108
Epoch: 146, Training Loss: 0.5207, Test Loss: 0.9157
Epoch: 147, Training Loss: 0.5169, Test Loss: 0.9251
Epoch: 148, Training Loss: 0.5142, Test Loss: 0.9106
Epoch: 149, Training Loss: 0.5113, Test Loss: 0.9176
Epoch: 150, Training Loss: 0.5086, Test Loss: 0.9117
Epoch: 151, Training Loss: 0.5093, Test Loss: 0.9082
Epoch: 152, Training Loss: 0.5048, Test Loss: 0.9143
Epoch: 153, Training Loss: 0.5028, Test Loss: 0.9255
Epoch: 154, Training Loss: 0.4972, Test Loss: 0.9165
Epoch: 155, Training Loss: 0.4963, Test Loss: 0.9119
Epoch: 156, Training Loss: 0.4938, Test Loss: 0.9049
Epoch: 157, Training Loss: 0.4923, Test Loss: 0.9170
Epoch: 158, Training Loss: 0.4899, Test Loss: 0.9177
Epoch: 159, Training Loss: 0.4862, Test Loss: 0.9144
Epoch: 160, Training Loss: 0.4842, Test Loss: 0.9183
Epoch: 161, Training Loss: 0.4794, Test Loss: 0.9246
Epoch: 162, Training Loss: 0.4784, Test Loss: 0.9122
Epoch: 163, Training Loss: 0.4761, Test Loss: 0.9373
Epoch: 164, Training Loss: 0.4739, Test Loss: 0.9260
Epoch: 165, Training Loss: 0.4708, Test Loss: 0.9217
Epoch: 166, Training Loss: 0.4688, Test Loss: 0.9357

 67%|█████████████████████████████████████████████████████████████████▏                               | 168/250 [00:13<00:06, 12.47it/s, loss_test=0.938]
Epoch: 168, Training Loss: 0.4633, Test Loss: 0.9249
Epoch: 169, Training Loss: 0.4618, Test Loss: 0.9405
Epoch: 170, Training Loss: 0.4580, Test Loss: 0.9214
Epoch: 171, Training Loss: 0.4557, Test Loss: 0.9305
Epoch: 172, Training Loss: 0.4544, Test Loss: 0.9296
Epoch: 173, Training Loss: 0.4506, Test Loss: 0.9344
Epoch: 174, Training Loss: 0.4483, Test Loss: 0.9389
Epoch: 175, Training Loss: 0.4451, Test Loss: 0.9384
Epoch: 176, Training Loss: 0.4457, Test Loss: 0.9365
Epoch: 177, Training Loss: 0.4412, Test Loss: 0.9481
Epoch: 178, Training Loss: 0.4403, Test Loss: 0.9431
Epoch: 179, Training Loss: 0.4388, Test Loss: 0.9385

 72%|█████████████████████████████████████████████████████████████████████▊                           | 180/250 [00:14<00:05, 11.93it/s, loss_test=0.939]
Epoch: 181, Training Loss: 0.4325, Test Loss: 0.9433
Epoch: 182, Training Loss: 0.4300, Test Loss: 0.9583
Epoch: 183, Training Loss: 0.4291, Test Loss: 0.9362
Epoch: 184, Training Loss: 0.4271, Test Loss: 0.9514
Epoch: 185, Training Loss: 0.4232, Test Loss: 0.9614
Epoch: 186, Training Loss: 0.4226, Test Loss: 0.9466
Epoch: 187, Training Loss: 0.4198, Test Loss: 0.9449
Epoch: 188, Training Loss: 0.4174, Test Loss: 0.9571
Epoch: 189, Training Loss: 0.4163, Test Loss: 0.9398
Epoch: 190, Training Loss: 0.4130, Test Loss: 0.9550
Epoch: 191, Training Loss: 0.4124, Test Loss: 0.9539
Epoch: 192, Training Loss: 0.4096, Test Loss: 0.9669
Epoch: 193, Training Loss: 0.4060, Test Loss: 0.9502
Epoch: 194, Training Loss: 0.4045, Test Loss: 0.9585
Epoch: 195, Training Loss: 0.4033, Test Loss: 0.9508
Epoch: 196, Training Loss: 0.3997, Test Loss: 0.9606
Epoch: 197, Training Loss: 0.3983, Test Loss: 0.9653
Epoch: 198, Training Loss: 0.3976, Test Loss: 0.9528
Epoch: 199, Training Loss: 0.3936, Test Loss: 0.9665
Epoch: 200, Training Loss: 0.3928, Test Loss: 0.9517
Epoch: 201, Training Loss: 0.3923, Test Loss: 0.9698
Epoch: 202, Training Loss: 0.3890, Test Loss: 0.9655
Epoch: 203, Training Loss: 0.3876, Test Loss: 0.9606
Epoch: 204, Training Loss: 0.3856, Test Loss: 0.9767
Epoch: 205, Training Loss: 0.3841, Test Loss: 0.9753
Epoch: 206, Training Loss: 0.3823, Test Loss: 0.9674
Epoch: 207, Training Loss: 0.3796, Test Loss: 0.9589
Epoch: 208, Training Loss: 0.3781, Test Loss: 0.9573
Epoch: 209, Training Loss: 0.3750, Test Loss: 0.9647
Epoch: 210, Training Loss: 0.3737, Test Loss: 0.9810
Epoch: 211, Training Loss: 0.3730, Test Loss: 0.9833
Epoch: 212, Training Loss: 0.3694, Test Loss: 0.9734
Epoch: 213, Training Loss: 0.3681, Test Loss: 0.9743
Epoch: 214, Training Loss: 0.3657, Test Loss: 0.9747

 86%|███████████████████████████████████████████████████████████████████████████████████▊             | 216/250 [00:17<00:02, 11.84it/s, loss_test=0.975]
Epoch: 216, Training Loss: 0.3623, Test Loss: 0.9616
Epoch: 217, Training Loss: 0.3618, Test Loss: 0.9812
Epoch: 218, Training Loss: 0.3597, Test Loss: 0.9925
Epoch: 219, Training Loss: 0.3578, Test Loss: 0.9829
Epoch: 220, Training Loss: 0.3553, Test Loss: 0.9818
Epoch: 221, Training Loss: 0.3555, Test Loss: 0.9841
Epoch: 222, Training Loss: 0.3532, Test Loss: 0.9909
Epoch: 223, Training Loss: 0.3505, Test Loss: 0.9874
Epoch: 224, Training Loss: 0.3490, Test Loss: 0.9761
Epoch: 225, Training Loss: 0.3473, Test Loss: 0.9952
Epoch: 226, Training Loss: 0.3457, Test Loss: 0.9797
Epoch: 227, Training Loss: 0.3435, Test Loss: 0.9884
Epoch: 228, Training Loss: 0.3419, Test Loss: 0.9791
Epoch: 229, Training Loss: 0.3413, Test Loss: 0.9869
Epoch: 230, Training Loss: 0.3393, Test Loss: 0.9825
Epoch: 231, Training Loss: 0.3390, Test Loss: 0.9923
Epoch: 232, Training Loss: 0.3357, Test Loss: 0.9963
Epoch: 233, Training Loss: 0.3344, Test Loss: 0.9930
Epoch: 234, Training Loss: 0.3321, Test Loss: 0.9822
Epoch: 235, Training Loss: 0.3315, Test Loss: 1.0005
Epoch: 236, Training Loss: 0.3300, Test Loss: 0.9878
Epoch: 237, Training Loss: 0.3292, Test Loss: 0.9971
Epoch: 238, Training Loss: 0.3274, Test Loss: 0.9954
Epoch: 239, Training Loss: 0.3251, Test Loss: 0.9983


100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.09it/s, loss_test=1.009]
Epoch: 241, Training Loss: 0.3218, Test Loss: 0.9977
Epoch: 242, Training Loss: 0.3204, Test Loss: 1.0089
Epoch: 243, Training Loss: 0.3191, Test Loss: 0.9953
Epoch: 244, Training Loss: 0.3176, Test Loss: 1.0076
Epoch: 245, Training Loss: 0.3164, Test Loss: 0.9991
Epoch: 246, Training Loss: 0.3155, Test Loss: 1.0005
Epoch: 247, Training Loss: 0.3138, Test Loss: 1.0189
Epoch: 248, Training Loss: 0.3112, Test Loss: 0.9950
Epoch: 249, Training Loss: 0.3109, Test Loss: 1.0087
Model saved as model_898446np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-1220000-898446np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9936257362365722, 0.9963206887245178, 0.9883338451385498, 0.9944998621940613, 0.9910872340202331, 0.9929442524909973, 0.9907197237014771, 0.9885906100273132, 0.9882511019706726, 0.9922128319740295, 0.9834826946258545, 0.9896963596343994, 0.9893814086914062, 0.9904388785362244, 0.9864706158638, 0.9832742810249329, 0.9859495162963867, 0.987927520275116, 0.9864124655723572, 0.9872866272926331, 0.9862551808357238, 0.9828750014305114, 0.9828245759010314, 0.9856059074401855, 0.9808586239814758, 0.9780380129814148, 0.9816861867904663, 0.978988778591156, 0.9833542227745056, 0.9756579518318176, 0.979106318950653, 0.9724922180175781, 0.9687812805175782, 0.9692688941955566, 0.9652665019035339, 0.955921185016632, 0.9475975036621094, 0.9478915810585022, 0.944365668296814, 0.9355515599250793, 0.9321238398551941, 0.9249955534934997, 0.9202468037605286, 0.9077799916267395, 0.9072343468666076, 0.9001052498817443, 0.8967085361480713, 0.8921503782272339, 0.8839059233665466, 0.8798763394355774, 0.8763460516929626, 0.8688151836395264, 0.8655272960662842, 0.8599460244178772, 0.8568355441093445, 0.8506473183631897, 0.8478801488876343, 0.8425807476043701, 0.8366852283477784, 0.835701835155487, 0.829265010356903, 0.8251363039016724, 0.8183305025100708, 0.8173217892646789, 0.8141712307929992, 0.808356761932373, 0.8054845929145813, 0.7981413125991821, 0.7949082970619201, 0.7906043410301209, 0.7847392439842225, 0.7802186965942383, 0.7778912901878356, 0.7730353116989136, 0.7672267079353332, 0.7642664551734925, 0.7585046768188477, 0.7525794982910157, 0.7460646986961365, 0.7452196717262268, 0.7431680202484131, 0.7382792592048645, 0.7333154320716858, 0.7279012799263, 0.7252095103263855, 0.7188874006271362, 0.7145043969154358, 0.7117702960968018, 0.7055573463439941, 0.7025981187820435, 0.6988296985626221, 0.698380982875824, 0.6907105326652527, 0.6852933764457703, 0.6817167162895202, 0.6766376256942749, 0.6746531844139099, 0.6706125020980835, 0.667819881439209, 0.6634627938270569, 0.6621057510375976, 0.6552791118621826, 0.6523433446884155, 0.6510098934173584, 0.6446343541145325, 0.6419197678565979, 0.6364304184913635, 0.6354708194732666, 0.6330117225646973, 0.6295424103736877, 0.6275325536727905, 0.6245806097984314, 0.6181627154350281, 0.6165313482284546, 0.6127159833908081, 0.6090181589126586, 0.6045123934745789, 0.6025980830192565, 0.6017484545707703, 0.5986373066902161, 0.5939009428024292, 0.5903692483901978, 0.5880354285240174, 0.5837671875953674, 0.5806349277496338, 0.5811774373054505, 0.5778819918632507, 0.5732411503791809, 0.5697279572486877, 0.5666625380516053, 0.5664140820503235, 0.5606740355491638, 0.5583616852760315, 0.55563884973526, 0.5532118082046509, 0.5504231333732605, 0.5468259215354919, 0.5435112714767456, 0.5424616694450378, 0.5403650283813477, 0.5364374637603759, 0.5330230832099915, 0.5296192646026612, 0.5273459672927856, 0.5246968626976013, 0.5209235787391663, 0.5207013010978698, 0.5169059872627259, 0.5141790866851806, 0.511338472366333, 0.5085592269897461, 0.5093336522579193, 0.5047649800777435, 0.5028315544128418, 0.49715731143951414, 0.49625024795532224, 0.4937947392463684, 0.49231027364730834, 0.4899111151695251, 0.48618919849395753, 0.4841994345188141, 0.4793557345867157, 0.4783812820911407, 0.47609835863113403, 0.473864471912384, 0.47083778977394103, 0.46880221366882324, 0.46667069792747495, 0.463309246301651, 0.46182205677032473, 0.45795966386795045, 0.45565893650054934, 0.4543642520904541, 0.4505503475666046, 0.4482657313346863, 0.4451334893703461, 0.4456734836101532, 0.4412160456180573, 0.44033005833625793, 0.43879416584968567, 0.43582808375358584, 0.43249598145484924, 0.4300452709197998, 0.4291447699069977, 0.42707104682922364, 0.42316007018089297, 0.42264044880867, 0.4198279082775116, 0.41738793849945066, 0.4163150429725647, 0.4129712462425232, 0.4124415457248688, 0.4095661163330078, 0.405973356962204, 0.4045263409614563, 0.40332710146903994, 0.39970908761024476, 0.39826740026474, 0.397643119096756, 0.3935539186000824, 0.39280358552932737, 0.39227484464645385, 0.3890278935432434, 0.38756234049797056, 0.38558913469314576, 0.38413684368133544, 0.3823330819606781, 0.37959036231040955, 0.37806824445724485, 0.3750309467315674, 0.3736541271209717, 0.3729957938194275, 0.369366979598999, 0.3681013286113739, 0.3656733989715576, 0.3658619999885559, 0.36228691339492797, 0.3617623448371887, 0.3597159028053284, 0.35778855085372924, 0.3552801191806793, 0.35547266006469724, 0.35315016508102415, 0.3504981517791748, 0.3489517688751221, 0.3473337829113007, 0.345727926492691, 0.3434602618217468, 0.3419053673744202, 0.34127277135849, 0.3393235683441162, 0.33904424905776975, 0.3357331931591034, 0.33435541987419126, 0.3321255028247833, 0.3314800560474396, 0.3299664855003357, 0.32915247678756715, 0.3274001359939575, 0.32509506344795225, 0.3236499488353729, 0.32178929448127747, 0.3203899562358856, 0.31914145946502687, 0.31762759685516356, 0.31638340950012206, 0.3155116319656372, 0.3138110935688019, 0.31119619607925414, 0.31087106466293335], 'loss_test': [1.0986567735671997, 1.0834957361221313, 1.0802068710327148, 1.1011135578155518, 1.091328740119934, 1.0673778057098389, 1.093316674232483, 1.0786943435668945, 1.0912443399429321, 1.104273796081543, 1.0725772380828857, 1.0878881216049194, 1.0849636793136597, 1.0839323997497559, 1.0970097780227661, 1.084917426109314, 1.0892200469970703, 1.0741881132125854, 1.0807373523712158, 1.0686074495315552, 1.0763038396835327, 1.0681159496307373, 1.067113995552063, 1.092864990234375, 1.0858423709869385, 1.090641736984253, 1.091857671737671, 1.1015925407409668, 1.087486743927002, 1.0780941247940063, 1.0563256740570068, 1.085178017616272, 1.0758029222488403, 1.1037383079528809, 1.0791248083114624, 1.0789066553115845, 1.0663909912109375, 1.0557701587677002, 1.0644655227661133, 1.050580382347107, 1.0375221967697144, 1.0388730764389038, 1.0261688232421875, 1.0144075155258179, 1.0107852220535278, 0.9971893429756165, 0.9957495927810669, 0.9857189059257507, 0.9890157580375671, 0.9785244464874268, 0.9868749976158142, 0.9801945686340332, 0.9642696976661682, 0.9699363112449646, 0.9759661555290222, 0.9593284130096436, 0.9535724520683289, 0.9550182819366455, 0.9595292806625366, 0.9535690546035767, 0.9501707553863525, 0.9445663094520569, 0.9505864977836609, 0.9427129626274109, 0.9401236772537231, 0.9411761164665222, 0.9417352080345154, 0.936312735080719, 0.9284895658493042, 0.9275408387184143, 0.9290427565574646, 0.9282245635986328, 0.9228061437606812, 0.9315458536148071, 0.9244766235351562, 0.9124717116355896, 0.9212042093276978, 0.9154634475708008, 0.9165633320808411, 0.9334430694580078, 0.9217222929000854, 0.9259400367736816, 0.9377081990242004, 0.9260790944099426, 0.9198378920555115, 0.9125020503997803, 0.913673996925354, 0.9214665293693542, 0.9163589477539062, 0.9142678380012512, 0.9095472693443298, 0.9076631665229797, 0.9188950657844543, 0.900229811668396, 0.9070950150489807, 0.9138214588165283, 0.9128096699714661, 0.9212112426757812, 0.914375901222229, 0.9094632863998413, 0.9034413695335388, 0.9082063436508179, 0.9091697335243225, 0.9031498432159424, 0.9161894917488098, 0.9150823354721069, 0.9026904702186584, 0.9161223769187927, 0.8949947953224182, 0.898746907711029, 0.9003134965896606, 0.909458339214325, 0.9048182368278503, 0.9070865511894226, 0.9078235626220703, 0.9144328236579895, 0.9134467244148254, 0.916962206363678, 0.9073054194450378, 0.913210928440094, 0.9158344268798828, 0.9076334834098816, 0.8815084099769592, 0.9090423583984375, 0.902755618095398, 0.9030891060829163, 0.9079828858375549, 0.8998199701309204, 0.9125757217407227, 0.9050247669219971, 0.9085995554924011, 0.9092547297477722, 0.9176385998725891, 0.905123233795166, 0.9069231152534485, 0.9052161574363708, 0.9023333191871643, 0.9122825264930725, 0.9101815223693848, 0.9169571995735168, 0.9009431004524231, 0.9058812856674194, 0.9029253125190735, 0.9047456979751587, 0.9021931886672974, 0.9107910990715027, 0.9157152771949768, 0.9251483678817749, 0.910563588142395, 0.9176175594329834, 0.911694347858429, 0.9081743359565735, 0.914332926273346, 0.9254689812660217, 0.9165191054344177, 0.9118515253067017, 0.9048933386802673, 0.9170489311218262, 0.9177267551422119, 0.9144362211227417, 0.9183405637741089, 0.9245734810829163, 0.9121670126914978, 0.9372847676277161, 0.9260208606719971, 0.9217462539672852, 0.9356580972671509, 0.9383131265640259, 0.9249330759048462, 0.9405370950698853, 0.9214165210723877, 0.9304777383804321, 0.9296436309814453, 0.9343831539154053, 0.9388782382011414, 0.9383604526519775, 0.9365324378013611, 0.9480578303337097, 0.9430872797966003, 0.9385022521018982, 0.9386319518089294, 0.9432619214057922, 0.9583312273025513, 0.9361662268638611, 0.9513717889785767, 0.9613898396492004, 0.9465833306312561, 0.9449167251586914, 0.9570885896682739, 0.939823567867279, 0.9549513459205627, 0.9538952708244324, 0.9668589234352112, 0.9501804113388062, 0.9584589600563049, 0.9507628083229065, 0.9605772495269775, 0.9653049111366272, 0.9527530670166016, 0.9664665460586548, 0.9517022371292114, 0.9698025584220886, 0.965456485748291, 0.9605864882469177, 0.9767161011695862, 0.9753429889678955, 0.9673702716827393, 0.9589027762413025, 0.9572898149490356, 0.9646716713905334, 0.981035053730011, 0.9833383560180664, 0.9733602404594421, 0.9743323922157288, 0.9746997356414795, 0.9747958779335022, 0.9615957736968994, 0.9812074899673462, 0.9924567341804504, 0.9828763008117676, 0.9817745685577393, 0.984121561050415, 0.9909486770629883, 0.9873552322387695, 0.9761201739311218, 0.9952452182769775, 0.9797059893608093, 0.9883683919906616, 0.9791391491889954, 0.98694908618927, 0.982452929019928, 0.992300271987915, 0.9962717890739441, 0.9929963946342468, 0.9822281002998352, 1.0005316734313965, 0.9877610206604004, 0.9971307516098022, 0.9953581094741821, 0.9982932209968567, 0.9907932877540588, 0.997748076915741, 1.008910059928894, 0.9953157305717468, 1.0075674057006836, 0.9990944862365723, 1.0004937648773193, 1.018947720527649, 0.9949748516082764, 1.008708119392395], 'identifier': '898446np'}