
  8%|███████▊                                                                                          | 20/250 [00:01<00:19, 12.06it/s, loss_test=1.086]
Epoch: 00, Training Loss: 0.9969, Test Loss: 1.0784
Epoch: 01, Training Loss: 0.9955, Test Loss: 1.0883
Epoch: 02, Training Loss: 0.9899, Test Loss: 1.0778
Epoch: 03, Training Loss: 0.9958, Test Loss: 1.0836
Epoch: 04, Training Loss: 0.9934, Test Loss: 1.0731
Epoch: 05, Training Loss: 0.9883, Test Loss: 1.0964
Epoch: 06, Training Loss: 0.9869, Test Loss: 1.0717
Epoch: 07, Training Loss: 0.9910, Test Loss: 1.0829
Epoch: 08, Training Loss: 0.9902, Test Loss: 1.0874
Epoch: 09, Training Loss: 0.9906, Test Loss: 1.0818
Epoch: 10, Training Loss: 0.9898, Test Loss: 1.0566
Epoch: 11, Training Loss: 0.9861, Test Loss: 1.0926
Epoch: 12, Training Loss: 0.9902, Test Loss: 1.0669
Epoch: 13, Training Loss: 0.9898, Test Loss: 1.0875
Epoch: 14, Training Loss: 0.9858, Test Loss: 1.0693
Epoch: 15, Training Loss: 0.9879, Test Loss: 1.0749
Epoch: 16, Training Loss: 0.9882, Test Loss: 1.0807
Epoch: 17, Training Loss: 0.9901, Test Loss: 1.0661
Epoch: 18, Training Loss: 0.9854, Test Loss: 1.0875

 18%|██████████████████                                                                                | 46/250 [00:03<00:17, 12.00it/s, loss_test=1.028]
Epoch: 20, Training Loss: 0.9850, Test Loss: 1.0861
Epoch: 21, Training Loss: 0.9849, Test Loss: 1.0832
Epoch: 22, Training Loss: 0.9872, Test Loss: 1.0892
Epoch: 23, Training Loss: 0.9900, Test Loss: 1.0838
Epoch: 24, Training Loss: 0.9845, Test Loss: 1.0708
Epoch: 25, Training Loss: 0.9889, Test Loss: 1.0928
Epoch: 26, Training Loss: 0.9784, Test Loss: 1.0939
Epoch: 27, Training Loss: 0.9816, Test Loss: 1.0920
Epoch: 28, Training Loss: 0.9818, Test Loss: 1.0916
Epoch: 29, Training Loss: 0.9801, Test Loss: 1.0772
Epoch: 30, Training Loss: 0.9783, Test Loss: 1.0860
Epoch: 31, Training Loss: 0.9784, Test Loss: 1.0631
Epoch: 32, Training Loss: 0.9742, Test Loss: 1.0772
Epoch: 33, Training Loss: 0.9661, Test Loss: 1.0678
Epoch: 34, Training Loss: 0.9703, Test Loss: 1.0710
Epoch: 35, Training Loss: 0.9620, Test Loss: 1.0816
Epoch: 36, Training Loss: 0.9589, Test Loss: 1.0622
Epoch: 37, Training Loss: 0.9546, Test Loss: 1.0650
Epoch: 38, Training Loss: 0.9492, Test Loss: 1.0594
Epoch: 39, Training Loss: 0.9445, Test Loss: 1.0506
Epoch: 40, Training Loss: 0.9379, Test Loss: 1.0580
Epoch: 41, Training Loss: 0.9300, Test Loss: 1.0454
Epoch: 42, Training Loss: 0.9225, Test Loss: 1.0369
Epoch: 43, Training Loss: 0.9140, Test Loss: 1.0327

 28%|███████████████████████████▍                                                                      | 70/250 [00:05<00:15, 11.90it/s, loss_test=0.934]
Epoch: 45, Training Loss: 0.8957, Test Loss: 1.0277
Epoch: 46, Training Loss: 0.8935, Test Loss: 1.0276
Epoch: 47, Training Loss: 0.8860, Test Loss: 1.0077
Epoch: 48, Training Loss: 0.8808, Test Loss: 1.0323
Epoch: 49, Training Loss: 0.8756, Test Loss: 1.0083
Epoch: 50, Training Loss: 0.8712, Test Loss: 1.0188
Epoch: 51, Training Loss: 0.8656, Test Loss: 1.0004
Epoch: 52, Training Loss: 0.8558, Test Loss: 0.9989
Epoch: 53, Training Loss: 0.8498, Test Loss: 0.9756
Epoch: 54, Training Loss: 0.8453, Test Loss: 0.9803
Epoch: 55, Training Loss: 0.8366, Test Loss: 0.9809
Epoch: 56, Training Loss: 0.8362, Test Loss: 0.9730
Epoch: 57, Training Loss: 0.8305, Test Loss: 0.9601
Epoch: 58, Training Loss: 0.8225, Test Loss: 0.9528
Epoch: 59, Training Loss: 0.8167, Test Loss: 0.9451
Epoch: 60, Training Loss: 0.8130, Test Loss: 0.9402
Epoch: 61, Training Loss: 0.8115, Test Loss: 0.9516
Epoch: 62, Training Loss: 0.8054, Test Loss: 0.9322
Epoch: 63, Training Loss: 0.8038, Test Loss: 0.9332
Epoch: 64, Training Loss: 0.7963, Test Loss: 0.9418
Epoch: 65, Training Loss: 0.7923, Test Loss: 0.9225
Epoch: 66, Training Loss: 0.7828, Test Loss: 0.9308
Epoch: 67, Training Loss: 0.7821, Test Loss: 0.9212

 38%|████████████████████████████████████▊                                                             | 94/250 [00:07<00:12, 12.56it/s, loss_test=0.880]
Epoch: 69, Training Loss: 0.7765, Test Loss: 0.9341
Epoch: 70, Training Loss: 0.7714, Test Loss: 0.9097
Epoch: 71, Training Loss: 0.7632, Test Loss: 0.9150
Epoch: 72, Training Loss: 0.7625, Test Loss: 0.9036
Epoch: 73, Training Loss: 0.7548, Test Loss: 0.9124
Epoch: 74, Training Loss: 0.7446, Test Loss: 0.9062
Epoch: 75, Training Loss: 0.7503, Test Loss: 0.9088
Epoch: 76, Training Loss: 0.7440, Test Loss: 0.8964
Epoch: 77, Training Loss: 0.7394, Test Loss: 0.9127
Epoch: 78, Training Loss: 0.7351, Test Loss: 0.8924
Epoch: 79, Training Loss: 0.7317, Test Loss: 0.8835
Epoch: 80, Training Loss: 0.7272, Test Loss: 0.9048
Epoch: 81, Training Loss: 0.7230, Test Loss: 0.9029
Epoch: 82, Training Loss: 0.7163, Test Loss: 0.8908
Epoch: 83, Training Loss: 0.7114, Test Loss: 0.8841
Epoch: 84, Training Loss: 0.7062, Test Loss: 0.8837
Epoch: 85, Training Loss: 0.7060, Test Loss: 0.8880
Epoch: 86, Training Loss: 0.7019, Test Loss: 0.8922
Epoch: 87, Training Loss: 0.7000, Test Loss: 0.8857
Epoch: 88, Training Loss: 0.6947, Test Loss: 0.8901
Epoch: 89, Training Loss: 0.6914, Test Loss: 0.8886
Epoch: 90, Training Loss: 0.6830, Test Loss: 0.8896
Epoch: 91, Training Loss: 0.6824, Test Loss: 0.8729
Epoch: 92, Training Loss: 0.6793, Test Loss: 0.8920

 47%|█████████████████████████████████████████████▊                                                   | 118/250 [00:09<00:11, 11.84it/s, loss_test=0.905]
Epoch: 94, Training Loss: 0.6712, Test Loss: 0.8798
Epoch: 95, Training Loss: 0.6670, Test Loss: 0.8770
Epoch: 96, Training Loss: 0.6647, Test Loss: 0.8859
Epoch: 97, Training Loss: 0.6621, Test Loss: 0.8830
Epoch: 98, Training Loss: 0.6542, Test Loss: 0.8868
Epoch: 99, Training Loss: 0.6525, Test Loss: 0.8828
Epoch: 100, Training Loss: 0.6500, Test Loss: 0.8896
Epoch: 101, Training Loss: 0.6464, Test Loss: 0.8877
Epoch: 102, Training Loss: 0.6411, Test Loss: 0.8884
Epoch: 103, Training Loss: 0.6373, Test Loss: 0.8828
Epoch: 104, Training Loss: 0.6344, Test Loss: 0.8711
Epoch: 105, Training Loss: 0.6344, Test Loss: 0.8930
Epoch: 106, Training Loss: 0.6306, Test Loss: 0.8901
Epoch: 107, Training Loss: 0.6290, Test Loss: 0.8861
Epoch: 108, Training Loss: 0.6251, Test Loss: 0.8952
Epoch: 109, Training Loss: 0.6225, Test Loss: 0.8992
Epoch: 110, Training Loss: 0.6154, Test Loss: 0.8892
Epoch: 111, Training Loss: 0.6163, Test Loss: 0.9004
Epoch: 112, Training Loss: 0.6101, Test Loss: 0.9019
Epoch: 113, Training Loss: 0.6091, Test Loss: 0.8901
Epoch: 114, Training Loss: 0.6061, Test Loss: 0.8949
Epoch: 115, Training Loss: 0.6038, Test Loss: 0.8954
Epoch: 116, Training Loss: 0.5992, Test Loss: 0.8869

 58%|███████████████████████████████████████████████████████▊                                         | 144/250 [00:11<00:08, 12.39it/s, loss_test=0.927]
Epoch: 118, Training Loss: 0.5918, Test Loss: 0.8928
Epoch: 119, Training Loss: 0.5916, Test Loss: 0.9050
Epoch: 120, Training Loss: 0.5873, Test Loss: 0.8982
Epoch: 121, Training Loss: 0.5833, Test Loss: 0.8929
Epoch: 122, Training Loss: 0.5789, Test Loss: 0.8999
Epoch: 123, Training Loss: 0.5794, Test Loss: 0.8884
Epoch: 124, Training Loss: 0.5766, Test Loss: 0.9087
Epoch: 125, Training Loss: 0.5726, Test Loss: 0.8952
Epoch: 126, Training Loss: 0.5712, Test Loss: 0.9112
Epoch: 127, Training Loss: 0.5658, Test Loss: 0.9075
Epoch: 128, Training Loss: 0.5656, Test Loss: 0.9094
Epoch: 129, Training Loss: 0.5588, Test Loss: 0.8988
Epoch: 130, Training Loss: 0.5582, Test Loss: 0.9005
Epoch: 131, Training Loss: 0.5543, Test Loss: 0.8973
Epoch: 132, Training Loss: 0.5515, Test Loss: 0.9025
Epoch: 133, Training Loss: 0.5502, Test Loss: 0.9035
Epoch: 134, Training Loss: 0.5475, Test Loss: 0.9143
Epoch: 135, Training Loss: 0.5440, Test Loss: 0.8965
Epoch: 136, Training Loss: 0.5429, Test Loss: 0.8997
Epoch: 137, Training Loss: 0.5391, Test Loss: 0.9095
Epoch: 138, Training Loss: 0.5373, Test Loss: 0.9046
Epoch: 139, Training Loss: 0.5343, Test Loss: 0.9288
Epoch: 140, Training Loss: 0.5307, Test Loss: 0.9169
Epoch: 141, Training Loss: 0.5290, Test Loss: 0.9195

 67%|█████████████████████████████████████████████████████████████████▏                               | 168/250 [00:13<00:06, 12.40it/s, loss_test=0.942]
Epoch: 143, Training Loss: 0.5197, Test Loss: 0.9129
Epoch: 144, Training Loss: 0.5209, Test Loss: 0.9275
Epoch: 145, Training Loss: 0.5200, Test Loss: 0.9165
Epoch: 146, Training Loss: 0.5154, Test Loss: 0.9273
Epoch: 147, Training Loss: 0.5134, Test Loss: 0.9225
Epoch: 148, Training Loss: 0.5120, Test Loss: 0.9266
Epoch: 149, Training Loss: 0.5059, Test Loss: 0.9264
Epoch: 150, Training Loss: 0.5071, Test Loss: 0.9277
Epoch: 151, Training Loss: 0.5026, Test Loss: 0.9197
Epoch: 152, Training Loss: 0.5010, Test Loss: 0.9387
Epoch: 153, Training Loss: 0.4957, Test Loss: 0.9254
Epoch: 154, Training Loss: 0.4957, Test Loss: 0.9217
Epoch: 155, Training Loss: 0.4930, Test Loss: 0.9113
Epoch: 156, Training Loss: 0.4911, Test Loss: 0.9184
Epoch: 157, Training Loss: 0.4877, Test Loss: 0.9231
Epoch: 158, Training Loss: 0.4858, Test Loss: 0.9240
Epoch: 159, Training Loss: 0.4817, Test Loss: 0.9292
Epoch: 160, Training Loss: 0.4788, Test Loss: 0.9353
Epoch: 161, Training Loss: 0.4770, Test Loss: 0.9399
Epoch: 162, Training Loss: 0.4733, Test Loss: 0.9389
Epoch: 163, Training Loss: 0.4731, Test Loss: 0.9387
Epoch: 164, Training Loss: 0.4701, Test Loss: 0.9365
Epoch: 165, Training Loss: 0.4674, Test Loss: 0.9252

 77%|██████████████████████████████████████████████████████████████████████████▍                      | 192/250 [00:15<00:04, 12.16it/s, loss_test=0.951]
Epoch: 167, Training Loss: 0.4618, Test Loss: 0.9378
Epoch: 168, Training Loss: 0.4614, Test Loss: 0.9415
Epoch: 169, Training Loss: 0.4591, Test Loss: 0.9268
Epoch: 170, Training Loss: 0.4557, Test Loss: 0.9313
Epoch: 171, Training Loss: 0.4547, Test Loss: 0.9478
Epoch: 172, Training Loss: 0.4522, Test Loss: 0.9183
Epoch: 173, Training Loss: 0.4481, Test Loss: 0.9347
Epoch: 174, Training Loss: 0.4456, Test Loss: 0.9446
Epoch: 175, Training Loss: 0.4454, Test Loss: 0.9494
Epoch: 176, Training Loss: 0.4423, Test Loss: 0.9387
Epoch: 177, Training Loss: 0.4392, Test Loss: 0.9360
Epoch: 178, Training Loss: 0.4370, Test Loss: 0.9541
Epoch: 179, Training Loss: 0.4347, Test Loss: 0.9489
Epoch: 180, Training Loss: 0.4330, Test Loss: 0.9478
Epoch: 181, Training Loss: 0.4310, Test Loss: 0.9529
Epoch: 182, Training Loss: 0.4274, Test Loss: 0.9373
Epoch: 183, Training Loss: 0.4257, Test Loss: 0.9446
Epoch: 184, Training Loss: 0.4243, Test Loss: 0.9334
Epoch: 185, Training Loss: 0.4218, Test Loss: 0.9592
Epoch: 186, Training Loss: 0.4195, Test Loss: 0.9331
Epoch: 187, Training Loss: 0.4174, Test Loss: 0.9424
Epoch: 188, Training Loss: 0.4165, Test Loss: 0.9581
Epoch: 189, Training Loss: 0.4125, Test Loss: 0.9497

 87%|████████████████████████████████████████████████████████████████████████████████████▌            | 218/250 [00:17<00:02, 11.99it/s, loss_test=0.961]
Epoch: 191, Training Loss: 0.4104, Test Loss: 0.9518
Epoch: 192, Training Loss: 0.4069, Test Loss: 0.9513
Epoch: 193, Training Loss: 0.4043, Test Loss: 0.9471
Epoch: 194, Training Loss: 0.4038, Test Loss: 0.9589
Epoch: 195, Training Loss: 0.4020, Test Loss: 0.9493
Epoch: 196, Training Loss: 0.3984, Test Loss: 0.9629
Epoch: 197, Training Loss: 0.3972, Test Loss: 0.9481
Epoch: 198, Training Loss: 0.3953, Test Loss: 0.9492
Epoch: 199, Training Loss: 0.3915, Test Loss: 0.9712
Epoch: 200, Training Loss: 0.3929, Test Loss: 0.9573
Epoch: 201, Training Loss: 0.3901, Test Loss: 0.9520
Epoch: 202, Training Loss: 0.3868, Test Loss: 0.9569
Epoch: 203, Training Loss: 0.3846, Test Loss: 0.9591
Epoch: 204, Training Loss: 0.3845, Test Loss: 0.9623
Epoch: 205, Training Loss: 0.3808, Test Loss: 0.9735
Epoch: 206, Training Loss: 0.3794, Test Loss: 0.9699
Epoch: 207, Training Loss: 0.3782, Test Loss: 0.9677
Epoch: 208, Training Loss: 0.3763, Test Loss: 0.9749
Epoch: 209, Training Loss: 0.3736, Test Loss: 0.9608
Epoch: 210, Training Loss: 0.3741, Test Loss: 0.9761
Epoch: 211, Training Loss: 0.3697, Test Loss: 0.9505
Epoch: 212, Training Loss: 0.3690, Test Loss: 0.9708
Epoch: 213, Training Loss: 0.3679, Test Loss: 0.9785

 97%|█████████████████████████████████████████████████████████████████████████████████████████████▉   | 242/250 [00:19<00:00, 12.33it/s, loss_test=0.987]
Epoch: 215, Training Loss: 0.3629, Test Loss: 0.9724
Epoch: 216, Training Loss: 0.3613, Test Loss: 0.9749
Epoch: 217, Training Loss: 0.3593, Test Loss: 0.9612
Epoch: 218, Training Loss: 0.3569, Test Loss: 0.9682
Epoch: 219, Training Loss: 0.3562, Test Loss: 0.9724
Epoch: 220, Training Loss: 0.3530, Test Loss: 0.9719
Epoch: 221, Training Loss: 0.3524, Test Loss: 0.9681
Epoch: 222, Training Loss: 0.3502, Test Loss: 0.9668
Epoch: 223, Training Loss: 0.3492, Test Loss: 0.9625
Epoch: 224, Training Loss: 0.3462, Test Loss: 0.9814
Epoch: 225, Training Loss: 0.3456, Test Loss: 0.9834
Epoch: 226, Training Loss: 0.3437, Test Loss: 0.9753
Epoch: 227, Training Loss: 0.3431, Test Loss: 0.9736
Epoch: 228, Training Loss: 0.3416, Test Loss: 0.9815
Epoch: 229, Training Loss: 0.3398, Test Loss: 0.9840
Epoch: 230, Training Loss: 0.3367, Test Loss: 0.9892
Epoch: 231, Training Loss: 0.3355, Test Loss: 0.9881
Epoch: 232, Training Loss: 0.3342, Test Loss: 0.9725
Epoch: 233, Training Loss: 0.3322, Test Loss: 0.9729
Epoch: 234, Training Loss: 0.3306, Test Loss: 0.9782
Epoch: 235, Training Loss: 0.3292, Test Loss: 0.9745
Epoch: 236, Training Loss: 0.3274, Test Loss: 0.9746
Epoch: 237, Training Loss: 0.3271, Test Loss: 0.9854
Epoch: 238, Training Loss: 0.3239, Test Loss: 0.9734

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.15it/s, loss_test=0.985]
Epoch: 240, Training Loss: 0.3214, Test Loss: 0.9922
Epoch: 241, Training Loss: 0.3205, Test Loss: 0.9867
Epoch: 242, Training Loss: 0.3177, Test Loss: 0.9672
Epoch: 243, Training Loss: 0.3166, Test Loss: 0.9928
Epoch: 244, Training Loss: 0.3169, Test Loss: 0.9956
Epoch: 245, Training Loss: 0.3138, Test Loss: 0.9745
Epoch: 246, Training Loss: 0.3122, Test Loss: 0.9856
Epoch: 247, Training Loss: 0.3118, Test Loss: 0.9917
Epoch: 248, Training Loss: 0.3091, Test Loss: 0.9822
Epoch: 249, Training Loss: 0.3094, Test Loss: 0.9848
Model saved as model_5936701np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-1270000-5936701np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9969413042068481, 0.9955405950546264, 0.9898752808570862, 0.9957585334777832, 0.9934121966362, 0.9883446216583252, 0.9869279742240906, 0.9909750461578369, 0.9902156114578247, 0.9906041026115417, 0.989794921875, 0.9860817313194274, 0.990221130847931, 0.9897605657577515, 0.9858088970184327, 0.9879431128501892, 0.9882260918617248, 0.990089726448059, 0.9853587746620178, 0.9880844712257385, 0.9849748611450195, 0.9848754286766053, 0.9871995210647583, 0.9899790406227111, 0.9844906210899353, 0.9889275789260864, 0.9783811092376709, 0.9816255569458008, 0.9817614912986755, 0.9801144599914551, 0.9782564163208007, 0.9784038305282593, 0.9741754651069641, 0.9660942077636718, 0.9702734351158142, 0.9620412945747375, 0.9588844418525696, 0.9545777916908265, 0.9492124319076538, 0.9445088028907775, 0.9378905534744263, 0.9299897909164428, 0.9225016474723816, 0.9139760494232178, 0.9082826495170593, 0.8956684231758117, 0.8934527516365052, 0.8859572291374207, 0.8807522058486938, 0.875602412223816, 0.8711717844009399, 0.8656004667282104, 0.8558061242103576, 0.8497653126716613, 0.8452990293502808, 0.8365684747695923, 0.8361945033073426, 0.8305462718009948, 0.8225074648857117, 0.8166527271270752, 0.8129619479179382, 0.8114540696144104, 0.8054344773292541, 0.8038480281829834, 0.7963345050811768, 0.7923269987106323, 0.782832658290863, 0.7821159362792969, 0.7760360240936279, 0.7765044927597046, 0.7713899970054626, 0.7631584048271179, 0.7624891638755799, 0.7547651529312134, 0.7446098685264587, 0.7503190159797668, 0.7440060019493103, 0.7393852353096009, 0.7350940585136414, 0.7317193031311036, 0.7271843552589417, 0.7229837298393249, 0.7163036584854126, 0.7113865733146667, 0.7061941504478455, 0.7059607028961181, 0.7018818140029908, 0.700010108947754, 0.6947303056716919, 0.6913590908050538, 0.6830344080924988, 0.6824487924575806, 0.6793368577957153, 0.6728610873222352, 0.6712279438972473, 0.6669830799102783, 0.6647140622138977, 0.6621117115020752, 0.6542249917984009, 0.6525026559829712, 0.6499865651130676, 0.6463582873344421, 0.6410788655281067, 0.6373333215713501, 0.6343529224395752, 0.6344184160232544, 0.630633533000946, 0.6289720177650452, 0.6251049160957336, 0.6225059390068054, 0.6153518438339234, 0.616346001625061, 0.6100875854492187, 0.6090921759605408, 0.6060748219490051, 0.603840446472168, 0.5991569399833679, 0.596598744392395, 0.5917658805847168, 0.5916381001472473, 0.587291145324707, 0.5833164930343628, 0.5789348721504212, 0.5794342279434204, 0.576632297039032, 0.5726331114768982, 0.5712180495262146, 0.5657624959945678, 0.5655773282051086, 0.5588473200798034, 0.5582463383674622, 0.5543150782585144, 0.5515019297599792, 0.5502091169357299, 0.5475343227386474, 0.544043505191803, 0.542919373512268, 0.5390794754028321, 0.5372618913650513, 0.5343359112739563, 0.5307129621505737, 0.5289701461791992, 0.5269678115844727, 0.5197160482406616, 0.5208623051643372, 0.5200480401515961, 0.5153613924980164, 0.5134442687034607, 0.5119541764259339, 0.5058817267417908, 0.5070934295654297, 0.5025841653347015, 0.5010175585746766, 0.49573746919631956, 0.49571717381477354, 0.49297870993614196, 0.4911130666732788, 0.48771487474441527, 0.4857504844665527, 0.48167001008987426, 0.4788199901580811, 0.4769678771495819, 0.47332352995872495, 0.4730906546115875, 0.47013282775878906, 0.46735221743583677, 0.4656775891780853, 0.4617701888084412, 0.4614462018013, 0.4591307282447815, 0.45572208762168886, 0.454688173532486, 0.4522447049617767, 0.44807192087173464, 0.44564626216888426, 0.445429927110672, 0.4422758162021637, 0.43919084072113035, 0.4370486855506897, 0.4346690058708191, 0.4329758286476135, 0.4310038983821869, 0.42743942737579343, 0.42569241523742674, 0.42432800531387327, 0.4218191742897034, 0.4194789409637451, 0.4174103856086731, 0.4165474474430084, 0.41253621578216554, 0.41135815978050233, 0.41039481163024905, 0.406922847032547, 0.4042579770088196, 0.4038317859172821, 0.4020324110984802, 0.39835729002952575, 0.39719548225402834, 0.39527114629745486, 0.39151570200920105, 0.39287749528884885, 0.39011377692222593, 0.38682481050491335, 0.38457287549972535, 0.38449453115463256, 0.38084975481033323, 0.37936162352561953, 0.3782374620437622, 0.37632381319999697, 0.37362863421440123, 0.3740654706954956, 0.36970060467720034, 0.36904844641685486, 0.36792242527008057, 0.36572892069816587, 0.3628790557384491, 0.3612850844860077, 0.3592674911022186, 0.35689066648483275, 0.3562396168708801, 0.35304621458053587, 0.3523681044578552, 0.350222235918045, 0.3491947889328003, 0.3462259769439697, 0.3455618619918823, 0.3437332987785339, 0.3431347727775574, 0.3415951728820801, 0.33983929753303527, 0.3367292881011963, 0.3354931056499481, 0.3342347860336304, 0.3322198212146759, 0.3306350648403168, 0.3291831910610199, 0.3273809731006622, 0.32707162499427794, 0.32386985421180725, 0.3241304039955139, 0.32143725752830504, 0.32051641345024107, 0.31768394112586973, 0.31663315892219546, 0.3168722867965698, 0.3138278901576996, 0.31218149662017824, 0.3118137776851654, 0.3090884268283844, 0.30936861634254453], 'loss_test': [1.0783945322036743, 1.088284969329834, 1.0777904987335205, 1.0835816860198975, 1.0731470584869385, 1.0964055061340332, 1.071747899055481, 1.0828779935836792, 1.0873547792434692, 1.0818381309509277, 1.0566359758377075, 1.0926094055175781, 1.0668971538543701, 1.0875164270401, 1.0692578554153442, 1.0748642683029175, 1.0806926488876343, 1.0661494731903076, 1.0874512195587158, 1.08529531955719, 1.0860681533813477, 1.0831695795059204, 1.0892044305801392, 1.0837533473968506, 1.0707926750183105, 1.092803955078125, 1.0939209461212158, 1.092011570930481, 1.0915749073028564, 1.0771613121032715, 1.0859895944595337, 1.063083291053772, 1.0771777629852295, 1.0678199529647827, 1.0710347890853882, 1.0816463232040405, 1.062157154083252, 1.0649707317352295, 1.0594358444213867, 1.0506118535995483, 1.0580003261566162, 1.045393705368042, 1.036943793296814, 1.0326589345932007, 1.026908278465271, 1.0277289152145386, 1.0276161432266235, 1.0076565742492676, 1.0322942733764648, 1.0083199739456177, 1.0187891721725464, 1.0004212856292725, 0.9988966584205627, 0.9755882620811462, 0.9803387522697449, 0.9808784127235413, 0.9729920625686646, 0.960102379322052, 0.9527587890625, 0.9451144337654114, 0.9401596188545227, 0.9515986442565918, 0.9322022795677185, 0.933199942111969, 0.9418343901634216, 0.9224547147750854, 0.9307527542114258, 0.9212304949760437, 0.929854154586792, 0.9340978860855103, 0.9097152948379517, 0.9150463938713074, 0.9036059975624084, 0.9124390482902527, 0.9061973094940186, 0.9088460803031921, 0.8964033722877502, 0.9127199053764343, 0.8924174904823303, 0.8834617137908936, 0.9047855138778687, 0.9029279947280884, 0.8908271193504333, 0.8841304779052734, 0.8837236166000366, 0.88800448179245, 0.8922122120857239, 0.8857443928718567, 0.8900956511497498, 0.8886183500289917, 0.8895633220672607, 0.8728741407394409, 0.8920062780380249, 0.8841457366943359, 0.8798133134841919, 0.8769783973693848, 0.8858602046966553, 0.8830428719520569, 0.8868150115013123, 0.8827630281448364, 0.8895825147628784, 0.8876951932907104, 0.8884474039077759, 0.8828182816505432, 0.87106853723526, 0.8930491209030151, 0.8901455402374268, 0.8861262798309326, 0.8952423334121704, 0.8992443680763245, 0.8891739249229431, 0.9004393219947815, 0.9018612504005432, 0.8900595903396606, 0.8949447870254517, 0.8953587412834167, 0.8869372010231018, 0.8920242786407471, 0.8927874565124512, 0.9049830436706543, 0.8982252478599548, 0.8928691744804382, 0.8998525142669678, 0.8884128928184509, 0.9087419509887695, 0.8952495455741882, 0.9111850261688232, 0.9074624180793762, 0.9093793630599976, 0.8988378047943115, 0.9005222320556641, 0.8972691893577576, 0.9024932980537415, 0.9034854769706726, 0.9142820239067078, 0.8964729309082031, 0.8996638059616089, 0.9094715118408203, 0.9046409726142883, 0.9287679195404053, 0.9168857932090759, 0.9194719195365906, 0.9154695272445679, 0.9129326939582825, 0.9274899363517761, 0.91645747423172, 0.9272578954696655, 0.9224768877029419, 0.9266301989555359, 0.9263841509819031, 0.9277476072311401, 0.9196678996086121, 0.9387152791023254, 0.9254083037376404, 0.9217228293418884, 0.9113152027130127, 0.9184149503707886, 0.9230892658233643, 0.9239808320999146, 0.9292395710945129, 0.9353476762771606, 0.9398849010467529, 0.9388983845710754, 0.9386920928955078, 0.9364729523658752, 0.9252066016197205, 0.9298628568649292, 0.9378282427787781, 0.9415092468261719, 0.9268364906311035, 0.9312872290611267, 0.9478146433830261, 0.9183180928230286, 0.9346734881401062, 0.9445800185203552, 0.9494045972824097, 0.9386574625968933, 0.9359556436538696, 0.9541252255439758, 0.9488528966903687, 0.9477787613868713, 0.9528956413269043, 0.9373072385787964, 0.9446308016777039, 0.9334059357643127, 0.9591853022575378, 0.9331499338150024, 0.9423903226852417, 0.9580687284469604, 0.9497464895248413, 0.95465087890625, 0.9517797231674194, 0.9513397216796875, 0.9470711946487427, 0.9588875770568848, 0.949269711971283, 0.9628940224647522, 0.9481180310249329, 0.9491576552391052, 0.9711836576461792, 0.9572938680648804, 0.9520447254180908, 0.9569319486618042, 0.9590744376182556, 0.9622955322265625, 0.9735494256019592, 0.9699122905731201, 0.9676895141601562, 0.9748677611351013, 0.9607740640640259, 0.9760522246360779, 0.9504532814025879, 0.970779299736023, 0.978538453578949, 0.9732282161712646, 0.9724114537239075, 0.9749355912208557, 0.9612417817115784, 0.9681559205055237, 0.9723963737487793, 0.9718766212463379, 0.9681351184844971, 0.9668124914169312, 0.9624621868133545, 0.9813876748085022, 0.9833542704582214, 0.9752970337867737, 0.9735630750656128, 0.9815354347229004, 0.9839606881141663, 0.9892193675041199, 0.9881422519683838, 0.9724972248077393, 0.9728882312774658, 0.9782178401947021, 0.9745154976844788, 0.9746384024620056, 0.9853673577308655, 0.973375141620636, 0.9857062101364136, 0.9922131896018982, 0.9866942167282104, 0.9672232866287231, 0.9927871823310852, 0.9956453442573547, 0.9745386242866516, 0.9856424331665039, 0.9917232394218445, 0.98215252161026, 0.9847891330718994], 'identifier': '5936701np'}