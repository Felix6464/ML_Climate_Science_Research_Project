
 20%|███████████████████▌                                                                              | 20/100 [00:01<00:06, 12.34it/s, loss_test=1.097]
Epoch: 00, Training Loss: 0.9951, Test Loss: 1.0807
Epoch: 01, Training Loss: 0.9892, Test Loss: 1.0867
Epoch: 02, Training Loss: 0.9945, Test Loss: 1.0954
Epoch: 03, Training Loss: 0.9920, Test Loss: 1.0877
Epoch: 04, Training Loss: 0.9916, Test Loss: 1.0902
Epoch: 05, Training Loss: 0.9955, Test Loss: 1.0842
Epoch: 06, Training Loss: 0.9940, Test Loss: 1.0863
Epoch: 07, Training Loss: 0.9897, Test Loss: 1.0918
Epoch: 08, Training Loss: 0.9860, Test Loss: 1.0934
Epoch: 09, Training Loss: 0.9856, Test Loss: 1.0901
Epoch: 10, Training Loss: 0.9907, Test Loss: 1.1048
Epoch: 11, Training Loss: 0.9899, Test Loss: 1.0893
Epoch: 12, Training Loss: 0.9938, Test Loss: 1.0897
Epoch: 13, Training Loss: 0.9863, Test Loss: 1.0832
Epoch: 14, Training Loss: 0.9861, Test Loss: 1.0929
Epoch: 15, Training Loss: 0.9883, Test Loss: 1.0765
Epoch: 16, Training Loss: 0.9897, Test Loss: 1.0816
Epoch: 17, Training Loss: 0.9872, Test Loss: 1.0935
Epoch: 18, Training Loss: 0.9864, Test Loss: 1.0898
Epoch: 19, Training Loss: 0.9848, Test Loss: 1.0971
Epoch: 20, Training Loss: 0.9835, Test Loss: 1.0872
Epoch: 21, Training Loss: 0.9847, Test Loss: 1.0924
Epoch: 22, Training Loss: 0.9799, Test Loss: 1.0609
Epoch: 23, Training Loss: 0.9837, Test Loss: 1.0742
Epoch: 24, Training Loss: 0.9812, Test Loss: 1.0850
Epoch: 25, Training Loss: 0.9799, Test Loss: 1.0895
Epoch: 26, Training Loss: 0.9845, Test Loss: 1.0987
Epoch: 27, Training Loss: 0.9831, Test Loss: 1.0828
Epoch: 28, Training Loss: 0.9796, Test Loss: 1.0734
Epoch: 29, Training Loss: 0.9785, Test Loss: 1.0804
Epoch: 30, Training Loss: 0.9692, Test Loss: 1.0676
Epoch: 31, Training Loss: 0.9670, Test Loss: 1.0683
Epoch: 32, Training Loss: 0.9613, Test Loss: 1.0784
Epoch: 33, Training Loss: 0.9519, Test Loss: 1.0437
Epoch: 34, Training Loss: 0.9466, Test Loss: 1.0497
Epoch: 35, Training Loss: 0.9345, Test Loss: 1.0337
Epoch: 36, Training Loss: 0.9269, Test Loss: 1.0191
Epoch: 37, Training Loss: 0.9224, Test Loss: 1.0344
Epoch: 38, Training Loss: 0.9171, Test Loss: 1.0159
Epoch: 39, Training Loss: 0.9098, Test Loss: 1.0066
Epoch: 40, Training Loss: 0.9009, Test Loss: 1.0171
Epoch: 41, Training Loss: 0.8990, Test Loss: 1.0023
Epoch: 42, Training Loss: 0.8967, Test Loss: 1.0089
Epoch: 43, Training Loss: 0.8897, Test Loss: 0.9967

 44%|███████████████████████████████████████████                                                       | 44/100 [00:03<00:04, 12.21it/s, loss_test=0.971]
Epoch: 45, Training Loss: 0.8842, Test Loss: 0.9831
Epoch: 46, Training Loss: 0.8793, Test Loss: 0.9804
Epoch: 47, Training Loss: 0.8757, Test Loss: 0.9876
Epoch: 48, Training Loss: 0.8705, Test Loss: 0.9733
Epoch: 49, Training Loss: 0.8650, Test Loss: 0.9801
Epoch: 50, Training Loss: 0.8594, Test Loss: 0.9517
Epoch: 51, Training Loss: 0.8574, Test Loss: 0.9574
Epoch: 52, Training Loss: 0.8544, Test Loss: 0.9431
Epoch: 53, Training Loss: 0.8474, Test Loss: 0.9559
Epoch: 54, Training Loss: 0.8473, Test Loss: 0.9417
Epoch: 55, Training Loss: 0.8379, Test Loss: 0.9452
Epoch: 56, Training Loss: 0.8372, Test Loss: 0.9438
Epoch: 57, Training Loss: 0.8339, Test Loss: 0.9568
Epoch: 58, Training Loss: 0.8262, Test Loss: 0.9431
Epoch: 59, Training Loss: 0.8245, Test Loss: 0.9477
Epoch: 60, Training Loss: 0.8212, Test Loss: 0.9135
Epoch: 61, Training Loss: 0.8160, Test Loss: 0.9160
Epoch: 62, Training Loss: 0.8089, Test Loss: 0.9395
Epoch: 63, Training Loss: 0.8069, Test Loss: 0.9289
Epoch: 64, Training Loss: 0.8024, Test Loss: 0.9313
Epoch: 65, Training Loss: 0.7978, Test Loss: 0.9300
Epoch: 66, Training Loss: 0.7973, Test Loss: 0.9384
Epoch: 67, Training Loss: 0.7891, Test Loss: 0.9144


 92%|██████████████████████████████████████████████████████████████████████████████████████████▏       | 92/100 [00:07<00:00, 12.53it/s, loss_test=0.894]
Epoch: 69, Training Loss: 0.7826, Test Loss: 0.9148
Epoch: 70, Training Loss: 0.7781, Test Loss: 0.9185
Epoch: 71, Training Loss: 0.7749, Test Loss: 0.9301
Epoch: 72, Training Loss: 0.7678, Test Loss: 0.9062
Epoch: 73, Training Loss: 0.7667, Test Loss: 0.9156
Epoch: 74, Training Loss: 0.7628, Test Loss: 0.9099
Epoch: 75, Training Loss: 0.7599, Test Loss: 0.9066
Epoch: 76, Training Loss: 0.7522, Test Loss: 0.9123
Epoch: 77, Training Loss: 0.7538, Test Loss: 0.9116
Epoch: 78, Training Loss: 0.7464, Test Loss: 0.9085
Epoch: 79, Training Loss: 0.7449, Test Loss: 0.9049
Epoch: 80, Training Loss: 0.7405, Test Loss: 0.8952
Epoch: 81, Training Loss: 0.7353, Test Loss: 0.9069
Epoch: 82, Training Loss: 0.7324, Test Loss: 0.9007
Epoch: 83, Training Loss: 0.7336, Test Loss: 0.9041
Epoch: 84, Training Loss: 0.7287, Test Loss: 0.9028
Epoch: 85, Training Loss: 0.7265, Test Loss: 0.8974
Epoch: 86, Training Loss: 0.7195, Test Loss: 0.9052
Epoch: 87, Training Loss: 0.7177, Test Loss: 0.8938
Epoch: 88, Training Loss: 0.7131, Test Loss: 0.8960
Epoch: 89, Training Loss: 0.7120, Test Loss: 0.8894
Epoch: 90, Training Loss: 0.7084, Test Loss: 0.8973
Epoch: 91, Training Loss: 0.7007, Test Loss: 0.8972

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.08it/s, loss_test=0.888]
Epoch: 93, Training Loss: 0.6978, Test Loss: 0.8918
Epoch: 94, Training Loss: 0.6917, Test Loss: 0.8884
Epoch: 95, Training Loss: 0.6878, Test Loss: 0.8944
Epoch: 96, Training Loss: 0.6872, Test Loss: 0.8942
Epoch: 97, Training Loss: 0.6804, Test Loss: 0.8947
Epoch: 98, Training Loss: 0.6783, Test Loss: 0.8974
Epoch: 99, Training Loss: 0.6773, Test Loss: 0.8884
Model saved as model_405269np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-127000-405269np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 100, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9950660228729248, 0.989230465888977, 0.9944556713104248, 0.991993510723114, 0.991585373878479, 0.9954919576644897, 0.9939694881439209, 0.989727771282196, 0.9860423326492309, 0.9856220841407776, 0.9906699657440186, 0.9898967981338501, 0.993810486793518, 0.9863042950630188, 0.9860801100730896, 0.9883000135421753, 0.9897024035453796, 0.9871973633766175, 0.9864257216453552, 0.9847648978233338, 0.9835438847541809, 0.9847064733505249, 0.9798811674118042, 0.9836982011795044, 0.981234359741211, 0.9799431085586547, 0.9845110416412354, 0.9831177234649658, 0.9795671582221985, 0.9785340189933777, 0.9692105054855347, 0.9669890761375427, 0.9612582683563232, 0.9518889784812927, 0.9465656757354737, 0.9344629168510437, 0.9268974781036377, 0.9224079132080079, 0.9171035170555115, 0.9098386287689209, 0.9008696436882019, 0.8989826083183289, 0.8966729044914246, 0.8896576046943665, 0.8847591757774353, 0.8842469096183777, 0.8792503595352172, 0.8757175326347351, 0.8705324769020081, 0.8649633407592774, 0.8594217777252198, 0.8574308753013611, 0.854445469379425, 0.8473928093910217, 0.8472761750221253, 0.8379160642623902, 0.8371550559997558, 0.8338623881340027, 0.8261781573295593, 0.8245286703109741, 0.821246063709259, 0.816037368774414, 0.8089089751243591, 0.806867253780365, 0.8023834824562073, 0.7977597117424011, 0.7972536683082581, 0.7890904545783997, 0.7814777731895447, 0.7826464176177979, 0.7781491041183471, 0.774893832206726, 0.7678177833557129, 0.7666735053062439, 0.7627516865730286, 0.7598751783370972, 0.7522165656089783, 0.7538346648216248, 0.7463733553886414, 0.7449053883552551, 0.7404595732688903, 0.735257089138031, 0.7323737621307373, 0.733594560623169, 0.7287213802337646, 0.7264957427978516, 0.7194983005523682, 0.717737340927124, 0.7130959868431092, 0.711965799331665, 0.7083652496337891, 0.7007205605506897, 0.6998703360557557, 0.697801148891449, 0.691714096069336, 0.687845504283905, 0.6871920824050903, 0.6804366707801819, 0.6782940745353698, 0.6772973418235779], 'loss_test': [1.0806692838668823, 1.0867085456848145, 1.095395565032959, 1.0877156257629395, 1.0901991128921509, 1.0841639041900635, 1.0863300561904907, 1.091805338859558, 1.0934010744094849, 1.0900596380233765, 1.1048156023025513, 1.0893093347549438, 1.0897401571273804, 1.0831960439682007, 1.0929423570632935, 1.0765159130096436, 1.081632375717163, 1.0935018062591553, 1.0897730588912964, 1.09713613986969, 1.0872162580490112, 1.0924350023269653, 1.0609495639801025, 1.0741796493530273, 1.0850157737731934, 1.0894721746444702, 1.0986517667770386, 1.0827901363372803, 1.0734026432037354, 1.080350399017334, 1.0675780773162842, 1.068304419517517, 1.0784313678741455, 1.0437088012695312, 1.0497065782546997, 1.0337027311325073, 1.0190813541412354, 1.034406304359436, 1.015943169593811, 1.0066206455230713, 1.017086148262024, 1.0022720098495483, 1.008872628211975, 0.9967368245124817, 0.971146821975708, 0.9831128120422363, 0.9804425239562988, 0.9876314401626587, 0.9733320474624634, 0.9801336526870728, 0.9517307877540588, 0.9573929905891418, 0.9430747628211975, 0.9558873772621155, 0.9417268633842468, 0.9452022314071655, 0.9438496232032776, 0.9568306803703308, 0.9430946707725525, 0.9477161765098572, 0.913491427898407, 0.9160453081130981, 0.9394928812980652, 0.9288797974586487, 0.9312620759010315, 0.9299681186676025, 0.9383612275123596, 0.914412796497345, 0.9259011149406433, 0.914769172668457, 0.9184979200363159, 0.9300622940063477, 0.9062171578407288, 0.9156453013420105, 0.9098520874977112, 0.906649649143219, 0.9123038053512573, 0.9115571975708008, 0.9085325598716736, 0.9049360752105713, 0.8951858878135681, 0.9068591594696045, 0.9007304310798645, 0.9041385650634766, 0.9028305411338806, 0.8974416255950928, 0.9051975011825562, 0.8937658071517944, 0.8959560990333557, 0.8894255757331848, 0.8973411917686462, 0.8972440958023071, 0.8936957716941833, 0.8918393850326538, 0.8883766531944275, 0.89436274766922, 0.8941854238510132, 0.8947421908378601, 0.8974282145500183, 0.8883927464485168], 'identifier': '405269np'}