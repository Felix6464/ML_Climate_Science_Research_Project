
  8%|███████▊                                                                                          | 20/250 [00:01<00:19, 12.09it/s, loss_test=1.081]
Epoch: 00, Training Loss: 0.9919, Test Loss: 1.1103
Epoch: 01, Training Loss: 0.9900, Test Loss: 1.0754
Epoch: 02, Training Loss: 0.9922, Test Loss: 1.0992
Epoch: 03, Training Loss: 0.9957, Test Loss: 1.0891
Epoch: 04, Training Loss: 0.9952, Test Loss: 1.0893
Epoch: 05, Training Loss: 0.9922, Test Loss: 1.0846
Epoch: 06, Training Loss: 0.9874, Test Loss: 1.0904
Epoch: 07, Training Loss: 0.9884, Test Loss: 1.0845
Epoch: 08, Training Loss: 0.9919, Test Loss: 1.0914
Epoch: 09, Training Loss: 0.9902, Test Loss: 1.0750
Epoch: 10, Training Loss: 0.9929, Test Loss: 1.0838
Epoch: 11, Training Loss: 0.9846, Test Loss: 1.0886
Epoch: 12, Training Loss: 0.9871, Test Loss: 1.0799
Epoch: 13, Training Loss: 0.9850, Test Loss: 1.0810
Epoch: 14, Training Loss: 0.9835, Test Loss: 1.0808
Epoch: 15, Training Loss: 0.9902, Test Loss: 1.0899
Epoch: 16, Training Loss: 0.9870, Test Loss: 1.0956
Epoch: 17, Training Loss: 0.9863, Test Loss: 1.0718
Epoch: 18, Training Loss: 0.9824, Test Loss: 1.0777
Epoch: 19, Training Loss: 0.9858, Test Loss: 1.0787

 18%|█████████████████▏                                                                                | 44/250 [00:03<00:17, 11.88it/s, loss_test=0.977]
Epoch: 21, Training Loss: 0.9863, Test Loss: 1.0645
Epoch: 22, Training Loss: 0.9815, Test Loss: 1.0856
Epoch: 23, Training Loss: 0.9812, Test Loss: 1.0716
Epoch: 24, Training Loss: 0.9840, Test Loss: 1.0679
Epoch: 25, Training Loss: 0.9822, Test Loss: 1.0885
Epoch: 26, Training Loss: 0.9797, Test Loss: 1.0873
Epoch: 27, Training Loss: 0.9853, Test Loss: 1.0683
Epoch: 28, Training Loss: 0.9780, Test Loss: 1.0829
Epoch: 29, Training Loss: 0.9723, Test Loss: 1.0864
Epoch: 30, Training Loss: 0.9696, Test Loss: 1.0680
Epoch: 31, Training Loss: 0.9590, Test Loss: 1.0707
Epoch: 32, Training Loss: 0.9535, Test Loss: 1.0593
Epoch: 33, Training Loss: 0.9476, Test Loss: 1.0553
Epoch: 34, Training Loss: 0.9353, Test Loss: 1.0479
Epoch: 35, Training Loss: 0.9331, Test Loss: 1.0472
Epoch: 36, Training Loss: 0.9220, Test Loss: 1.0316
Epoch: 37, Training Loss: 0.9116, Test Loss: 1.0351
Epoch: 38, Training Loss: 0.9079, Test Loss: 1.0165
Epoch: 39, Training Loss: 0.9022, Test Loss: 1.0184
Epoch: 40, Training Loss: 0.8952, Test Loss: 1.0028
Epoch: 41, Training Loss: 0.8857, Test Loss: 0.9954
Epoch: 42, Training Loss: 0.8810, Test Loss: 0.9948
Epoch: 43, Training Loss: 0.8750, Test Loss: 0.9907

 27%|██████████████████████████▋                                                                       | 68/250 [00:05<00:14, 12.18it/s, loss_test=0.927]
Epoch: 45, Training Loss: 0.8665, Test Loss: 0.9847
Epoch: 46, Training Loss: 0.8631, Test Loss: 0.9669
Epoch: 47, Training Loss: 0.8528, Test Loss: 0.9767
Epoch: 48, Training Loss: 0.8494, Test Loss: 0.9767
Epoch: 49, Training Loss: 0.8461, Test Loss: 0.9522
Epoch: 50, Training Loss: 0.8419, Test Loss: 0.9741
Epoch: 51, Training Loss: 0.8316, Test Loss: 0.9722
Epoch: 52, Training Loss: 0.8310, Test Loss: 0.9555
Epoch: 53, Training Loss: 0.8247, Test Loss: 0.9469
Epoch: 54, Training Loss: 0.8223, Test Loss: 0.9611
Epoch: 55, Training Loss: 0.8203, Test Loss: 0.9561
Epoch: 56, Training Loss: 0.8187, Test Loss: 0.9623
Epoch: 57, Training Loss: 0.8130, Test Loss: 0.9400
Epoch: 58, Training Loss: 0.8049, Test Loss: 0.9330
Epoch: 59, Training Loss: 0.8037, Test Loss: 0.9420
Epoch: 60, Training Loss: 0.7991, Test Loss: 0.9229
Epoch: 61, Training Loss: 0.7969, Test Loss: 0.9175
Epoch: 62, Training Loss: 0.7921, Test Loss: 0.9412
Epoch: 63, Training Loss: 0.7909, Test Loss: 0.9375
Epoch: 64, Training Loss: 0.7858, Test Loss: 0.9214
Epoch: 65, Training Loss: 0.7831, Test Loss: 0.9316
Epoch: 66, Training Loss: 0.7761, Test Loss: 0.9209
Epoch: 67, Training Loss: 0.7750, Test Loss: 0.9166

 38%|████████████████████████████████████▊                                                             | 94/250 [00:07<00:13, 11.85it/s, loss_test=0.906]
Epoch: 69, Training Loss: 0.7699, Test Loss: 0.9251
Epoch: 70, Training Loss: 0.7647, Test Loss: 0.9205
Epoch: 71, Training Loss: 0.7596, Test Loss: 0.9121
Epoch: 72, Training Loss: 0.7569, Test Loss: 0.9171
Epoch: 73, Training Loss: 0.7559, Test Loss: 0.9150
Epoch: 74, Training Loss: 0.7508, Test Loss: 0.9248
Epoch: 75, Training Loss: 0.7503, Test Loss: 0.9158
Epoch: 76, Training Loss: 0.7400, Test Loss: 0.9102
Epoch: 77, Training Loss: 0.7413, Test Loss: 0.9133
Epoch: 78, Training Loss: 0.7368, Test Loss: 0.9058
Epoch: 79, Training Loss: 0.7291, Test Loss: 0.9023
Epoch: 80, Training Loss: 0.7278, Test Loss: 0.9039
Epoch: 81, Training Loss: 0.7251, Test Loss: 0.9042
Epoch: 82, Training Loss: 0.7208, Test Loss: 0.9027
Epoch: 83, Training Loss: 0.7164, Test Loss: 0.9032
Epoch: 84, Training Loss: 0.7165, Test Loss: 0.9073
Epoch: 85, Training Loss: 0.7105, Test Loss: 0.8932
Epoch: 86, Training Loss: 0.7078, Test Loss: 0.8868
Epoch: 87, Training Loss: 0.7038, Test Loss: 0.9056
Epoch: 88, Training Loss: 0.6994, Test Loss: 0.8919
Epoch: 89, Training Loss: 0.6958, Test Loss: 0.9007
Epoch: 90, Training Loss: 0.6926, Test Loss: 0.8911
Epoch: 91, Training Loss: 0.6889, Test Loss: 0.9054

 46%|█████████████████████████████████████████████                                                    | 116/250 [00:09<00:10, 12.47it/s, loss_test=0.909]
Epoch: 93, Training Loss: 0.6827, Test Loss: 0.9056
Epoch: 94, Training Loss: 0.6802, Test Loss: 0.9059
Epoch: 95, Training Loss: 0.6750, Test Loss: 0.8961
Epoch: 96, Training Loss: 0.6736, Test Loss: 0.8836
Epoch: 97, Training Loss: 0.6717, Test Loss: 0.9007
Epoch: 98, Training Loss: 0.6673, Test Loss: 0.8973
Epoch: 99, Training Loss: 0.6643, Test Loss: 0.8933
Epoch: 100, Training Loss: 0.6616, Test Loss: 0.9029
Epoch: 101, Training Loss: 0.6581, Test Loss: 0.9060
Epoch: 102, Training Loss: 0.6544, Test Loss: 0.8988
Epoch: 103, Training Loss: 0.6511, Test Loss: 0.9057
Epoch: 104, Training Loss: 0.6476, Test Loss: 0.8921
Epoch: 105, Training Loss: 0.6436, Test Loss: 0.8913
Epoch: 106, Training Loss: 0.6423, Test Loss: 0.8915
Epoch: 107, Training Loss: 0.6378, Test Loss: 0.9002
Epoch: 108, Training Loss: 0.6372, Test Loss: 0.8955
Epoch: 109, Training Loss: 0.6339, Test Loss: 0.9018
Epoch: 110, Training Loss: 0.6300, Test Loss: 0.8971
Epoch: 111, Training Loss: 0.6263, Test Loss: 0.9031
Epoch: 112, Training Loss: 0.6228, Test Loss: 0.9013
Epoch: 113, Training Loss: 0.6200, Test Loss: 0.8995
Epoch: 114, Training Loss: 0.6181, Test Loss: 0.8995
Epoch: 115, Training Loss: 0.6149, Test Loss: 0.9023
Epoch: 116, Training Loss: 0.6137, Test Loss: 0.9141
Epoch: 117, Training Loss: 0.6116, Test Loss: 0.9093
Epoch: 118, Training Loss: 0.6080, Test Loss: 0.9164
Epoch: 119, Training Loss: 0.6052, Test Loss: 0.9096
Epoch: 120, Training Loss: 0.5996, Test Loss: 0.9081
Epoch: 121, Training Loss: 0.5975, Test Loss: 0.9071
Epoch: 122, Training Loss: 0.5939, Test Loss: 0.9220
Epoch: 123, Training Loss: 0.5935, Test Loss: 0.9091
Epoch: 124, Training Loss: 0.5868, Test Loss: 0.9078
Epoch: 125, Training Loss: 0.5857, Test Loss: 0.9065
Epoch: 126, Training Loss: 0.5835, Test Loss: 0.9187
Epoch: 127, Training Loss: 0.5799, Test Loss: 0.9140
Epoch: 128, Training Loss: 0.5771, Test Loss: 0.9178
Epoch: 129, Training Loss: 0.5730, Test Loss: 0.9160
Epoch: 130, Training Loss: 0.5709, Test Loss: 0.9244
Epoch: 131, Training Loss: 0.5687, Test Loss: 0.9281
Epoch: 132, Training Loss: 0.5674, Test Loss: 0.9143
Epoch: 133, Training Loss: 0.5633, Test Loss: 0.9194
Epoch: 134, Training Loss: 0.5620, Test Loss: 0.9166
Epoch: 135, Training Loss: 0.5582, Test Loss: 0.9216
Epoch: 136, Training Loss: 0.5541, Test Loss: 0.9312
Epoch: 137, Training Loss: 0.5524, Test Loss: 0.9182
Epoch: 138, Training Loss: 0.5479, Test Loss: 0.9230
Epoch: 139, Training Loss: 0.5472, Test Loss: 0.9252

 56%|██████████████████████████████████████████████████████▎                                          | 140/250 [00:11<00:09, 11.56it/s, loss_test=0.912]
Epoch: 141, Training Loss: 0.5420, Test Loss: 0.9213
Epoch: 142, Training Loss: 0.5382, Test Loss: 0.9199
Epoch: 143, Training Loss: 0.5373, Test Loss: 0.9327
Epoch: 144, Training Loss: 0.5321, Test Loss: 0.9278
Epoch: 145, Training Loss: 0.5288, Test Loss: 0.9304
Epoch: 146, Training Loss: 0.5285, Test Loss: 0.9262
Epoch: 147, Training Loss: 0.5257, Test Loss: 0.9283
Epoch: 148, Training Loss: 0.5248, Test Loss: 0.9277
Epoch: 149, Training Loss: 0.5183, Test Loss: 0.9262
Epoch: 150, Training Loss: 0.5188, Test Loss: 0.9331
Epoch: 151, Training Loss: 0.5167, Test Loss: 0.9280
Epoch: 152, Training Loss: 0.5129, Test Loss: 0.9281
Epoch: 153, Training Loss: 0.5117, Test Loss: 0.9146
Epoch: 154, Training Loss: 0.5065, Test Loss: 0.9394
Epoch: 155, Training Loss: 0.5053, Test Loss: 0.9281
Epoch: 156, Training Loss: 0.5045, Test Loss: 0.9464
Epoch: 157, Training Loss: 0.5008, Test Loss: 0.9304
Epoch: 158, Training Loss: 0.4983, Test Loss: 0.9448
Epoch: 159, Training Loss: 0.4959, Test Loss: 0.9264
Epoch: 160, Training Loss: 0.4901, Test Loss: 0.9333
Epoch: 161, Training Loss: 0.4906, Test Loss: 0.9418
Epoch: 162, Training Loss: 0.4860, Test Loss: 0.9388
Epoch: 163, Training Loss: 0.4866, Test Loss: 0.9365
Epoch: 164, Training Loss: 0.4829, Test Loss: 0.9438

 66%|████████████████████████████████████████████████████████████████▍                                | 166/250 [00:13<00:06, 12.28it/s, loss_test=0.926]
Epoch: 166, Training Loss: 0.4787, Test Loss: 0.9346
Epoch: 167, Training Loss: 0.4763, Test Loss: 0.9306
Epoch: 168, Training Loss: 0.4721, Test Loss: 0.9514
Epoch: 169, Training Loss: 0.4733, Test Loss: 0.9475
Epoch: 170, Training Loss: 0.4692, Test Loss: 0.9467
Epoch: 171, Training Loss: 0.4678, Test Loss: 0.9433
Epoch: 172, Training Loss: 0.4645, Test Loss: 0.9523
Epoch: 173, Training Loss: 0.4610, Test Loss: 0.9243
Epoch: 174, Training Loss: 0.4595, Test Loss: 0.9379
Epoch: 175, Training Loss: 0.4577, Test Loss: 0.9332
Epoch: 176, Training Loss: 0.4552, Test Loss: 0.9460
Epoch: 177, Training Loss: 0.4539, Test Loss: 0.9302
Epoch: 178, Training Loss: 0.4525, Test Loss: 0.9440
Epoch: 179, Training Loss: 0.4483, Test Loss: 0.9452
Epoch: 180, Training Loss: 0.4465, Test Loss: 0.9419
Epoch: 181, Training Loss: 0.4437, Test Loss: 0.9425
Epoch: 182, Training Loss: 0.4413, Test Loss: 0.9344
Epoch: 183, Training Loss: 0.4413, Test Loss: 0.9502
Epoch: 184, Training Loss: 0.4365, Test Loss: 0.9492
Epoch: 185, Training Loss: 0.4353, Test Loss: 0.9444
Epoch: 186, Training Loss: 0.4337, Test Loss: 0.9526
Epoch: 187, Training Loss: 0.4317, Test Loss: 0.9473
Epoch: 188, Training Loss: 0.4287, Test Loss: 0.9480


 86%|███████████████████████████████████████████████████████████████████████████████████              | 214/250 [00:17<00:03, 11.34it/s, loss_test=0.962]
Epoch: 190, Training Loss: 0.4246, Test Loss: 0.9329
Epoch: 191, Training Loss: 0.4222, Test Loss: 0.9463
Epoch: 192, Training Loss: 0.4204, Test Loss: 0.9494
Epoch: 193, Training Loss: 0.4161, Test Loss: 0.9603
Epoch: 194, Training Loss: 0.4167, Test Loss: 0.9441
Epoch: 195, Training Loss: 0.4162, Test Loss: 0.9419
Epoch: 196, Training Loss: 0.4123, Test Loss: 0.9450
Epoch: 197, Training Loss: 0.4098, Test Loss: 0.9690
Epoch: 198, Training Loss: 0.4084, Test Loss: 0.9477
Epoch: 199, Training Loss: 0.4069, Test Loss: 0.9437
Epoch: 200, Training Loss: 0.4057, Test Loss: 0.9512
Epoch: 201, Training Loss: 0.4032, Test Loss: 0.9585
Epoch: 202, Training Loss: 0.4004, Test Loss: 0.9498
Epoch: 203, Training Loss: 0.3989, Test Loss: 0.9542
Epoch: 204, Training Loss: 0.3961, Test Loss: 0.9660
Epoch: 205, Training Loss: 0.3956, Test Loss: 0.9631
Epoch: 206, Training Loss: 0.3929, Test Loss: 0.9641
Epoch: 207, Training Loss: 0.3908, Test Loss: 0.9771
Epoch: 208, Training Loss: 0.3896, Test Loss: 0.9768
Epoch: 209, Training Loss: 0.3895, Test Loss: 0.9608
Epoch: 210, Training Loss: 0.3842, Test Loss: 0.9599
Epoch: 211, Training Loss: 0.3844, Test Loss: 0.9652
Epoch: 212, Training Loss: 0.3822, Test Loss: 0.9561

 95%|████████████████████████████████████████████████████████████████████████████████████████████▎    | 238/250 [00:19<00:01, 11.77it/s, loss_test=0.965]
Epoch: 214, Training Loss: 0.3785, Test Loss: 0.9618
Epoch: 215, Training Loss: 0.3761, Test Loss: 0.9668
Epoch: 216, Training Loss: 0.3736, Test Loss: 0.9594
Epoch: 217, Training Loss: 0.3738, Test Loss: 0.9632
Epoch: 218, Training Loss: 0.3716, Test Loss: 0.9616
Epoch: 219, Training Loss: 0.3692, Test Loss: 0.9601
Epoch: 220, Training Loss: 0.3676, Test Loss: 0.9678
Epoch: 221, Training Loss: 0.3670, Test Loss: 0.9584
Epoch: 222, Training Loss: 0.3664, Test Loss: 0.9685
Epoch: 223, Training Loss: 0.3631, Test Loss: 0.9616
Epoch: 224, Training Loss: 0.3612, Test Loss: 0.9595
Epoch: 225, Training Loss: 0.3599, Test Loss: 0.9523
Epoch: 226, Training Loss: 0.3579, Test Loss: 0.9684
Epoch: 227, Training Loss: 0.3555, Test Loss: 0.9782
Epoch: 228, Training Loss: 0.3552, Test Loss: 0.9708
Epoch: 229, Training Loss: 0.3527, Test Loss: 0.9730
Epoch: 230, Training Loss: 0.3506, Test Loss: 0.9562
Epoch: 231, Training Loss: 0.3500, Test Loss: 0.9677
Epoch: 232, Training Loss: 0.3478, Test Loss: 0.9756
Epoch: 233, Training Loss: 0.3469, Test Loss: 0.9700
Epoch: 234, Training Loss: 0.3454, Test Loss: 0.9625
Epoch: 235, Training Loss: 0.3434, Test Loss: 0.9643
Epoch: 236, Training Loss: 0.3407, Test Loss: 0.9654

100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.04it/s, loss_test=0.982]
Epoch: 238, Training Loss: 0.3380, Test Loss: 0.9652
Epoch: 239, Training Loss: 0.3374, Test Loss: 0.9665
Epoch: 240, Training Loss: 0.3363, Test Loss: 0.9580
Epoch: 241, Training Loss: 0.3344, Test Loss: 0.9804
Epoch: 242, Training Loss: 0.3339, Test Loss: 0.9763
Epoch: 243, Training Loss: 0.3320, Test Loss: 0.9601
Epoch: 244, Training Loss: 0.3302, Test Loss: 0.9772
Epoch: 245, Training Loss: 0.3278, Test Loss: 0.9765
Epoch: 246, Training Loss: 0.3273, Test Loss: 0.9903
Epoch: 247, Training Loss: 0.3250, Test Loss: 0.9830
Epoch: 248, Training Loss: 0.3237, Test Loss: 0.9836
Epoch: 249, Training Loss: 0.3228, Test Loss: 0.9815
Model saved as model_7491203np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-1280000-7491203np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 250, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-100k-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9918921828269959, 0.9900133848190308, 0.9922431111335754, 0.9957242250442505, 0.9952250480651855, 0.9921780705451966, 0.9874353528022766, 0.988408362865448, 0.9919301867485046, 0.9901973724365234, 0.9929175972938538, 0.9845857620239258, 0.9871094226837158, 0.9850164651870728, 0.9834919691085815, 0.9902398943901062, 0.9869678258895874, 0.9863401532173157, 0.98244309425354, 0.9857935905456543, 0.9816823720932006, 0.9863141655921936, 0.9815364718437195, 0.981219494342804, 0.9839976668357849, 0.9822447896003723, 0.9796569466590881, 0.9853387236595154, 0.9780441999435425, 0.9723056793212891, 0.9695951223373414, 0.9589809775352478, 0.9534605503082275, 0.9475826382637024, 0.9352891445159912, 0.9330839514732361, 0.9219570875167846, 0.9115940570831299, 0.9078827977180481, 0.9021944761276245, 0.8952337741851807, 0.8857410907745361, 0.8810108542442322, 0.8750201106071472, 0.8693044304847717, 0.8665109992027282, 0.863059651851654, 0.8528335571289063, 0.849408483505249, 0.846121883392334, 0.8419175267219543, 0.8315873742103577, 0.8309569478034973, 0.8246604323387146, 0.8223186254501342, 0.8203151226043701, 0.8187090635299683, 0.8130159974098206, 0.8049474120140075, 0.8036882638931274, 0.799066138267517, 0.7969429135322571, 0.7920528054237366, 0.7908934354782104, 0.7858383417129516, 0.783135986328125, 0.7760908007621765, 0.7750441670417786, 0.7724046111106873, 0.7698660254478454, 0.7647441744804382, 0.7595577597618103, 0.7569191455841064, 0.7558843851089477, 0.750809121131897, 0.7503046512603759, 0.7399525165557861, 0.7413163185119629, 0.7367592096328736, 0.7290890932083129, 0.7277638077735901, 0.7250603914260865, 0.7208097815513611, 0.7164315700531005, 0.7164502620697022, 0.7104788303375245, 0.7078164339065551, 0.7038173198699951, 0.6994462609291077, 0.6957779169082642, 0.6926251173019409, 0.6889104604721069, 0.6872743010520935, 0.6826624274253845, 0.680240023136139, 0.6749794840812683, 0.6736091375350952, 0.6717142105102539, 0.6672764420509338, 0.664290976524353, 0.6616188287734985, 0.6581236720085144, 0.6544109940528869, 0.6511162161827088, 0.6475685715675354, 0.6436185240745544, 0.6422553181648254, 0.6377678155899048, 0.6372197508811951, 0.6339486956596374, 0.6299684643745422, 0.6263059139251709, 0.6228135347366333, 0.6199930787086487, 0.6181436777114868, 0.614861786365509, 0.6137116193771363, 0.611636757850647, 0.6079977750778198, 0.6052075147628784, 0.599558413028717, 0.5974842786788941, 0.5938804626464844, 0.5935030460357666, 0.5868264317512513, 0.5857033133506775, 0.5835106372833252, 0.5798762679100037, 0.5770729064941407, 0.5730416059494019, 0.570905864238739, 0.5686802983283996, 0.5673957347869873, 0.5633150219917298, 0.5620317697525025, 0.5582211136817932, 0.5540796518325806, 0.5524415731430053, 0.5478727579116821, 0.5471903800964355, 0.5431131601333619, 0.5419831991195678, 0.5381596207618713, 0.5372822165489197, 0.5320963025093078, 0.5288197755813598, 0.528541100025177, 0.5257134556770324, 0.5247551441192627, 0.5183302998542786, 0.5188213944435119, 0.5166990995407105, 0.5129233717918396, 0.5116877555847168, 0.5065333604812622, 0.5053171873092651, 0.5044800341129303, 0.5008246004581451, 0.498277473449707, 0.4958944857120514, 0.49008708000183104, 0.49058679938316346, 0.4859646737575531, 0.4866173803806305, 0.4828972101211548, 0.4788341045379639, 0.4787129402160645, 0.4762607514858246, 0.4721475481987, 0.4733345746994019, 0.469221305847168, 0.46775501370429995, 0.46445308327674867, 0.4609633207321167, 0.45948250889778136, 0.45774824023246763, 0.45520148873329164, 0.45387197732925416, 0.45250102281570437, 0.44826277494430544, 0.4465444803237915, 0.44366676211357114, 0.44125681519508364, 0.4412695229053497, 0.4365254282951355, 0.43532333374023435, 0.4337323486804962, 0.43165833950042726, 0.428722482919693, 0.4285330593585968, 0.4246375381946564, 0.42217338681221006, 0.42037482261657716, 0.41611040830612184, 0.4166930615901947, 0.4161637663841248, 0.4122611343860626, 0.40978865027427674, 0.4084160506725311, 0.4069319009780884, 0.40567087531089785, 0.4032483518123627, 0.4004067897796631, 0.39892702698707583, 0.3960966169834137, 0.3955999851226807, 0.39292330741882325, 0.3908142924308777, 0.3895972967147827, 0.389510715007782, 0.38422076106071473, 0.3844397127628326, 0.38224405646324155, 0.37996103763580324, 0.3784911811351776, 0.37607569098472593, 0.3736314713954926, 0.37377606630325316, 0.37158831357955935, 0.3692281782627106, 0.36758082509040835, 0.3669621765613556, 0.3663933515548706, 0.3631226718425751, 0.36120655536651614, 0.3599293649196625, 0.35786024332046507, 0.35548866987228395, 0.3552276074886322, 0.35265603065490725, 0.3505620718002319, 0.35001310110092165, 0.3478417158126831, 0.34689403176307676, 0.34542542695999146, 0.34344189167022704, 0.34067551493644715, 0.3412499904632568, 0.33802309036254885, 0.3373803436756134, 0.3363134801387787, 0.3343918740749359, 0.33394150733947753, 0.33202104568481444, 0.3301538109779358, 0.32776952981948854, 0.32726024389266967, 0.32503568530082705, 0.3237065255641937, 0.3227636516094208], 'loss_test': [1.1103370189666748, 1.0754454135894775, 1.099204421043396, 1.0890672206878662, 1.0892637968063354, 1.08457612991333, 1.090404748916626, 1.0844694375991821, 1.0913891792297363, 1.074954628944397, 1.083827018737793, 1.0886355638504028, 1.0798852443695068, 1.0810461044311523, 1.0808191299438477, 1.089931607246399, 1.0956100225448608, 1.071769118309021, 1.0777125358581543, 1.0787065029144287, 1.0805424451828003, 1.06452476978302, 1.0856279134750366, 1.0716233253479004, 1.0679435729980469, 1.0885090827941895, 1.087315559387207, 1.0683066844940186, 1.0828737020492554, 1.0864371061325073, 1.0680193901062012, 1.0706660747528076, 1.0592641830444336, 1.0553408861160278, 1.047890067100525, 1.0471653938293457, 1.0315614938735962, 1.0351160764694214, 1.0164583921432495, 1.0184210538864136, 1.002753496170044, 0.9953982830047607, 0.9947690367698669, 0.990699052810669, 0.9765452146530151, 0.9846740365028381, 0.9668543934822083, 0.9766688346862793, 0.9766583442687988, 0.9521801471710205, 0.9741424918174744, 0.9721757173538208, 0.955460786819458, 0.9469316601753235, 0.9611261487007141, 0.9560620188713074, 0.9622812867164612, 0.9400021433830261, 0.9329772591590881, 0.9420028924942017, 0.9229161739349365, 0.9175111651420593, 0.9411541819572449, 0.9375461339950562, 0.9213667511940002, 0.9316387176513672, 0.9208996295928955, 0.9166029691696167, 0.9272305965423584, 0.9250759482383728, 0.9204531311988831, 0.9121344685554504, 0.917081892490387, 0.914989173412323, 0.9247975945472717, 0.9157898426055908, 0.9101734161376953, 0.913308322429657, 0.9057968258857727, 0.9022547602653503, 0.9038798809051514, 0.9042227268218994, 0.9026992321014404, 0.9031895995140076, 0.9072791337966919, 0.8931916952133179, 0.8867803812026978, 0.9055562615394592, 0.8918678760528564, 0.9006768465042114, 0.8910788893699646, 0.9054431915283203, 0.8902485370635986, 0.9055630564689636, 0.905874490737915, 0.8961212635040283, 0.8835563063621521, 0.900702953338623, 0.8973144888877869, 0.8933344483375549, 0.9029185175895691, 0.9059712886810303, 0.8988013863563538, 0.9057116508483887, 0.8920996785163879, 0.8912821412086487, 0.8915224671363831, 0.9001617431640625, 0.8955230712890625, 0.9018182754516602, 0.897139310836792, 0.9030631184577942, 0.9012691974639893, 0.899524986743927, 0.8995187878608704, 0.9022585153579712, 0.9140877723693848, 0.9093374013900757, 0.9163687825202942, 0.9096342325210571, 0.9081398248672485, 0.907050371170044, 0.9220155477523804, 0.9090923070907593, 0.9077972769737244, 0.9064756631851196, 0.9187071919441223, 0.9139690399169922, 0.9178414344787598, 0.9159634113311768, 0.9243603348731995, 0.928135097026825, 0.9142628312110901, 0.9193546175956726, 0.916628360748291, 0.9215826392173767, 0.9311520457267761, 0.918248176574707, 0.9230062365531921, 0.9252152442932129, 0.9119850397109985, 0.9212547540664673, 0.9199337363243103, 0.9327070713043213, 0.9278130531311035, 0.9304043650627136, 0.9261608719825745, 0.9282814860343933, 0.9276700615882874, 0.9262460470199585, 0.9331480264663696, 0.9279587268829346, 0.9280824661254883, 0.9146049618721008, 0.9393969178199768, 0.9280531406402588, 0.9463624358177185, 0.9304353594779968, 0.9447619915008545, 0.9263883829116821, 0.9332612156867981, 0.9418165683746338, 0.9388355612754822, 0.9364712238311768, 0.9437684416770935, 0.925828754901886, 0.9345722198486328, 0.9305835366249084, 0.9514159560203552, 0.94745934009552, 0.9467042684555054, 0.943272054195404, 0.9522573947906494, 0.9242639541625977, 0.9378621578216553, 0.933185338973999, 0.9459577798843384, 0.9302263259887695, 0.9440138339996338, 0.9452477693557739, 0.9419394135475159, 0.9425359964370728, 0.9344205260276794, 0.9502255916595459, 0.9491991996765137, 0.9444395303726196, 0.952629804611206, 0.9473075270652771, 0.9479883313179016, 0.9511075615882874, 0.9328663349151611, 0.9462870359420776, 0.9494279623031616, 0.9602578282356262, 0.9441496133804321, 0.9418543577194214, 0.9449658989906311, 0.9690043330192566, 0.9476563334465027, 0.9437309503555298, 0.9511737823486328, 0.9584600925445557, 0.9498151540756226, 0.9542478919029236, 0.9660338163375854, 0.9631019234657288, 0.9641456007957458, 0.9771386384963989, 0.9767699837684631, 0.9608233571052551, 0.9599135518074036, 0.9651835560798645, 0.9561169147491455, 0.9801189303398132, 0.9618465304374695, 0.9667783975601196, 0.9593570232391357, 0.9632189869880676, 0.9615510702133179, 0.9601259231567383, 0.9677797555923462, 0.9583630561828613, 0.9685371518135071, 0.9616360664367676, 0.9594696164131165, 0.9522932767868042, 0.9683754444122314, 0.9781538248062134, 0.9707680940628052, 0.9730492830276489, 0.9562307000160217, 0.9677454829216003, 0.9756225943565369, 0.9700080156326294, 0.9625244140625, 0.9642899632453918, 0.965377151966095, 0.9741611480712891, 0.9651604890823364, 0.9665352702140808, 0.9580339193344116, 0.9804099202156067, 0.9763395190238953, 0.9601437449455261, 0.9772005081176758, 0.9765401482582092, 0.9902979135513306, 0.9830131530761719, 0.983632504940033, 0.9815460443496704], 'identifier': '7491203np'}