
 20%|███████████████████▌                                                                              | 20/100 [00:01<00:06, 11.85it/s, loss_test=1.065]
Epoch: 00, Training Loss: 0.9894, Test Loss: 1.0948
Epoch: 01, Training Loss: 0.9863, Test Loss: 1.0856
Epoch: 02, Training Loss: 0.9870, Test Loss: 1.0958
Epoch: 03, Training Loss: 0.9922, Test Loss: 1.0958
Epoch: 04, Training Loss: 0.9929, Test Loss: 1.0878
Epoch: 05, Training Loss: 0.9889, Test Loss: 1.0736
Epoch: 06, Training Loss: 0.9846, Test Loss: 1.0898
Epoch: 07, Training Loss: 0.9897, Test Loss: 1.0855
Epoch: 08, Training Loss: 0.9906, Test Loss: 1.0954
Epoch: 09, Training Loss: 0.9838, Test Loss: 1.0801
Epoch: 10, Training Loss: 0.9890, Test Loss: 1.0885
Epoch: 11, Training Loss: 0.9875, Test Loss: 1.1120
Epoch: 12, Training Loss: 0.9831, Test Loss: 1.0882
Epoch: 13, Training Loss: 0.9912, Test Loss: 1.0744
Epoch: 14, Training Loss: 0.9889, Test Loss: 1.0706
Epoch: 15, Training Loss: 0.9875, Test Loss: 1.0758
Epoch: 16, Training Loss: 0.9818, Test Loss: 1.0839
Epoch: 17, Training Loss: 0.9808, Test Loss: 1.0772
Epoch: 18, Training Loss: 0.9896, Test Loss: 1.0835
Epoch: 19, Training Loss: 0.9835, Test Loss: 1.0863
Epoch: 20, Training Loss: 0.9839, Test Loss: 1.0646
Epoch: 21, Training Loss: 0.9818, Test Loss: 1.0735
Epoch: 22, Training Loss: 0.9831, Test Loss: 1.0686
Epoch: 23, Training Loss: 0.9817, Test Loss: 1.0624
Epoch: 24, Training Loss: 0.9849, Test Loss: 1.0702
Epoch: 25, Training Loss: 0.9805, Test Loss: 1.0843
Epoch: 26, Training Loss: 0.9796, Test Loss: 1.0819
Epoch: 27, Training Loss: 0.9818, Test Loss: 1.0809
Epoch: 28, Training Loss: 0.9798, Test Loss: 1.0731
Epoch: 29, Training Loss: 0.9753, Test Loss: 1.0951
Epoch: 30, Training Loss: 0.9738, Test Loss: 1.0865
Epoch: 31, Training Loss: 0.9726, Test Loss: 1.0812
Epoch: 32, Training Loss: 0.9720, Test Loss: 1.0718
Epoch: 33, Training Loss: 0.9595, Test Loss: 1.0782
Epoch: 34, Training Loss: 0.9552, Test Loss: 1.0841
Epoch: 35, Training Loss: 0.9496, Test Loss: 1.0654
Epoch: 36, Training Loss: 0.9428, Test Loss: 1.0484
Epoch: 37, Training Loss: 0.9343, Test Loss: 1.0549
Epoch: 38, Training Loss: 0.9232, Test Loss: 1.0481
Epoch: 39, Training Loss: 0.9152, Test Loss: 1.0450
Epoch: 40, Training Loss: 0.9090, Test Loss: 1.0361
Epoch: 41, Training Loss: 0.9039, Test Loss: 1.0160
Epoch: 42, Training Loss: 0.8946, Test Loss: 1.0095
Epoch: 43, Training Loss: 0.8890, Test Loss: 1.0147

 46%|█████████████████████████████████████████████                                                     | 46/100 [00:03<00:04, 12.05it/s, loss_test=0.987]
Epoch: 45, Training Loss: 0.8756, Test Loss: 0.9872
Epoch: 46, Training Loss: 0.8700, Test Loss: 0.9828
Epoch: 47, Training Loss: 0.8688, Test Loss: 1.0039
Epoch: 48, Training Loss: 0.8644, Test Loss: 0.9920
Epoch: 49, Training Loss: 0.8560, Test Loss: 0.9829
Epoch: 50, Training Loss: 0.8528, Test Loss: 0.9788
Epoch: 51, Training Loss: 0.8495, Test Loss: 0.9960
Epoch: 52, Training Loss: 0.8426, Test Loss: 0.9814
Epoch: 53, Training Loss: 0.8361, Test Loss: 0.9667
Epoch: 54, Training Loss: 0.8325, Test Loss: 0.9728
Epoch: 55, Training Loss: 0.8270, Test Loss: 0.9508
Epoch: 56, Training Loss: 0.8217, Test Loss: 0.9528
Epoch: 57, Training Loss: 0.8161, Test Loss: 0.9497
Epoch: 58, Training Loss: 0.8099, Test Loss: 0.9562
Epoch: 59, Training Loss: 0.8044, Test Loss: 0.9514
Epoch: 60, Training Loss: 0.8012, Test Loss: 0.9461
Epoch: 61, Training Loss: 0.7932, Test Loss: 0.9229
Epoch: 62, Training Loss: 0.7926, Test Loss: 0.9371
Epoch: 63, Training Loss: 0.7860, Test Loss: 0.9368
Epoch: 64, Training Loss: 0.7807, Test Loss: 0.9211
Epoch: 65, Training Loss: 0.7781, Test Loss: 0.9443
Epoch: 66, Training Loss: 0.7699, Test Loss: 0.9194
Epoch: 67, Training Loss: 0.7656, Test Loss: 0.9242
Epoch: 68, Training Loss: 0.7610, Test Loss: 0.9283

 70%|████████████████████████████████████████████████████████████████████▌                             | 70/100 [00:05<00:02, 12.03it/s, loss_test=0.925]
Epoch: 70, Training Loss: 0.7533, Test Loss: 0.9168
Epoch: 71, Training Loss: 0.7472, Test Loss: 0.9092
Epoch: 72, Training Loss: 0.7436, Test Loss: 0.9148
Epoch: 73, Training Loss: 0.7373, Test Loss: 0.9025
Epoch: 74, Training Loss: 0.7336, Test Loss: 0.8911
Epoch: 75, Training Loss: 0.7305, Test Loss: 0.9070
Epoch: 76, Training Loss: 0.7256, Test Loss: 0.9035
Epoch: 77, Training Loss: 0.7170, Test Loss: 0.9029
Epoch: 78, Training Loss: 0.7183, Test Loss: 0.8932
Epoch: 79, Training Loss: 0.7110, Test Loss: 0.9020
Epoch: 80, Training Loss: 0.7085, Test Loss: 0.8972
Epoch: 81, Training Loss: 0.7043, Test Loss: 0.8983
Epoch: 82, Training Loss: 0.6989, Test Loss: 0.8913
Epoch: 83, Training Loss: 0.6965, Test Loss: 0.9050
Epoch: 84, Training Loss: 0.6900, Test Loss: 0.9128
Epoch: 85, Training Loss: 0.6867, Test Loss: 0.8946
Epoch: 86, Training Loss: 0.6836, Test Loss: 0.9014
Epoch: 87, Training Loss: 0.6780, Test Loss: 0.8899
Epoch: 88, Training Loss: 0.6741, Test Loss: 0.8731
Epoch: 89, Training Loss: 0.6702, Test Loss: 0.8921
Epoch: 90, Training Loss: 0.6692, Test Loss: 0.8957
Epoch: 91, Training Loss: 0.6651, Test Loss: 0.9024
Epoch: 92, Training Loss: 0.6597, Test Loss: 0.8885


100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.02it/s, loss_test=0.886]
Epoch: 94, Training Loss: 0.6523, Test Loss: 0.8911
Epoch: 95, Training Loss: 0.6528, Test Loss: 0.8890
Epoch: 96, Training Loss: 0.6471, Test Loss: 0.8972
Epoch: 97, Training Loss: 0.6426, Test Loss: 0.8931
Epoch: 98, Training Loss: 0.6422, Test Loss: 0.8928
Epoch: 99, Training Loss: 0.6368, Test Loss: 0.8855
Model saved as model_6020575np.pt
Config : {'wandb': True, 'name': 'lstm-enc-dec-0.0001-2-1230000-6020575np', 'num_features': 30, 'hidden_size': 128, 'dropout': 0, 'weight_decay': 0, 'input_window': 2, 'output_window': 12, 'learning_rate': 0.0001, 'num_layers': 1, 'num_epochs': 100, 'batch_size': 128, 'train_data_len': 200000, 'training_prediction': 'recursive', 'loss_type': 'MSE', 'model_label': 'ENC-DEC-DATA', 'teacher_forcing_ratio': -4.198030811863873e-16, 'dynamic_tf': True, 'shuffle': True, 'one_hot_month': False, 'num_of_weigths': 1932, 'num_of_params': 217886, 'loss_train': [0.9893725991249085, 0.986349368095398, 0.9870312571525574, 0.9922400832176208, 0.992910635471344, 0.9888545155525208, 0.9845955491065979, 0.9896907448768616, 0.9905783414840699, 0.9837673544883728, 0.9889687418937683, 0.9874555110931397, 0.983106815814972, 0.9911788940429688, 0.9888735890388489, 0.9875389814376831, 0.981790566444397, 0.9807862639427185, 0.9895509004592895, 0.9834704637527466, 0.983880090713501, 0.981830096244812, 0.9831157922744751, 0.9816651105880737, 0.9849270582199097, 0.9804792046546936, 0.9795516490936279, 0.9817958474159241, 0.979753828048706, 0.9752821326255798, 0.9737536668777466, 0.9725537538528443, 0.9719512939453125, 0.9595221877098083, 0.9552131652832031, 0.9495857238769532, 0.9427533864974975, 0.9342827558517456, 0.9232266426086426, 0.9152057051658631, 0.9090198159217835, 0.9039078116416931, 0.894628894329071, 0.8889632821083069, 0.8798622965812684, 0.8755510330200196, 0.8700268745422364, 0.8687773227691651, 0.8644422054290771, 0.8559805274009704, 0.8527791619300842, 0.8494612455368042, 0.8426302671432495, 0.8360692024230957, 0.8325324058532715, 0.8269725561141967, 0.8217263460159302, 0.8160553336143493, 0.8099281787872314, 0.8044150710105896, 0.8012381792068481, 0.7932285666465759, 0.7926190257072449, 0.7860284924507142, 0.7807447075843811, 0.7781295776367188, 0.7699497461318969, 0.7656331062316895, 0.7610348224639892, 0.7575173735618591, 0.753282904624939, 0.747165036201477, 0.7436349511146545, 0.737290894985199, 0.7335717797279357, 0.7304502964019776, 0.7255871415138244, 0.7169883966445922, 0.7182881712913514, 0.7109665513038635, 0.7084534049034119, 0.7043090224266052, 0.6988638520240784, 0.6965167164802551, 0.6900211453437806, 0.6867282032966614, 0.6836284518241882, 0.6780350923538208, 0.6741208434104919, 0.6702316880226136, 0.6691947937011719, 0.6650643944740295, 0.6597107529640198, 0.656228494644165, 0.6522628784179687, 0.6527643322944641, 0.6471485018730163, 0.6426483154296875, 0.6421712875366211, 0.6368417859077453], 'loss_test': [1.0948072671890259, 1.0856469869613647, 1.0957764387130737, 1.095778465270996, 1.0877652168273926, 1.0735812187194824, 1.0897692441940308, 1.085485816001892, 1.095395565032959, 1.0801388025283813, 1.0884662866592407, 1.1120187044143677, 1.088151454925537, 1.0743589401245117, 1.070648193359375, 1.0758382081985474, 1.0838682651519775, 1.0771870613098145, 1.0835169553756714, 1.0862927436828613, 1.0646247863769531, 1.0734783411026, 1.0685776472091675, 1.062436819076538, 1.0702334642410278, 1.084259271621704, 1.0818551778793335, 1.080941081047058, 1.073081135749817, 1.0950781106948853, 1.0865211486816406, 1.0811618566513062, 1.07177734375, 1.07821786403656, 1.0841031074523926, 1.065375804901123, 1.0483944416046143, 1.0548615455627441, 1.0480730533599854, 1.0450124740600586, 1.0360502004623413, 1.0159506797790527, 1.0094906091690063, 1.0146836042404175, 1.009243369102478, 0.9872040152549744, 0.9827896952629089, 1.0039488077163696, 0.9919888973236084, 0.9829152822494507, 0.9787530899047852, 0.9959781169891357, 0.9813587069511414, 0.9667353630065918, 0.9728109836578369, 0.950798749923706, 0.9527798295021057, 0.9497244954109192, 0.956211507320404, 0.9514133334159851, 0.9460742473602295, 0.9228732585906982, 0.9371321201324463, 0.9367842078208923, 0.9210925698280334, 0.9442576169967651, 0.9193922281265259, 0.9241535663604736, 0.928287923336029, 0.924593448638916, 0.916755199432373, 0.9091631174087524, 0.9147586822509766, 0.9025270342826843, 0.8910510540008545, 0.9069979786872864, 0.9035075306892395, 0.902855634689331, 0.8931515216827393, 0.9019969701766968, 0.8971894979476929, 0.8982706665992737, 0.8913291096687317, 0.9050359129905701, 0.9127607941627502, 0.8945607542991638, 0.9014497995376587, 0.8898932933807373, 0.8730562329292297, 0.8920888304710388, 0.8957220315933228, 0.9024457335472107, 0.8884772062301636, 0.8922989964485168, 0.8910803198814392, 0.8889836072921753, 0.8971512317657471, 0.8930894732475281, 0.8927580118179321, 0.8855104446411133], 'identifier': '6020575np'}